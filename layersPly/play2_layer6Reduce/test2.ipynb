{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* only the bad tree layers\n",
    "\n",
    "```\n",
    "TimeContextBlock1d\n",
    "└── tcm (Sequential)\n",
    "    ├── ConvNeXtLikeBlock (kernel=7)\n",
    "    │   └── dwconv → norm → GELU → pwconv1\n",
    "    ├── ConvNeXtLikeBlock (kernel=19)\n",
    "    │   └── dwconv → norm → GELU → pwconv1\n",
    "    ├── ConvNeXtLikeBlock (kernel=31)\n",
    "    │   └── dwconv → norm → GELU → pwconv1\n",
    "    ├── ConvNeXtLikeBlock (kernel=59)\n",
    "    │   └── dwconv → norm → GELU → pwconv1\n",
    "    └── TransformerEncoderLayer\n",
    "        ├── MultiHeadAttention\n",
    "        │   └── k_proj, q_proj, v_proj, out_proj\n",
    "        ├── LayerNorm\n",
    "        └── FeedForward\n",
    "            └── intermediate_dense → GELU → output_dense\n",
    "└── exp_dim_conv: Conv1d(20 → 600)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "that hangs on:\n",
    "\n",
    "\n",
    "```\n",
    "I     input_align_4D_add: remove node = [], add node = ['/backbone/stage0/stage0.6/tcm/tcm.3/Add_reshape']\n",
    "I     fuse_two_reshape: remove node = ['/backbone/stage0/stage0.6/tcm/tcm.4/attention/Mul_0_unsqueeze0']\n",
    "I     input_align_4D_mul: remove node = [], add node = ['/backbone/stage0/stage0.6/tcm/tcm.4/attention/Mul_reshape']\n",
    "I     fuse_two_reshape: remove node = ['/backbone/stage0/stage0.6/tcm/tcm.4/attention/MatMul_0_unsqueeze1', '/backbone/stage0/stage0.6/tcm/tcm.4/attention/Softmax_0_unsqueeze1']\n",
    "I     input_align_4D_add: remove node = [], add node = ['/backbone/stage0/stage0.6/tcm/tcm.4/Add_reshape']\n",
    "I     input_align_4D_add: remove node = [], add node = ['/backbone/stage0/stage0.6/tcm/tcm.4/Add_1_reshape']\n",
    "I     input_align_4D_add: remove node = [], add node = ['/backbone/stage0/stage0.6/Add_reshape']\n",
    "I     input_align_4D_mul: remove node = [], add node = ['/pool/Mul_reshape']\n",
    "I     fuse_two_reshape: remove node = ['/pool/Mul_1_0_unsqueeze0']\n",
    "I     input_align_4D_mul: remove node = [], add node = ['/pool/Mul_1_reshape']\n",
    "I     fuse_two_reshape: remove node = ['/pool/Mul_1_0_unsqueeze1']\n",
    "I     input_align_4D_mul: remove node = [], add node = ['/pool/Div_2mul_reshape']\n",
    "I     fuse_two_reshape: remove node = ['/pool/Div_2mul_0_unsqueeze1']\n",
    "I     input_align_4D_add: remove node = [], add node = ['/pool/Add_reshape']\n",
    "I     remove_parallel_reshape: remove node = ['/pool/Mul_2_reshape']\n",
    "I     input_align_4D_mul: remove node = [], add node = ['/pool/Mul_2_reshape']\n",
    "I     fuse_two_reshape: remove node = ['/pool/ReduceSum_reshape']\n",
    "\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mNetworkTest1\u001b[39;00m(\u001b[43mnn\u001b[49m\u001b[38;5;241m.\u001b[39mModule):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, kernel_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m7\u001b[39m):\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "class NetworkTest1(nn.Module):\n",
    "    def __init__(self, dim=20, kernel_size=7):\n",
    "        super().__init__()\n",
    "        self.dwconv = nn.Conv1d(dim, dim, kernel_size, padding=\"same\", groups=dim)\n",
    "        self.norm = nn.BatchNorm1d(dim)\n",
    "        self.act = nn.GELU()\n",
    "        self.pwconv1 = nn.Conv1d(dim, dim, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dwconv(x)\n",
    "        x = self.norm(x)\n",
    "        x = self.act(x)\n",
    "        x = self.pwconv1(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetworkTest1_fix(nn.Module):\n",
    "    def __init__(self, dim=20, kernel_size=7):\n",
    "        super().__init__()\n",
    "        # Manually compute SAME padding\n",
    "        self.kernel_size = kernel_size\n",
    "        self.dwconv = nn.Conv1d(dim, dim, kernel_size, padding=0, groups=dim)\n",
    "        self.norm = nn.BatchNorm1d(dim)\n",
    "        self.act = nn.ReLU()  # GELU not fully supported in ONNX/RKNN\n",
    "        self.pwconv1 = nn.Conv1d(dim, dim, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):  # x: [B, C=20, T=100]\n",
    "        pad = (self.kernel_size - 1) // 2\n",
    "        x = F.pad(x, (pad, pad))  # simulate 'same' padding\n",
    "        x = self.dwconv(x)\n",
    "        x = self.norm(x)\n",
    "        x = self.act(x)\n",
    "        x = self.pwconv1(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetworkTest2(nn.Module):\n",
    "    def __init__(self, dim=20):\n",
    "        super().__init__()\n",
    "        self.q_proj = nn.Linear(dim, dim)\n",
    "        self.k_proj = nn.Linear(dim, dim)\n",
    "        self.v_proj = nn.Linear(dim, dim)\n",
    "        self.out_proj = nn.Linear(dim, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # assume input x: (B, T, dim)\n",
    "        q = self.q_proj(x)\n",
    "        k = self.k_proj(x)\n",
    "        v = self.v_proj(x)\n",
    "        out = self.out_proj(q)  # simulate attention output\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetworkTest3(nn.Module):\n",
    "    def __init__(self, dim=20, num_heads=2):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(embed_dim=dim, num_heads=num_heads, batch_first=True)\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # assume input x: (B, T, dim)\n",
    "        attn_output, _ = self.attn(x, x, x)\n",
    "        x = self.norm(attn_output)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetworkTest4(nn.Module):\n",
    "    def __init__(self, dim=20, num_heads=2, ff_dim=64):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(embed_dim=dim, num_heads=num_heads, batch_first=True)\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(dim, ff_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(ff_dim, dim),\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn_output, _ = self.attn(x, x, x)\n",
    "        x = self.norm1(attn_output)\n",
    "        x_ff = self.ff(x)\n",
    "        x = self.norm2(x_ff)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_onnx(model, name, shape):\n",
    "    model.eval()\n",
    "    dummy_input = torch.randn(*shape)\n",
    "\n",
    "    torch.onnx.export(\n",
    "        model,\n",
    "        dummy_input,\n",
    "        f\"{name}.onnx\",\n",
    "        input_names=[\"input\"],\n",
    "        output_names=[\"output\"],\n",
    "        opset_version=13,\n",
    "    )\n",
    "    print(f\"Exported {name}.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported network_test1.onnx\n",
      "Exported network_test1_fix.onnx\n",
      "Exported network_test2.onnx\n",
      "Exported network_test3.onnx\n",
      "Exported network_test4.onnx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/proj/voice/pyvoice_venv/lib/python3.10/site-packages/torch/onnx/_internal/jit_utils.py:308: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at /pytorch/torch/csrc/jit/passes/onnx/constant_fold.cpp:178.)\n",
      "  _C._jit_pass_onnx_node_shape_type_inference(node, params_dict, opset_version)\n",
      "/data/proj/voice/pyvoice_venv/lib/python3.10/site-packages/torch/onnx/utils.py:657: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at /pytorch/torch/csrc/jit/passes/onnx/constant_fold.cpp:178.)\n",
      "  _C._jit_pass_onnx_graph_shape_type_inference(\n",
      "/data/proj/voice/pyvoice_venv/lib/python3.10/site-packages/torch/onnx/utils.py:1127: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at /pytorch/torch/csrc/jit/passes/onnx/constant_fold.cpp:178.)\n",
      "  _C._jit_pass_onnx_graph_shape_type_inference(\n"
     ]
    }
   ],
   "source": [
    "# Test1: [B, C, T]\n",
    "model1 = NetworkTest1()\n",
    "export_onnx(model1, \"network_test1\", shape=(1, 20, 100))\n",
    "\n",
    "model1_fix = NetworkTest1_fix()\n",
    "export_onnx(model1_fix, \"network_test1_fix\", shape=(1, 20, 100))\n",
    "\n",
    "\n",
    "# Test2-4: [B, T, C]\n",
    "model2 = NetworkTest2()\n",
    "export_onnx(model2, \"network_test2\", shape=(1, 100, 20))\n",
    "\n",
    "model3 = NetworkTest3()\n",
    "export_onnx(model3, \"network_test3\", shape=(1, 100, 20))\n",
    "\n",
    "model4 = NetworkTest4()\n",
    "export_onnx(model4, \"network_test4\", shape=(1, 100, 20))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyvoice_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
