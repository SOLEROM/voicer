{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* only the bad tree layers\n",
    "\n",
    "```\n",
    "TimeContextBlock1d\n",
    "└── tcm (Sequential)\n",
    "    ├── ConvNeXtLikeBlock (kernel=7)\n",
    "    │   └── dwconv → norm → GELU → pwconv1\n",
    "    ├── ConvNeXtLikeBlock (kernel=19)\n",
    "    │   └── dwconv → norm → GELU → pwconv1\n",
    "    ├── ConvNeXtLikeBlock (kernel=31)\n",
    "    │   └── dwconv → norm → GELU → pwconv1\n",
    "    ├── ConvNeXtLikeBlock (kernel=59)\n",
    "    │   └── dwconv → norm → GELU → pwconv1\n",
    "    └── TransformerEncoderLayer\n",
    "        ├── MultiHeadAttention\n",
    "        │   └── k_proj, q_proj, v_proj, out_proj\n",
    "        ├── LayerNorm\n",
    "        └── FeedForward\n",
    "            └── intermediate_dense → GELU → output_dense\n",
    "└── exp_dim_conv: Conv1d(20 → 600)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "that hangs on:\n",
    "\n",
    "\n",
    "```\n",
    "I     input_align_4D_add: remove node = [], add node = ['/backbone/stage0/stage0.6/tcm/tcm.3/Add_reshape']\n",
    "I     fuse_two_reshape: remove node = ['/backbone/stage0/stage0.6/tcm/tcm.4/attention/Mul_0_unsqueeze0']\n",
    "I     input_align_4D_mul: remove node = [], add node = ['/backbone/stage0/stage0.6/tcm/tcm.4/attention/Mul_reshape']\n",
    "I     fuse_two_reshape: remove node = ['/backbone/stage0/stage0.6/tcm/tcm.4/attention/MatMul_0_unsqueeze1', '/backbone/stage0/stage0.6/tcm/tcm.4/attention/Softmax_0_unsqueeze1']\n",
    "I     input_align_4D_add: remove node = [], add node = ['/backbone/stage0/stage0.6/tcm/tcm.4/Add_reshape']\n",
    "I     input_align_4D_add: remove node = [], add node = ['/backbone/stage0/stage0.6/tcm/tcm.4/Add_1_reshape']\n",
    "I     input_align_4D_add: remove node = [], add node = ['/backbone/stage0/stage0.6/Add_reshape']\n",
    "I     input_align_4D_mul: remove node = [], add node = ['/pool/Mul_reshape']\n",
    "I     fuse_two_reshape: remove node = ['/pool/Mul_1_0_unsqueeze0']\n",
    "I     input_align_4D_mul: remove node = [], add node = ['/pool/Mul_1_reshape']\n",
    "I     fuse_two_reshape: remove node = ['/pool/Mul_1_0_unsqueeze1']\n",
    "I     input_align_4D_mul: remove node = [], add node = ['/pool/Div_2mul_reshape']\n",
    "I     fuse_two_reshape: remove node = ['/pool/Div_2mul_0_unsqueeze1']\n",
    "I     input_align_4D_add: remove node = [], add node = ['/pool/Add_reshape']\n",
    "I     remove_parallel_reshape: remove node = ['/pool/Mul_2_reshape']\n",
    "I     input_align_4D_mul: remove node = [], add node = ['/pool/Mul_2_reshape']\n",
    "I     fuse_two_reshape: remove node = ['/pool/ReduceSum_reshape']\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "* the transformer-style self-attention operations, especially:\n",
    "    * MatMul and Softmax operators working on [B, H, T, D] shapes.\n",
    "    * The unsqueeze1 reshape points to an attempt to realign tensors for broadcasting in attention.\n",
    "    * RKNN often has problems with ONNX ops that rely on dynamic reshaping or broadcasting across mismatched dims — typical in attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewGELUActivation(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return F.gelu(x, approximate='none')\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.intermediate_dense = nn.Linear(dim, dim)\n",
    "        self.intermediate_act_fn = NewGELUActivation()\n",
    "        self.output_dense = nn.Linear(dim, dim)\n",
    "        self.intermediate_dropout = nn.Dropout(0.0)\n",
    "        self.output_dropout = nn.Dropout(0.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.intermediate_dense(x)\n",
    "        x = self.intermediate_act_fn(x)\n",
    "        x = self.intermediate_dropout(x)\n",
    "        x = self.output_dense(x)\n",
    "        x = self.output_dropout(x)\n",
    "        return x\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "\n",
    "        self.q_proj = nn.Linear(dim, dim)\n",
    "        self.k_proj = nn.Linear(dim, dim)\n",
    "        self.v_proj = nn.Linear(dim, dim)\n",
    "        self.out_proj = nn.Linear(dim, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        q = self.q_proj(x).view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        k = self.k_proj(x).view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        v = self.v_proj(x).view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        attn_weights = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn_weights = attn_weights.softmax(dim=-1)\n",
    "\n",
    "        out = (attn_weights @ v).transpose(1, 2).contiguous().view(B, T, C)\n",
    "        return self.out_proj(out)\n",
    "\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(dim, num_heads)\n",
    "        self.layer_norm = nn.LayerNorm(dim, eps=1e-6)\n",
    "        self.feed_forward = FeedForward(dim)\n",
    "        self.final_layer_norm = nn.LayerNorm(dim, eps=1e-6)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attention(self.layer_norm(x))\n",
    "        x = x + self.feed_forward(self.final_layer_norm(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Test4(nn.Module):\n",
    "    def __init__(self, dim=20, num_heads=4):\n",
    "        super().__init__()\n",
    "        self.block = TransformerEncoderLayer(dim, num_heads)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Test4()\n",
    "model.eval()\n",
    "dummy = torch.randn(1, 100, 20)  # B, T, C\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    dummy,\n",
    "    \"test4_transformer.onnx\",\n",
    "    input_names=[\"input\"],\n",
    "    output_names=[\"output\"],\n",
    "    opset_version=11\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOT FAILS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyvoice_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
