{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* only the bad tree layers\n",
    "\n",
    "```\n",
    "TimeContextBlock1d\n",
    "└── tcm (Sequential)\n",
    "    ├── ConvNeXtLikeBlock (kernel=7)\n",
    "    │   └── dwconv → norm → GELU → pwconv1\n",
    "    ├── ConvNeXtLikeBlock (kernel=19)\n",
    "    │   └── dwconv → norm → GELU → pwconv1\n",
    "    ├── ConvNeXtLikeBlock (kernel=31)\n",
    "    │   └── dwconv → norm → GELU → pwconv1\n",
    "    ├── ConvNeXtLikeBlock (kernel=59)\n",
    "    │   └── dwconv → norm → GELU → pwconv1\n",
    "    └── TransformerEncoderLayer\n",
    "        ├── MultiHeadAttention\n",
    "        │   └── k_proj, q_proj, v_proj, out_proj\n",
    "        ├── LayerNorm\n",
    "        └── FeedForward\n",
    "            └── intermediate_dense → GELU → output_dense\n",
    "└── exp_dim_conv: Conv1d(20 → 600)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "that hangs on:\n",
    "\n",
    "\n",
    "```\n",
    "I     input_align_4D_add: remove node = [], add node = ['/backbone/stage0/stage0.6/tcm/tcm.3/Add_reshape']\n",
    "I     fuse_two_reshape: remove node = ['/backbone/stage0/stage0.6/tcm/tcm.4/attention/Mul_0_unsqueeze0']\n",
    "I     input_align_4D_mul: remove node = [], add node = ['/backbone/stage0/stage0.6/tcm/tcm.4/attention/Mul_reshape']\n",
    "I     fuse_two_reshape: remove node = ['/backbone/stage0/stage0.6/tcm/tcm.4/attention/MatMul_0_unsqueeze1', '/backbone/stage0/stage0.6/tcm/tcm.4/attention/Softmax_0_unsqueeze1']\n",
    "I     input_align_4D_add: remove node = [], add node = ['/backbone/stage0/stage0.6/tcm/tcm.4/Add_reshape']\n",
    "I     input_align_4D_add: remove node = [], add node = ['/backbone/stage0/stage0.6/tcm/tcm.4/Add_1_reshape']\n",
    "I     input_align_4D_add: remove node = [], add node = ['/backbone/stage0/stage0.6/Add_reshape']\n",
    "I     input_align_4D_mul: remove node = [], add node = ['/pool/Mul_reshape']\n",
    "I     fuse_two_reshape: remove node = ['/pool/Mul_1_0_unsqueeze0']\n",
    "I     input_align_4D_mul: remove node = [], add node = ['/pool/Mul_1_reshape']\n",
    "I     fuse_two_reshape: remove node = ['/pool/Mul_1_0_unsqueeze1']\n",
    "I     input_align_4D_mul: remove node = [], add node = ['/pool/Div_2mul_reshape']\n",
    "I     fuse_two_reshape: remove node = ['/pool/Div_2mul_0_unsqueeze1']\n",
    "I     input_align_4D_add: remove node = [], add node = ['/pool/Add_reshape']\n",
    "I     remove_parallel_reshape: remove node = ['/pool/Mul_2_reshape']\n",
    "I     input_align_4D_mul: remove node = [], add node = ['/pool/Mul_2_reshape']\n",
    "I     fuse_two_reshape: remove node = ['/pool/ReduceSum_reshape']\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "* the transformer-style self-attention operations, especially:\n",
    "    * MatMul and Softmax operators working on [B, H, T, D] shapes.\n",
    "    * The unsqueeze1 reshape points to an attempt to realign tensors for broadcasting in attention.\n",
    "    * RKNN often has problems with ONNX ops that rely on dynamic reshaping or broadcasting across mismatched dims — typical in attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewGELUActivation(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return F.gelu(x, approximate='none')\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.intermediate_dense = nn.Linear(dim, dim)\n",
    "        self.intermediate_act_fn = NewGELUActivation()\n",
    "        self.output_dense = nn.Linear(dim, dim)\n",
    "        self.intermediate_dropout = nn.Dropout(0.0)\n",
    "        self.output_dropout = nn.Dropout(0.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.intermediate_dense(x)\n",
    "        x = self.intermediate_act_fn(x)\n",
    "        x = self.intermediate_dropout(x)\n",
    "        x = self.output_dense(x)\n",
    "        x = self.output_dropout(x)\n",
    "        return x\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "\n",
    "        self.q_proj = nn.Linear(dim, dim)\n",
    "        self.k_proj = nn.Linear(dim, dim)\n",
    "        self.v_proj = nn.Linear(dim, dim)\n",
    "        self.out_proj = nn.Linear(dim, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        q = self.q_proj(x).view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        k = self.k_proj(x).view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        v = self.v_proj(x).view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        attn_weights = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn_weights = attn_weights.softmax(dim=-1)\n",
    "\n",
    "        out = (attn_weights @ v).transpose(1, 2).contiguous().view(B, T, C)\n",
    "        return self.out_proj(out)\n",
    "\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(dim, num_heads)\n",
    "        self.layer_norm = nn.LayerNorm(dim, eps=1e-6)\n",
    "        self.feed_forward = FeedForward(dim)\n",
    "        self.final_layer_norm = nn.LayerNorm(dim, eps=1e-6)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attention(self.layer_norm(x))\n",
    "        x = x + self.feed_forward(self.final_layer_norm(x))\n",
    "        return x\n",
    "\n",
    "class Test4(nn.Module):\n",
    "    def __init__(self, in_channels=600, mid_channels=20, seq_len=100):\n",
    "        super().__init__()\n",
    "        self.reduce = nn.Conv1d(in_channels, mid_channels, kernel_size=1)\n",
    "        self.norm = nn.LayerNorm(mid_channels)  # applied after transpose to (B, T, C)\n",
    "        self.transformer = TransformerEncoderLayer(mid_channels, num_heads=4)\n",
    "        self.expand = nn.Conv1d(mid_channels, in_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, C, T] (like your full model)\n",
    "        x = self.reduce(x)  # [B, 600, T] -> [B, 20, T]\n",
    "        x = x.permute(0, 2, 1)  # → [B, T, 20]\n",
    "        x = self.transformer(x)  # → [B, T, 20]\n",
    "        x = x.permute(0, 2, 1)  # → [B, 20, T]\n",
    "        x = self.expand(x)  # [B, 600, T]\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported graph: graph(%input : Float(1, 600, 100, strides=[60000, 100, 1], requires_grad=0, device=cpu),\n",
      "      %reduce.weight : Float(20, 600, 1, strides=[600, 1, 1], requires_grad=1, device=cpu),\n",
      "      %reduce.bias : Float(20, strides=[1], requires_grad=1, device=cpu),\n",
      "      %transformer.attention.q_proj.bias : Float(20, strides=[1], requires_grad=1, device=cpu),\n",
      "      %transformer.attention.k_proj.bias : Float(20, strides=[1], requires_grad=1, device=cpu),\n",
      "      %transformer.attention.v_proj.bias : Float(20, strides=[1], requires_grad=1, device=cpu),\n",
      "      %transformer.attention.out_proj.bias : Float(20, strides=[1], requires_grad=1, device=cpu),\n",
      "      %transformer.layer_norm.weight : Float(20, strides=[1], requires_grad=1, device=cpu),\n",
      "      %transformer.layer_norm.bias : Float(20, strides=[1], requires_grad=1, device=cpu),\n",
      "      %transformer.feed_forward.intermediate_dense.bias : Float(20, strides=[1], requires_grad=1, device=cpu),\n",
      "      %transformer.feed_forward.output_dense.bias : Float(20, strides=[1], requires_grad=1, device=cpu),\n",
      "      %expand.weight : Float(600, 20, 1, strides=[20, 1, 1], requires_grad=1, device=cpu),\n",
      "      %expand.bias : Float(600, strides=[1], requires_grad=1, device=cpu),\n",
      "      %onnx::MatMul_129 : Float(20, 20, strides=[1, 20], requires_grad=0, device=cpu),\n",
      "      %onnx::MatMul_145 : Float(20, 20, strides=[1, 20], requires_grad=0, device=cpu),\n",
      "      %onnx::MatMul_146 : Float(20, 20, strides=[1, 20], requires_grad=0, device=cpu),\n",
      "      %onnx::MatMul_151 : Float(20, 20, strides=[1, 20], requires_grad=0, device=cpu),\n",
      "      %onnx::MatMul_152 : Float(20, 20, strides=[1, 20], requires_grad=0, device=cpu),\n",
      "      %onnx::MatMul_153 : Float(20, 20, strides=[1, 20], requires_grad=0, device=cpu)):\n",
      "  %transformer.final_layer_norm.bias : Float(20, strides=[1], requires_grad=1, device=cpu) = onnx::Identity(%transformer.layer_norm.bias)\n",
      "  %transformer.final_layer_norm.weight : Float(20, strides=[1], requires_grad=1, device=cpu) = onnx::Identity(%transformer.layer_norm.weight)\n",
      "  %/reduce/Conv_output_0 : Float(1, 20, 100, strides=[2000, 100, 1], requires_grad=0, device=cpu) = onnx::Conv[dilations=[1], group=1, kernel_shape=[1], pads=[0, 0], strides=[1], onnx_name=\"/reduce/Conv\"](%input, %reduce.weight, %reduce.bias), scope: __main__.Test4::/torch.nn.modules.conv.Conv1d::reduce # /data/proj/voice/pyvoice_venv/lib/python3.10/site-packages/torch/nn/modules/conv.py:370:0\n",
      "  %/Transpose_output_0 : Float(1, 100, 20, strides=[2000, 1, 100], requires_grad=1, device=cpu) = onnx::Transpose[perm=[0, 2, 1], onnx_name=\"/Transpose\"](%/reduce/Conv_output_0), scope: __main__.Test4:: # /tmp/ipykernel_601266/3680961281.py:70:0\n",
      "  %/transformer/layer_norm/ReduceMean_output_0 : Float(1, 100, 1, strides=[100, 1, 1], device=cpu) = onnx::ReduceMean[axes=[-1], onnx_name=\"/transformer/layer_norm/ReduceMean\"](%/Transpose_output_0), scope: __main__.Test4::/__main__.TransformerEncoderLayer::transformer/torch.nn.modules.normalization.LayerNorm::layer_norm # /data/proj/voice/pyvoice_venv/lib/python3.10/site-packages/torch/nn/functional.py:2910:0\n",
      "  %/transformer/layer_norm/Sub_output_0 : Float(1, 100, 20, strides=[2000, 20, 1], device=cpu) = onnx::Sub[onnx_name=\"/transformer/layer_norm/Sub\"](%/Transpose_output_0, %/transformer/layer_norm/ReduceMean_output_0), scope: __main__.Test4::/__main__.TransformerEncoderLayer::transformer/torch.nn.modules.normalization.LayerNorm::layer_norm # /data/proj/voice/pyvoice_venv/lib/python3.10/site-packages/torch/nn/functional.py:2910:0\n",
      "  %/transformer/layer_norm/Constant_output_0 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={2}, onnx_name=\"/transformer/layer_norm/Constant\"](), scope: __main__.Test4::/__main__.TransformerEncoderLayer::transformer/torch.nn.modules.normalization.LayerNorm::layer_norm # /data/proj/voice/pyvoice_venv/lib/python3.10/site-packages/torch/nn/functional.py:2910:0\n",
      "  %/transformer/layer_norm/Pow_output_0 : Float(1, 100, 20, strides=[2000, 20, 1], device=cpu) = onnx::Pow[onnx_name=\"/transformer/layer_norm/Pow\"](%/transformer/layer_norm/Sub_output_0, %/transformer/layer_norm/Constant_output_0), scope: __main__.Test4::/__main__.TransformerEncoderLayer::transformer/torch.nn.modules.normalization.LayerNorm::layer_norm # /data/proj/voice/pyvoice_venv/lib/python3.10/site-packages/torch/nn/functional.py:2910:0\n",
      "  %/transformer/layer_norm/ReduceMean_1_output_0 : Float(1, 100, 1, strides=[100, 1, 1], device=cpu) = onnx::ReduceMean[axes=[-1], onnx_name=\"/transformer/layer_norm/ReduceMean_1\"](%/transformer/layer_norm/Pow_output_0), scope: __main__.Test4::/__main__.TransformerEncoderLayer::transformer/torch.nn.modules.normalization.LayerNorm::layer_norm # /data/proj/voice/pyvoice_venv/lib/python3.10/site-packages/torch/nn/functional.py:2910:0\n",
      "  %/transformer/layer_norm/Constant_1_output_0 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={1e-06}, onnx_name=\"/transformer/layer_norm/Constant_1\"](), scope: __main__.Test4::/__main__.TransformerEncoderLayer::transformer/torch.nn.modules.normalization.LayerNorm::layer_norm # /data/proj/voice/pyvoice_venv/lib/python3.10/site-packages/torch/nn/functional.py:2910:0\n",
      "  %/transformer/layer_norm/Add_output_0 : Float(1, 100, 1, strides=[100, 1, 1], device=cpu) = onnx::Add[onnx_name=\"/transformer/layer_norm/Add\"](%/transformer/layer_norm/ReduceMean_1_output_0, %/transformer/layer_norm/Constant_1_output_0), scope: __main__.Test4::/__main__.TransformerEncoderLayer::transformer/torch.nn.modules.normalization.LayerNorm::layer_norm # /data/proj/voice/pyvoice_venv/lib/python3.10/site-packages/torch/nn/functional.py:2910:0\n",
      "  %/transformer/layer_norm/Sqrt_output_0 : Float(1, 100, 1, strides=[100, 1, 1], device=cpu) = onnx::Sqrt[onnx_name=\"/transformer/layer_norm/Sqrt\"](%/transformer/layer_norm/Add_output_0), scope: __main__.Test4::/__main__.TransformerEncoderLayer::transformer/torch.nn.modules.normalization.LayerNorm::layer_norm # /data/proj/voice/pyvoice_venv/lib/python3.10/site-packages/torch/nn/functional.py:2910:0\n",
      "  %/transformer/layer_norm/Div_output_0 : Float(1, 100, 20, strides=[2000, 20, 1], device=cpu) = onnx::Div[onnx_name=\"/transformer/layer_norm/Div\"](%/transformer/layer_norm/Sub_output_0, %/transformer/layer_norm/Sqrt_output_0), scope: __main__.Test4::/__main__.TransformerEncoderLayer::transformer/torch.nn.modules.normalization.LayerNorm::layer_norm # /data/proj/voice/pyvoice_venv/lib/python3.10/site-packages/torch/nn/functional.py:2910:0\n",
      "  %/transformer/layer_norm/Mul_output_0 : Float(1, 100, 20, strides=[2000, 20, 1], device=cpu) = onnx::Mul[onnx_name=\"/transformer/layer_norm/Mul\"](%/transformer/layer_norm/Div_output_0, %transformer.layer_norm.weight), scope: __main__.Test4::/__main__.TransformerEncoderLayer::transformer/torch.nn.modules.normalization.LayerNorm::layer_norm # /data/proj/voice/pyvoice_venv/lib/python3.10/site-packages/torch/nn/functional.py:2910:0\n",
      "  %/transformer/layer_norm/Add_1_output_0 : Float(1, 100, 20, strides=[2000, 20, 1], requires_grad=1, device=cpu) = onnx::Add[onnx_name=\"/transformer/layer_norm/Add_1\"](%/transformer/layer_norm/Mul_output_0, %transformer.layer_norm.bias), scope: __main__.Test4::/__main__.TransformerEncoderLayer::transformer/torch.nn.modules.normalization.LayerNorm::layer_norm # /data/proj/voice/pyvoice_venv/lib/python3.10/site-packages/torch/nn/functional.py:2910:0\n",
      "  %/transformer/attention/q_proj/MatMul_output_0 : Float(1, 100, 20, strides=[2000, 20, 1], device=cpu) = onnx::MatMul[onnx_name=\"/transformer/attention/q_proj/MatMul\"](%/transformer/layer_norm/Add_1_output_0, %onnx::MatMul_129), scope: __main__.Test4::/__main__.TransformerEncoderLayer::transformer/__main__.MultiHeadAttention::attention/torch.nn.modules.linear.Linear::q_proj # /data/proj/voice/pyvoice_venv/lib/python3.10/site-packages/torch/nn/modules/linear.py:125:0\n",
      "  %/transformer/attention/q_proj/Add_output_0 : Float(1, 100, 20, strides=[2000, 20, 1], requires_grad=1, device=cpu) = onnx::Add[onnx_name=\"/transformer/attention/q_proj/Add\"](%transformer.attention.q_proj.bias, %/transformer/attention/q_proj/MatMul_output_0), scope: __main__.Test4::/__main__.TransformerEncoderLayer::transformer/__main__.MultiHeadAttention::attention/torch.nn.modules.linear.Linear::q_proj # /data/proj/voice/pyvoice_venv/lib/python3.10/site-packages/torch/nn/modules/linear.py:125:0\n",
      "  %/transformer/attention/Constant_output_0 : Long(4, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value=   1  100    4    5 [ CPULongType{4} ], onnx_name=\"/transformer/attention/Constant\"](), scope: __main__.Test4::/__main__.TransformerEncoderLayer::transformer/__main__.MultiHeadAttention::attention # /tmp/ipykernel_601266/3680961281.py:36:0\n",
      "  %/transformer/attention/Constant_1_output_0 : Long(4, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value=   1  100    4    5 [ CPULongType{4} ], onnx_name=\"/transformer/attention/Constant_1\"](), scope: __main__.Test4::/__main__.TransformerEncoderLayer::transformer/__main__.MultiHeadAttention::attention # /tmp/ipykernel_601266/3680961281.py:37:0\n",
      "  %/transformer/attention/Constant_2_output_0 : Long(4, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value=   1  100    4    5 [ CPULongType{4} ], onnx_name=\"/transformer/attention/Constant_2\"](), scope: __main__.Test4::/__main__.TransformerEncoderLayer::transformer/__main__.MultiHeadAttention::attention # /tmp/ipykernel_601266/3680961281.py:38:0\n",
      "  %/transformer/attention/Reshape_output_0 : Float(1, 100, 4, 5, strides=[2000, 20, 5, 1], requires_grad=1, device=cpu) = onnx::Reshape[onnx_name=\"/transformer/attention/Reshape\"](%/transformer/attention/q_proj/Add_output_0, %/transformer/attention/Constant_output_0), scope: __main__.Test4::/__main__.TransformerEncoderLayer::transformer/__main__.MultiHeadAttention::attention # /tmp/ipykernel_601266/3680961281.py:36:0\n",
      "  %/transformer/attention/Transpose_output_0 : Float(1, 4, 100, 5, strides=[2000, 5, 20, 1], requires_grad=1, device=cpu) = onnx::Transpose[perm=[0, 2, 1, 3], onnx_name=\"/transformer/attention/Transpose\"](%/transformer/attention/Reshape_output_0), scope: __main__.Test4::/__main__.TransformerEncoderLayer::transformer/__main__.MultiHeadAttention::attention # /tmp/ipykernel_601266/3680961281.py:36:0\n",
      "  %/transformer/attention/k_proj/MatMul_output_0 : Float(1, 100, 20, strides=[2000, 20, 1], device=cpu) = onnx::MatMul[onnx_name=\"/transformer/attention/k_proj/MatMul\"](%/transformer/layer_norm/Add_1_output_0, %onnx::MatMul_145), scope: __main__.Test4::/__main__.TransformerEncoderLayer::transformer/__main__.MultiHeadAttention::attention/torch.nn.modules.linear.Linear::k_proj # /data/proj/voice/pyvoice_venv/lib/python3.10/site-packages/torch/nn/modules/linear.py:125:0\n",
      "  %/transformer/attention/k_proj/Add_output_0 : Float(1, 100, 20, strides=[2000, 20, 1], requires_grad=1, device=cpu) = onnx::Add[onnx_name=\"/transformer/attention/k_proj/Add\"](%transformer.attention.k_proj.bias, %/transformer/attention/k_proj/MatMul_output_0), scope: __main__.Test4::/__main__.TransformerEncoderLayer::transformer/__main__.MultiHeadAttention::attention/torch.nn.modules.linear.Linear::k_proj # /data/proj/voice/pyvoice_venv/lib/python3.10/site-packages/torch/nn/modules/linear.py:125:0\n",
      "  %/transformer/attention/Reshape_1_output_0 : Float(1, 100, 4, 5, strides=[2000, 20, 5, 1], requires_grad=1, device=cpu) = onnx::Reshape[onnx_name=\"/transformer/attention/Reshape_1\"](%/transformer/attention/k_proj/Add_output_0, %/transformer/attention/Constant_1_output_0), scope: __main__.Test4::/__main__.TransformerEncoderLayer::transformer/__main__.MultiHeadAttention::attention # /tmp/ipykernel_601266/3680961281.py:37:0\n",
      "  %/transformer/attention/v_proj/MatMul_output_0 : Float(1, 100, 20, strides=[2000, 20, 1], device=cpu) = onnx::MatMul[onnx_name=\"/transformer/attention/v_proj/MatMul\"](%/transformer/layer_norm/Add_1_output_0, %onnx::MatMul_146), scope: __main__.Test4::/__main__.TransformerEncoderLayer::transformer/__main__.MultiHeadAttention::attention/torch.nn.modules.linear.Linear::v_proj # /data/proj/voice/pyvoice_venv/lib/python3.10/site-packages/torch/nn/modules/linear.py:125:0\n",
      "  %/transformer/attention/v_proj/Add_output_0 : Float(1, 100, 20, strides=[2000, 20, 1], requires_grad=1, device=cpu) = onnx::Add[onnx_name=\"/transformer/attention/v_proj/Add\"](%transformer.attention.v_proj.bias, %/transformer/attention/v_proj/MatMul_output_0), scope: __main__.Test4::/__main__.TransformerEncoderLayer::transformer/__main__.MultiHeadAttention::attention/torch.nn.modules.linear.Linear::v_proj # /data/proj/voice/pyvoice_venv/lib/python3.10/site-packages/torch/nn/modules/linear.py:125:0\n",
      "  %/transformer/attention/Reshape_2_output_0 : Float(1, 100, 4, 5, strides=[2000, 20, 5, 1], requires_grad=1, device=cpu) = onnx::Reshape[onnx_name=\"/transformer/attention/Reshape_2\"](%/transformer/attention/v_proj/Add_output_0, %/transformer/attention/Constant_2_output_0), scope: __main__.Test4::/__main__.TransformerEncoderLayer::transformer/__main__.MultiHeadAttention::attention # /tmp/ipykernel_601266/3680961281.py:38:0\n",
      "  %/transformer/attention/Transpose_1_output_0 : Float(1, 4, 100, 5, strides=[2000, 5, 20, 1], requires_grad=1, device=cpu) = onnx::Transpose[perm=[0, 2, 1, 3], onnx_name=\"/transformer/attention/Transpose_1\"](%/transformer/attention/Reshape_2_output_0), scope: __main__.Test4::/__main__.TransformerEncoderLayer::transformer/__main__.MultiHeadAttention::attention # /tmp/ipykernel_601266/3680961281.py:38:0\n",
      "  %/transformer/attention/Transpose_2_output_0 : Float(1, 4, 5, 100, strides=[2000, 5, 1, 20], requires_grad=1, device=cpu) = onnx::Transpose[perm=[0, 2, 3, 1], onnx_name=\"/transformer/attention/Transpose_2\"](%/transformer/attention/Reshape_1_output_0), scope: __main__.Test4::/__main__.TransformerEncoderLayer::transformer/__main__.MultiHeadAttention::attention # /tmp/ipykernel_601266/3680961281.py:40:0\n",
      "  %/transformer/attention/MatMul_output_0 : Float(1, 4, 100, 100, strides=[40000, 10000, 100, 1], requires_grad=1, device=cpu) = onnx::MatMul[onnx_name=\"/transformer/attention/MatMul\"](%/transformer/attention/Transpose_output_0, %/transformer/attention/Transpose_2_output_0), scope: __main__.Test4::/__main__.TransformerEncoderLayer::transformer/__main__.MultiHeadAttention::attention # /tmp/ipykernel_601266/3680961281.py:40:0\n",
      "  %/transformer/attention/Constant_3_output_0 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={0.447214}, onnx_name=\"/transformer/attention/Constant_3\"](), scope: __main__.Test4::/__main__.TransformerEncoderLayer::transformer/__main__.MultiHeadAttention::attention # /tmp/ipykernel_601266/3680961281.py:40:0\n",
      "  %/transformer/attention/Mul_output_0 : Float(1, 4, 100, 100, strides=[40000, 10000, 100, 1], requires_grad=1, device=cpu) = onnx::Mul[onnx_name=\"/transformer/attention/Mul\"](%/transformer/attention/MatMul_output_0, %/transformer/attention/Constant_3_output_0), scope: __main__.Test4::/__main__.TransformerEncoderLayer::transformer/__main__.MultiHeadAttention::attention # /tmp/ipykernel_601266/3680961281.py:40:0\n",
      "  %/transformer/attention/Softmax_output_0 : Float(1, 4, 100, 100, strides=[40000, 10000, 100, 1], requires_grad=1, device=cpu) = onnx::Softmax[axis=-1, onnx_name=\"/transformer/attention/Softmax\"](%/transformer/attention/Mul_output_0), scope: __main__.Test4::/__main__.TransformerEncoderLayer::transformer/__main__.MultiHeadAttention::attention # /tmp/ipykernel_601266/3680961281.py:41:0\n",
      "  %/transformer/attention/MatMul_1_output_0 : Float(1, 4, 100, 5, strides=[2000, 500, 5, 1], requires_grad=1, device=cpu) = onnx::MatMul[onnx_name=\"/transformer/attention/MatMul_1\"](%/transformer/attention/Softmax_output_0, %/transformer/attention/Transpose_1_output_0), scope: __main__.Test4::/__main__.TransformerEncoderLayer::transformer/__main__.MultiHeadAttention::attention # /tmp/ipykernel_601266/3680961281.py:43:0\n",
      "  %/transformer/attention/Transpose_3_output_0 : Float(1, 100, 4, 5, strides=[2000, 20, 5, 1], requires_grad=1, device=cpu) = onnx::Transpose[perm=[0, 2, 1, 3], onnx_name=\"/transformer/attention/Transpose_3\"](%/transformer/attention/MatMul_1_output_0), scope: __main__.Test4::/__main__.TransformerEncoderLayer::transformer/__main__.MultiHeadAttention::attention # /tmp/ipykernel_601266/3680961281.py:43:0\n",
      "  %/transformer/attention/Constant_4_output_0 : Long(3, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value=   1  100   20 [ CPULongType{3} ], onnx_name=\"/transformer/attention/Constant_4\"](), scope: __main__.Test4::/__main__.TransformerEncoderLayer::transformer/__main__.MultiHeadAttention::attention # /tmp/ipykernel_601266/3680961281.py:43:0\n",
      "  %/transformer/attention/Reshape_3_output_0 : Float(1, 100, 20, strides=[2000, 20, 1], requires_grad=1, device=cpu) = onnx::Reshape[onnx_name=\"/transformer/attention/Reshape_3\"](%/transformer/attention/Transpose_3_output_0, %/transformer/attention/Constant_4_output_0), scope: __main__.Test4::/__main__.TransformerEncoderLayer::transformer/__main__.MultiHeadAttention::attention # /tmp/ipykernel_601266/3680961281.py:43:0\n",
      "  %/transformer/attention/out_proj/MatMul_output_0 : Float(1, 100, 20, strides=[2000, 20, 1], device=cpu) = onnx::MatMul[onnx_name=\"/transformer/attention/out_proj/MatMul\"](%/transformer/attention/Reshape_3_output_0, %onnx::MatMul_151), scope: __main__.Test4::/__main__.TransformerEncoderLayer::transformer/__main__.MultiHeadAttention::attention/torch.nn.modules.linear.Linear::out_proj # /data/proj/voice/pyvoice_venv/lib/python3.10/site-packages/torch/nn/modules/linear.py:125:0\n",
      "  %/transformer/attention/out_proj/Add_output_0 : Float(1, 100, 20, strides=[2000, 20, 1], requires_grad=1, device=cpu) = onnx::Add[onnx_name=\"/transformer/attention/out_proj/Add\"](%transformer.attention.out_proj.bias, %/transformer/attention/out_proj/MatMul_output_0), scope: __main__.Test4::/__main__.TransformerEncoderLayer::transformer/__main__.MultiHeadAttention::attention/torch.nn.modules.linear.Linear::out_proj # /data/proj/voice/pyvoice_venv/lib/python3.10/site-packages/torch/nn/modules/linear.py:125:0\n",
      "  %/transformer/Add_output_0 : Float(1, 100, 20, strides=[2000, 1, 100], requires_grad=1, device=cpu) = onnx::Add[onnx_name=\"/transformer/Add\"](%/Transpose_output_0, %/transformer/attention/out_proj/Add_output_0), scope: __main__.Test4::/__main__.TransformerEncoderLayer::transformer # /tmp/ipykernel_601266/3680961281.py:55:0\n",
      "  %/transformer/final_layer_norm/ReduceMean_output_0 : Float(1, 100, 1, strides=[100, 1, 1], device=cpu) = onnx::ReduceMean[axes=[-1], onnx_name=\"/transformer/final_layer_norm/ReduceMean\"](%/transformer/Add_output_0), scope: __main__.Test4::/__main__.TransformerEncoderLayer::transformer/torch.nn.modules.normalization.LayerNorm::final_layer_norm # /data/proj/voice/pyvoice_venv/lib/python3.10/site-packages/torch/nn/functional.py:2910:0\n",
      "  %/transformer/final_layer_norm/Sub_output_0 : Float(1, 100, 20, strides=[2000, 20, 1], device=cpu) = onnx::Sub[onnx_name=\"/transformer/final_layer_norm/Sub\"](%/transformer/Add_output_0, %/transformer/final_layer_norm/ReduceMean_output_0), scope: __main__.Test4::/__main__.TransformerEncoderLayer::transformer/torch.nn.modules.normalization.LayerNorm::final_layer_norm # /data/proj/voice/pyvoice_venv/lib/python3.10/site-packages/torch/nn/functional.py:2910:0\n",
      "  %/transformer/final_layer_norm/Constant_output_0 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={2}, onnx_name=\"/transformer/final_layer_norm/Constant\"](), scope: __main__.Test4::/__main__.TransformerEncoderLayer::transformer/torch.nn.modules.normalization.LayerNorm::final_layer_norm # /data/proj/voice/pyvoice_venv/lib/python3.10/site-packages/torch/nn/functional.py:2910:0\n",
      "  %/transformer/final_layer_norm/Pow_output_0 : Float(1, 100, 20, strides=[2000, 20, 1], device=cpu) = onnx::Pow[onnx_name=\"/transformer/final_layer_norm/Pow\"](%/transformer/final_layer_norm/Sub_output_0, %/transformer/final_layer_norm/Constant_output_0), scope: __main__.Test4::/__main__.TransformerEncoderLayer::transformer/torch.nn.modules.normalization.LayerNorm::final_layer_norm # /data/proj/voice/pyvoice_venv/lib/python3.10/site-packages/torch/nn/functional.py:2910:0\n",
      "  %/transformer/final_layer_norm/ReduceMean_1_output_0 : Float(1, 100, 1, strides=[100, 1, 1], device=cpu) = onnx::ReduceMean[axes=[-1], onnx_name=\"/transformer/final_layer_norm/ReduceMean_1\"](%/transformer/final_layer_norm/Pow_output_0), scope: __main__.Test4::/__main__.TransformerEncoderLayer::transformer/torch.nn.modules.normalization.LayerNorm::final_layer_norm # /data/proj/voice/pyvoice_venv/lib/python3.10/site-packages/torch/nn/functional.py:2910:0\n",
      "  %/transformer/final_layer_norm/Constant_1_output_0 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={1e-06}, onnx_name=\"/transformer/final_layer_norm/Constant_1\"](), scope: __main__.Test4::/__main__.TransformerEncoderLayer::transformer/torch.nn.modules.normalization.LayerNorm::final_layer_norm # /data/proj/voice/pyvoice_venv/lib/python3.10/site-packages/torch/nn/functional.py:2910:0\n",
      "  %/transformer/final_layer_norm/Add_output_0 : Float(1, 100, 1, strides=[100, 1, 1], device=cpu) = onnx::Add[onnx_name=\"/transformer/final_layer_norm/Add\"](%/transformer/final_layer_norm/ReduceMean_1_output_0, %/transformer/final_layer_norm/Constant_1_output_0), scope: __main__.Test4::/__main__.TransformerEncoderLayer::transformer/torch.nn.modules.normalization.LayerNorm::final_layer_norm # /data/proj/voice/pyvoice_venv/lib/python3.10/site-packages/torch/nn/functional.py:2910:0\n",
      "  %/transformer/final_layer_norm/Sqrt_output_0 : Float(1, 100, 1, strides=[100, 1, 1], device=cpu) = onnx::Sqrt[onnx_name=\"/transformer/final_layer_norm/Sqrt\"](%/transformer/final_layer_norm/Add_output_0), scope: __main__.Test4::/__main__.TransformerEncoderLayer::transformer/torch.nn.modules.normalization.LayerNorm::final_layer_norm # /data/proj/voice/pyvoice_venv/lib/python3.10/site-packages/torch/nn/functional.py:2910:0\n",
      "  %/transformer/final_layer_norm/Div_output_0 : Float(1, 100, 20, strides=[2000, 20, 1], device=cpu) = onnx::Div[onnx_name=\"/transformer/final_layer_norm/Div\"](%/transformer/final_layer_norm/Sub_output_0, %/transformer/final_layer_norm/Sqrt_output_0), scope: __main__.Test4::/__main__.TransformerEncoderLayer::transformer/torch.nn.modules.normalization.LayerNorm::final_layer_norm # /data/proj/voice/pyvoice_venv/lib/python3.10/site-packages/torch/nn/functional.py:2910:0\n",
      "  %/transformer/final_layer_norm/Mul_output_0 : Float(1, 100, 20, strides=[2000, 20, 1], device=cpu) = onnx::Mul[onnx_name=\"/transformer/final_layer_norm/Mul\"](%/transformer/final_layer_norm/Div_output_0, %transformer.final_layer_norm.weight), scope: __main__.Test4::/__main__.TransformerEncoderLayer::transformer/torch.nn.modules.normalization.LayerNorm::final_layer_norm # /data/proj/voice/pyvoice_venv/lib/python3.10/site-packages/torch/nn/functional.py:2910:0\n",
      "  %/transformer/final_layer_norm/Add_1_output_0 : Float(1, 100, 20, strides=[2000, 20, 1], requires_grad=1, device=cpu) = onnx::Add[onnx_name=\"/transformer/final_layer_norm/Add_1\"](%/transformer/final_layer_norm/Mul_output_0, %transformer.final_layer_norm.bias), scope: __main__.Test4::/__main__.TransformerEncoderLayer::transformer/torch.nn.modules.normalization.LayerNorm::final_layer_norm # /data/proj/voice/pyvoice_venv/lib/python3.10/site-packages/torch/nn/functional.py:2910:0\n",
      "  %/transformer/feed_forward/intermediate_dense/MatMul_output_0 : Float(1, 100, 20, strides=[2000, 20, 1], device=cpu) = onnx::MatMul[onnx_name=\"/transformer/feed_forward/intermediate_dense/MatMul\"](%/transformer/final_layer_norm/Add_1_output_0, %onnx::MatMul_152), scope: __main__.Test4::/__main__.TransformerEncoderLayer::transformer/__main__.FeedForward::feed_forward/torch.nn.modules.linear.Linear::intermediate_dense # /data/proj/voice/pyvoice_venv/lib/python3.10/site-packages/torch/nn/modules/linear.py:125:0\n",
      "  %/transformer/feed_forward/intermediate_dense/Add_output_0 : Float(1, 100, 20, strides=[2000, 20, 1], requires_grad=1, device=cpu) = onnx::Add[onnx_name=\"/transformer/feed_forward/intermediate_dense/Add\"](%transformer.feed_forward.intermediate_dense.bias, %/transformer/feed_forward/intermediate_dense/MatMul_output_0), scope: __main__.Test4::/__main__.TransformerEncoderLayer::transformer/__main__.FeedForward::feed_forward/torch.nn.modules.linear.Linear::intermediate_dense # /data/proj/voice/pyvoice_venv/lib/python3.10/site-packages/torch/nn/modules/linear.py:125:0\n",
      "  %/transformer/feed_forward/intermediate_act_fn/Constant_output_0 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={1.41421}, onnx_name=\"/transformer/feed_forward/intermediate_act_fn/Constant\"](), scope: __main__.Test4::/__main__.TransformerEncoderLayer::transformer/__main__.FeedForward::feed_forward/__main__.NewGELUActivation::intermediate_act_fn # /tmp/ipykernel_601266/3680961281.py:3:0\n",
      "  %/transformer/feed_forward/intermediate_act_fn/Div_output_0 : Float(1, 100, 20, strides=[2000, 20, 1], device=cpu) = onnx::Div[onnx_name=\"/transformer/feed_forward/intermediate_act_fn/Div\"](%/transformer/feed_forward/intermediate_dense/Add_output_0, %/transformer/feed_forward/intermediate_act_fn/Constant_output_0), scope: __main__.Test4::/__main__.TransformerEncoderLayer::transformer/__main__.FeedForward::feed_forward/__main__.NewGELUActivation::intermediate_act_fn # /tmp/ipykernel_601266/3680961281.py:3:0\n",
      "  %/transformer/feed_forward/intermediate_act_fn/Erf_output_0 : Float(1, 100, 20, strides=[2000, 20, 1], device=cpu) = onnx::Erf[onnx_name=\"/transformer/feed_forward/intermediate_act_fn/Erf\"](%/transformer/feed_forward/intermediate_act_fn/Div_output_0), scope: __main__.Test4::/__main__.TransformerEncoderLayer::transformer/__main__.FeedForward::feed_forward/__main__.NewGELUActivation::intermediate_act_fn # /tmp/ipykernel_601266/3680961281.py:3:0\n",
      "  %/transformer/feed_forward/intermediate_act_fn/Constant_1_output_0 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={1}, onnx_name=\"/transformer/feed_forward/intermediate_act_fn/Constant_1\"](), scope: __main__.Test4::/__main__.TransformerEncoderLayer::transformer/__main__.FeedForward::feed_forward/__main__.NewGELUActivation::intermediate_act_fn # /tmp/ipykernel_601266/3680961281.py:3:0\n",
      "  %/transformer/feed_forward/intermediate_act_fn/Add_output_0 : Float(1, 100, 20, strides=[2000, 20, 1], device=cpu) = onnx::Add[onnx_name=\"/transformer/feed_forward/intermediate_act_fn/Add\"](%/transformer/feed_forward/intermediate_act_fn/Erf_output_0, %/transformer/feed_forward/intermediate_act_fn/Constant_1_output_0), scope: __main__.Test4::/__main__.TransformerEncoderLayer::transformer/__main__.FeedForward::feed_forward/__main__.NewGELUActivation::intermediate_act_fn # /tmp/ipykernel_601266/3680961281.py:3:0\n",
      "  %/transformer/feed_forward/intermediate_act_fn/Mul_output_0 : Float(1, 100, 20, strides=[2000, 20, 1], device=cpu) = onnx::Mul[onnx_name=\"/transformer/feed_forward/intermediate_act_fn/Mul\"](%/transformer/feed_forward/intermediate_dense/Add_output_0, %/transformer/feed_forward/intermediate_act_fn/Add_output_0), scope: __main__.Test4::/__main__.TransformerEncoderLayer::transformer/__main__.FeedForward::feed_forward/__main__.NewGELUActivation::intermediate_act_fn # /tmp/ipykernel_601266/3680961281.py:3:0\n",
      "  %/transformer/feed_forward/intermediate_act_fn/Constant_2_output_0 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={0.5}, onnx_name=\"/transformer/feed_forward/intermediate_act_fn/Constant_2\"](), scope: __main__.Test4::/__main__.TransformerEncoderLayer::transformer/__main__.FeedForward::feed_forward/__main__.NewGELUActivation::intermediate_act_fn # /tmp/ipykernel_601266/3680961281.py:3:0\n",
      "  %/transformer/feed_forward/intermediate_act_fn/Mul_1_output_0 : Float(1, 100, 20, strides=[2000, 20, 1], requires_grad=1, device=cpu) = onnx::Mul[onnx_name=\"/transformer/feed_forward/intermediate_act_fn/Mul_1\"](%/transformer/feed_forward/intermediate_act_fn/Mul_output_0, %/transformer/feed_forward/intermediate_act_fn/Constant_2_output_0), scope: __main__.Test4::/__main__.TransformerEncoderLayer::transformer/__main__.FeedForward::feed_forward/__main__.NewGELUActivation::intermediate_act_fn # /tmp/ipykernel_601266/3680961281.py:3:0\n",
      "  %/transformer/feed_forward/output_dense/MatMul_output_0 : Float(1, 100, 20, strides=[2000, 20, 1], device=cpu) = onnx::MatMul[onnx_name=\"/transformer/feed_forward/output_dense/MatMul\"](%/transformer/feed_forward/intermediate_act_fn/Mul_1_output_0, %onnx::MatMul_153), scope: __main__.Test4::/__main__.TransformerEncoderLayer::transformer/__main__.FeedForward::feed_forward/torch.nn.modules.linear.Linear::output_dense # /data/proj/voice/pyvoice_venv/lib/python3.10/site-packages/torch/nn/modules/linear.py:125:0\n",
      "  %/transformer/feed_forward/output_dense/Add_output_0 : Float(1, 100, 20, strides=[2000, 20, 1], requires_grad=1, device=cpu) = onnx::Add[onnx_name=\"/transformer/feed_forward/output_dense/Add\"](%transformer.feed_forward.output_dense.bias, %/transformer/feed_forward/output_dense/MatMul_output_0), scope: __main__.Test4::/__main__.TransformerEncoderLayer::transformer/__main__.FeedForward::feed_forward/torch.nn.modules.linear.Linear::output_dense # /data/proj/voice/pyvoice_venv/lib/python3.10/site-packages/torch/nn/modules/linear.py:125:0\n",
      "  %/transformer/Add_1_output_0 : Float(1, 100, 20, strides=[2000, 1, 100], requires_grad=1, device=cpu) = onnx::Add[onnx_name=\"/transformer/Add_1\"](%/transformer/Add_output_0, %/transformer/feed_forward/output_dense/Add_output_0), scope: __main__.Test4::/__main__.TransformerEncoderLayer::transformer # /tmp/ipykernel_601266/3680961281.py:56:0\n",
      "  %/Transpose_1_output_0 : Float(1, 20, 100, strides=[2000, 100, 1], requires_grad=1, device=cpu) = onnx::Transpose[perm=[0, 2, 1], onnx_name=\"/Transpose_1\"](%/transformer/Add_1_output_0), scope: __main__.Test4:: # /tmp/ipykernel_601266/3680961281.py:72:0\n",
      "  %output : Float(1, 600, 100, strides=[60000, 100, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1], group=1, kernel_shape=[1], pads=[0, 0], strides=[1], onnx_name=\"/expand/Conv\"](%/Transpose_1_output_0, %expand.weight, %expand.bias), scope: __main__.Test4::/torch.nn.modules.conv.Conv1d::expand # /data/proj/voice/pyvoice_venv/lib/python3.10/site-packages/torch/nn/modules/conv.py:370:0\n",
      "  return (%output)\n",
      "\n",
      "Exported to test4_fail.onnx\n"
     ]
    }
   ],
   "source": [
    "model = Test4()\n",
    "model.eval()\n",
    "dummy = torch.randn(1, 600, 100)  # [B, C, T]\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    dummy,\n",
    "    \"test4_fail.onnx\",\n",
    "    input_names=[\"input\"],\n",
    "    output_names=[\"output\"],\n",
    "    opset_version=13,\n",
    "    verbose=True\n",
    ")\n",
    "print(\"Exported to test4_fail.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyvoice_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
