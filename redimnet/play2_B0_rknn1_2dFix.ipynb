{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ReDimNetNoMel Disable bad layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0+cu124\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "from torchsummary import summary\n",
    "\n",
    "\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from math import floor\n",
    "import copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /data/deep/redimnet/models/IDRnD_ReDimNet_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/deep/redimnet/models/IDRnD_ReDimNet_master\n",
      "load_res : <All keys matched successfully>\n"
     ]
    }
   ],
   "source": [
    "model_name='B0'\n",
    "# train_type='ft_lm'\n",
    "train_type='ptn'\n",
    "dataset='vox2'\n",
    "\n",
    "torch.hub.set_dir('/data/deep/redimnet/models')\n",
    "\n",
    "original_model = torch.hub.load('IDRnD/ReDimNet', 'ReDimNet', \n",
    "                       model_name=model_name, \n",
    "                       train_type=train_type, \n",
    "                       dataset=dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*  ReDimNetWrap expects raw 16 kHz mono audio, exactly 32 000 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/deep/redimnet/models/IDRnD_ReDimNet_master/redimnet/layers/features.py:137: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=False):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==============================================================================================================\n",
       "Layer (type:depth-idx)                                       Output Shape              Param #\n",
       "==============================================================================================================\n",
       "ReDimNetWrap                                                 [1, 192]                  --\n",
       "├─MelBanks: 1-1                                              [1, 60, 134]              --\n",
       "│    └─Sequential: 2-1                                       [1, 60, 134]              --\n",
       "│    │    └─Identity: 3-1                                    [1, 32000]                --\n",
       "│    │    └─PreEmphasis: 3-2                                 [1, 32000]                --\n",
       "│    │    └─MelSpectrogram: 3-3                              [1, 60, 134]              --\n",
       "├─ReDimNet: 1-2                                              [1, 600, 134]             --\n",
       "│    └─Sequential: 2-2                                       [1, 600, 134]             --\n",
       "│    │    └─Conv2d: 3-4                                      [1, 10, 60, 134]          100\n",
       "│    │    └─LayerNorm: 3-5                                   [1, 10, 60, 134]          20\n",
       "│    │    └─to1d: 3-6                                        [1, 600, 134]             --\n",
       "│    └─Sequential: 2-3                                       [1, 600, 134]             --\n",
       "│    │    └─weigth1d: 3-7                                    [1, 600, 134]             (1)\n",
       "│    │    └─to2d: 3-8                                        [1, 10, 60, 134]          --\n",
       "│    │    └─Conv2d: 3-9                                      [1, 10, 60, 134]          110\n",
       "│    │    └─ConvBlock2d: 3-10                                [1, 10, 60, 134]          440\n",
       "│    │    └─ConvBlock2d: 3-11                                [1, 10, 60, 134]          440\n",
       "│    │    └─to1d: 3-12                                       [1, 600, 134]             --\n",
       "│    │    └─TimeContextBlock1d: 3-13                         [1, 600, 134]             31,500\n",
       "│    └─Sequential: 2-4                                       [1, 600, 134]             --\n",
       "│    │    └─weigth1d: 3-14                                   [1, 600, 134]             1,200\n",
       "│    │    └─to2d: 3-15                                       [1, 10, 60, 134]          --\n",
       "│    │    └─Conv2d: 3-16                                     [1, 40, 30, 134]          840\n",
       "│    │    └─ConvBlock2d: 3-17                                [1, 40, 30, 134]          4,160\n",
       "│    │    └─ConvBlock2d: 3-18                                [1, 40, 30, 134]          4,160\n",
       "│    │    └─ConvBlock2d: 3-19                                [1, 40, 30, 134]          4,160\n",
       "│    │    └─Sequential: 3-20                                 [1, 20, 30, 134]          840\n",
       "│    │    └─to1d: 3-21                                       [1, 600, 134]             --\n",
       "│    │    └─TimeContextBlock1d: 3-22                         [1, 600, 134]             31,500\n",
       "│    └─Sequential: 2-5                                       [1, 600, 134]             --\n",
       "│    │    └─weigth1d: 3-23                                   [1, 600, 134]             1,800\n",
       "│    │    └─to2d: 3-24                                       [1, 20, 30, 134]          --\n",
       "│    │    └─Conv2d: 3-25                                     [1, 60, 30, 134]          1,260\n",
       "│    │    └─ConvBlock2d: 3-26                                [1, 60, 30, 134]          8,640\n",
       "│    │    └─ConvBlock2d: 3-27                                [1, 60, 30, 134]          8,640\n",
       "│    │    └─ConvBlock2d: 3-28                                [1, 60, 30, 134]          8,640\n",
       "│    │    └─Sequential: 3-29                                 [1, 20, 30, 134]          1,020\n",
       "│    │    └─to1d: 3-30                                       [1, 600, 134]             --\n",
       "│    │    └─TimeContextBlock1d: 3-31                         [1, 600, 134]             31,500\n",
       "│    └─Sequential: 2-6                                       [1, 600, 134]             --\n",
       "│    │    └─weigth1d: 3-32                                   [1, 600, 134]             2,400\n",
       "│    │    └─to2d: 3-33                                       [1, 20, 30, 134]          --\n",
       "│    │    └─Conv2d: 3-34                                     [1, 80, 15, 134]          3,280\n",
       "│    │    └─ConvBlock2d: 3-35                                [1, 80, 15, 134]          14,720\n",
       "│    │    └─ConvBlock2d: 3-36                                [1, 80, 15, 134]          14,720\n",
       "│    │    └─ConvBlock2d: 3-37                                [1, 80, 15, 134]          14,720\n",
       "│    │    └─ConvBlock2d: 3-38                                [1, 80, 15, 134]          14,720\n",
       "│    │    └─Sequential: 3-39                                 [1, 40, 15, 134]          2,480\n",
       "│    │    └─to1d: 3-40                                       [1, 600, 134]             --\n",
       "│    │    └─TimeContextBlock1d: 3-41                         [1, 600, 134]             117,300\n",
       "│    └─Sequential: 2-7                                       [1, 600, 134]             --\n",
       "│    │    └─weigth1d: 3-42                                   [1, 600, 134]             3,000\n",
       "│    │    └─to2d: 3-43                                       [1, 40, 15, 134]          --\n",
       "│    │    └─Conv2d: 3-44                                     [1, 40, 15, 134]          1,640\n",
       "│    │    └─ConvBlock2d: 3-45                                [1, 40, 15, 134]          4,160\n",
       "│    │    └─ConvBlock2d: 3-46                                [1, 40, 15, 134]          4,160\n",
       "│    │    └─ConvBlock2d: 3-47                                [1, 40, 15, 134]          4,160\n",
       "│    │    └─to1d: 3-48                                       [1, 600, 134]             --\n",
       "│    │    └─TimeContextBlock1d: 3-49                         [1, 600, 134]             117,300\n",
       "│    └─weigth1d: 2-8                                         [1, 600, 134]             3,600\n",
       "│    └─Identity: 2-9                                         [1, 600, 134]             --\n",
       "│    └─Identity: 2-10                                        [1, 600, 134]             --\n",
       "├─ASTP: 1-3                                                  [1, 1200]                 --\n",
       "│    └─Conv1d: 2-11                                          [1, 128, 134]             230,528\n",
       "│    └─Conv1d: 2-12                                          [1, 600, 134]             77,400\n",
       "├─BatchNorm1d: 1-4                                           [1, 1200]                 2,400\n",
       "├─Linear: 1-5                                                [1, 192]                  230,592\n",
       "==============================================================================================================\n",
       "Total params: 1,004,251\n",
       "Trainable params: 1,004,250\n",
       "Non-trainable params: 1\n",
       "Total mult-adds (M): 406.29\n",
       "==============================================================================================================\n",
       "Input size (MB): 0.13\n",
       "Forward/backward pass size (MB): 133.03\n",
       "Params size (MB): 4.02\n",
       "Estimated Total Size (MB): 137.18\n",
       "=============================================================================================================="
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "summary(original_model, input_size=(1, 32000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can see MelSpectrogram inside the model ; lets take it outside the model;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "backbone => ReDimNet(\n",
      "  (stem): Sequential(\n",
      "    (0): Conv2d(1, 10, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (1): LayerNorm(C=(10,), data_format=channels_first, eps=1e-06)\n",
      "    (2): to1d()\n",
      "  )\n",
      "  (stage0): Sequential(\n",
      "    (0): weigth1d(w=(1, 1, 1, 1),sequential=False)\n",
      "    (1): to2d(f=60,c=10)\n",
      "    (2): Conv2d(10, 10, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (3): ConvBlock2d(\n",
      "      (conv_block): ResBasicBlock(\n",
      "        (conv1): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=10, bias=False)\n",
      "        (conv1pw): Conv2d(10, 10, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=10, bias=False)\n",
      "        (conv2pw): Conv2d(10, 10, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (se): Identity()\n",
      "        (downsample): Identity()\n",
      "      )\n",
      "    )\n",
      "    (4): ConvBlock2d(\n",
      "      (conv_block): ResBasicBlock(\n",
      "        (conv1): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=10, bias=False)\n",
      "        (conv1pw): Conv2d(10, 10, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=10, bias=False)\n",
      "        (conv2pw): Conv2d(10, 10, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (se): Identity()\n",
      "        (downsample): Identity()\n",
      "      )\n",
      "    )\n",
      "    (5): to1d()\n",
      "    (6): TimeContextBlock1d(\n",
      "      (red_dim_conv): Sequential(\n",
      "        (0): Conv1d(600, 20, kernel_size=(1,), stride=(1,))\n",
      "        (1): LayerNorm(C=(20,), data_format=channels_first, eps=1e-06)\n",
      "      )\n",
      "      (tcm): Sequential(\n",
      "        (0): ConvNeXtLikeBlock(\n",
      "          (dwconvs): ModuleList(\n",
      "            (0): Conv1d(20, 20, kernel_size=(7,), stride=(1,), padding=same, groups=20)\n",
      "          )\n",
      "          (norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (pwconv1): Conv1d(20, 20, kernel_size=(1,), stride=(1,))\n",
      "        )\n",
      "        (1): ConvNeXtLikeBlock(\n",
      "          (dwconvs): ModuleList(\n",
      "            (0): Conv1d(20, 20, kernel_size=(19,), stride=(1,), padding=same, groups=20)\n",
      "          )\n",
      "          (norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (pwconv1): Conv1d(20, 20, kernel_size=(1,), stride=(1,))\n",
      "        )\n",
      "        (2): ConvNeXtLikeBlock(\n",
      "          (dwconvs): ModuleList(\n",
      "            (0): Conv1d(20, 20, kernel_size=(31,), stride=(1,), padding=same, groups=20)\n",
      "          )\n",
      "          (norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (pwconv1): Conv1d(20, 20, kernel_size=(1,), stride=(1,))\n",
      "        )\n",
      "        (3): ConvNeXtLikeBlock(\n",
      "          (dwconvs): ModuleList(\n",
      "            (0): Conv1d(20, 20, kernel_size=(59,), stride=(1,), padding=same, groups=20)\n",
      "          )\n",
      "          (norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (pwconv1): Conv1d(20, 20, kernel_size=(1,), stride=(1,))\n",
      "        )\n",
      "        (4): TransformerEncoderLayer(\n",
      "          (attention): MultiHeadAttention(\n",
      "            (k_proj): Linear(in_features=20, out_features=20, bias=True)\n",
      "            (v_proj): Linear(in_features=20, out_features=20, bias=True)\n",
      "            (q_proj): Linear(in_features=20, out_features=20, bias=True)\n",
      "            (out_proj): Linear(in_features=20, out_features=20, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm((20,), eps=1e-06, elementwise_affine=True)\n",
      "          (feed_forward): FeedForward(\n",
      "            (intermediate_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (intermediate_dense): Linear(in_features=20, out_features=20, bias=True)\n",
      "            (intermediate_act_fn): NewGELUActivation()\n",
      "            (output_dense): Linear(in_features=20, out_features=20, bias=True)\n",
      "            (output_dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (final_layer_norm): LayerNorm((20,), eps=1e-06, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (exp_dim_conv): Conv1d(20, 600, kernel_size=(1,), stride=(1,))\n",
      "    )\n",
      "  )\n",
      "  (stage1): Sequential(\n",
      "    (0): weigth1d(w=(1, 2, 600, 1),sequential=False)\n",
      "    (1): to2d(f=60,c=10)\n",
      "    (2): Conv2d(10, 40, kernel_size=(2, 1), stride=(2, 1))\n",
      "    (3): ConvBlock2d(\n",
      "      (conv_block): ResBasicBlock(\n",
      "        (conv1): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=40, bias=False)\n",
      "        (conv1pw): Conv2d(40, 40, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (bn1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=40, bias=False)\n",
      "        (conv2pw): Conv2d(40, 40, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (bn2): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (se): Identity()\n",
      "        (downsample): Identity()\n",
      "      )\n",
      "    )\n",
      "    (4): ConvBlock2d(\n",
      "      (conv_block): ResBasicBlock(\n",
      "        (conv1): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=40, bias=False)\n",
      "        (conv1pw): Conv2d(40, 40, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (bn1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=40, bias=False)\n",
      "        (conv2pw): Conv2d(40, 40, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (bn2): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (se): Identity()\n",
      "        (downsample): Identity()\n",
      "      )\n",
      "    )\n",
      "    (5): ConvBlock2d(\n",
      "      (conv_block): ResBasicBlock(\n",
      "        (conv1): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=40, bias=False)\n",
      "        (conv1pw): Conv2d(40, 40, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (bn1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=40, bias=False)\n",
      "        (conv2pw): Conv2d(40, 40, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (bn2): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (se): Identity()\n",
      "        (downsample): Identity()\n",
      "      )\n",
      "    )\n",
      "    (6): Sequential(\n",
      "      (0): Conv2d(40, 20, kernel_size=(3, 3), stride=(1, 1), padding=same, groups=20)\n",
      "      (1): BatchNorm2d(20, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): GELU(approximate='none')\n",
      "      (3): Conv2d(20, 20, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (7): to1d()\n",
      "    (8): TimeContextBlock1d(\n",
      "      (red_dim_conv): Sequential(\n",
      "        (0): Conv1d(600, 20, kernel_size=(1,), stride=(1,))\n",
      "        (1): LayerNorm(C=(20,), data_format=channels_first, eps=1e-06)\n",
      "      )\n",
      "      (tcm): Sequential(\n",
      "        (0): ConvNeXtLikeBlock(\n",
      "          (dwconvs): ModuleList(\n",
      "            (0): Conv1d(20, 20, kernel_size=(7,), stride=(1,), padding=same, groups=20)\n",
      "          )\n",
      "          (norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (pwconv1): Conv1d(20, 20, kernel_size=(1,), stride=(1,))\n",
      "        )\n",
      "        (1): ConvNeXtLikeBlock(\n",
      "          (dwconvs): ModuleList(\n",
      "            (0): Conv1d(20, 20, kernel_size=(19,), stride=(1,), padding=same, groups=20)\n",
      "          )\n",
      "          (norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (pwconv1): Conv1d(20, 20, kernel_size=(1,), stride=(1,))\n",
      "        )\n",
      "        (2): ConvNeXtLikeBlock(\n",
      "          (dwconvs): ModuleList(\n",
      "            (0): Conv1d(20, 20, kernel_size=(31,), stride=(1,), padding=same, groups=20)\n",
      "          )\n",
      "          (norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (pwconv1): Conv1d(20, 20, kernel_size=(1,), stride=(1,))\n",
      "        )\n",
      "        (3): ConvNeXtLikeBlock(\n",
      "          (dwconvs): ModuleList(\n",
      "            (0): Conv1d(20, 20, kernel_size=(59,), stride=(1,), padding=same, groups=20)\n",
      "          )\n",
      "          (norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (pwconv1): Conv1d(20, 20, kernel_size=(1,), stride=(1,))\n",
      "        )\n",
      "        (4): TransformerEncoderLayer(\n",
      "          (attention): MultiHeadAttention(\n",
      "            (k_proj): Linear(in_features=20, out_features=20, bias=True)\n",
      "            (v_proj): Linear(in_features=20, out_features=20, bias=True)\n",
      "            (q_proj): Linear(in_features=20, out_features=20, bias=True)\n",
      "            (out_proj): Linear(in_features=20, out_features=20, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm((20,), eps=1e-06, elementwise_affine=True)\n",
      "          (feed_forward): FeedForward(\n",
      "            (intermediate_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (intermediate_dense): Linear(in_features=20, out_features=20, bias=True)\n",
      "            (intermediate_act_fn): NewGELUActivation()\n",
      "            (output_dense): Linear(in_features=20, out_features=20, bias=True)\n",
      "            (output_dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (final_layer_norm): LayerNorm((20,), eps=1e-06, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (exp_dim_conv): Conv1d(20, 600, kernel_size=(1,), stride=(1,))\n",
      "    )\n",
      "  )\n",
      "  (stage2): Sequential(\n",
      "    (0): weigth1d(w=(1, 3, 600, 1),sequential=False)\n",
      "    (1): to2d(f=30,c=20)\n",
      "    (2): Conv2d(20, 60, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (3): ConvBlock2d(\n",
      "      (conv_block): ResBasicBlock(\n",
      "        (conv1): Conv2d(60, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=60, bias=False)\n",
      "        (conv1pw): Conv2d(60, 60, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (bn1): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(60, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=60, bias=False)\n",
      "        (conv2pw): Conv2d(60, 60, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (bn2): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (se): Identity()\n",
      "        (downsample): Identity()\n",
      "      )\n",
      "    )\n",
      "    (4): ConvBlock2d(\n",
      "      (conv_block): ResBasicBlock(\n",
      "        (conv1): Conv2d(60, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=60, bias=False)\n",
      "        (conv1pw): Conv2d(60, 60, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (bn1): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(60, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=60, bias=False)\n",
      "        (conv2pw): Conv2d(60, 60, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (bn2): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (se): Identity()\n",
      "        (downsample): Identity()\n",
      "      )\n",
      "    )\n",
      "    (5): ConvBlock2d(\n",
      "      (conv_block): ResBasicBlock(\n",
      "        (conv1): Conv2d(60, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=60, bias=False)\n",
      "        (conv1pw): Conv2d(60, 60, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (bn1): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(60, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=60, bias=False)\n",
      "        (conv2pw): Conv2d(60, 60, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (bn2): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (se): Identity()\n",
      "        (downsample): Identity()\n",
      "      )\n",
      "    )\n",
      "    (6): Sequential(\n",
      "      (0): Conv2d(60, 20, kernel_size=(3, 3), stride=(1, 1), padding=same, groups=20)\n",
      "      (1): BatchNorm2d(20, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): GELU(approximate='none')\n",
      "      (3): Conv2d(20, 20, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (7): to1d()\n",
      "    (8): TimeContextBlock1d(\n",
      "      (red_dim_conv): Sequential(\n",
      "        (0): Conv1d(600, 20, kernel_size=(1,), stride=(1,))\n",
      "        (1): LayerNorm(C=(20,), data_format=channels_first, eps=1e-06)\n",
      "      )\n",
      "      (tcm): Sequential(\n",
      "        (0): ConvNeXtLikeBlock(\n",
      "          (dwconvs): ModuleList(\n",
      "            (0): Conv1d(20, 20, kernel_size=(7,), stride=(1,), padding=same, groups=20)\n",
      "          )\n",
      "          (norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (pwconv1): Conv1d(20, 20, kernel_size=(1,), stride=(1,))\n",
      "        )\n",
      "        (1): ConvNeXtLikeBlock(\n",
      "          (dwconvs): ModuleList(\n",
      "            (0): Conv1d(20, 20, kernel_size=(19,), stride=(1,), padding=same, groups=20)\n",
      "          )\n",
      "          (norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (pwconv1): Conv1d(20, 20, kernel_size=(1,), stride=(1,))\n",
      "        )\n",
      "        (2): ConvNeXtLikeBlock(\n",
      "          (dwconvs): ModuleList(\n",
      "            (0): Conv1d(20, 20, kernel_size=(31,), stride=(1,), padding=same, groups=20)\n",
      "          )\n",
      "          (norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (pwconv1): Conv1d(20, 20, kernel_size=(1,), stride=(1,))\n",
      "        )\n",
      "        (3): ConvNeXtLikeBlock(\n",
      "          (dwconvs): ModuleList(\n",
      "            (0): Conv1d(20, 20, kernel_size=(59,), stride=(1,), padding=same, groups=20)\n",
      "          )\n",
      "          (norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (pwconv1): Conv1d(20, 20, kernel_size=(1,), stride=(1,))\n",
      "        )\n",
      "        (4): TransformerEncoderLayer(\n",
      "          (attention): MultiHeadAttention(\n",
      "            (k_proj): Linear(in_features=20, out_features=20, bias=True)\n",
      "            (v_proj): Linear(in_features=20, out_features=20, bias=True)\n",
      "            (q_proj): Linear(in_features=20, out_features=20, bias=True)\n",
      "            (out_proj): Linear(in_features=20, out_features=20, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm((20,), eps=1e-06, elementwise_affine=True)\n",
      "          (feed_forward): FeedForward(\n",
      "            (intermediate_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (intermediate_dense): Linear(in_features=20, out_features=20, bias=True)\n",
      "            (intermediate_act_fn): NewGELUActivation()\n",
      "            (output_dense): Linear(in_features=20, out_features=20, bias=True)\n",
      "            (output_dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (final_layer_norm): LayerNorm((20,), eps=1e-06, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (exp_dim_conv): Conv1d(20, 600, kernel_size=(1,), stride=(1,))\n",
      "    )\n",
      "  )\n",
      "  (stage3): Sequential(\n",
      "    (0): weigth1d(w=(1, 4, 600, 1),sequential=False)\n",
      "    (1): to2d(f=30,c=20)\n",
      "    (2): Conv2d(20, 80, kernel_size=(2, 1), stride=(2, 1))\n",
      "    (3): ConvBlock2d(\n",
      "      (conv_block): ResBasicBlock(\n",
      "        (conv1): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=80, bias=False)\n",
      "        (conv1pw): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (bn1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=80, bias=False)\n",
      "        (conv2pw): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (bn2): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (se): Identity()\n",
      "        (downsample): Identity()\n",
      "      )\n",
      "    )\n",
      "    (4): ConvBlock2d(\n",
      "      (conv_block): ResBasicBlock(\n",
      "        (conv1): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=80, bias=False)\n",
      "        (conv1pw): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (bn1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=80, bias=False)\n",
      "        (conv2pw): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (bn2): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (se): Identity()\n",
      "        (downsample): Identity()\n",
      "      )\n",
      "    )\n",
      "    (5): ConvBlock2d(\n",
      "      (conv_block): ResBasicBlock(\n",
      "        (conv1): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=80, bias=False)\n",
      "        (conv1pw): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (bn1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=80, bias=False)\n",
      "        (conv2pw): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (bn2): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (se): Identity()\n",
      "        (downsample): Identity()\n",
      "      )\n",
      "    )\n",
      "    (6): ConvBlock2d(\n",
      "      (conv_block): ResBasicBlock(\n",
      "        (conv1): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=80, bias=False)\n",
      "        (conv1pw): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (bn1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=80, bias=False)\n",
      "        (conv2pw): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (bn2): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (se): Identity()\n",
      "        (downsample): Identity()\n",
      "      )\n",
      "    )\n",
      "    (7): Sequential(\n",
      "      (0): Conv2d(80, 40, kernel_size=(3, 3), stride=(1, 1), padding=same, groups=40)\n",
      "      (1): BatchNorm2d(40, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): GELU(approximate='none')\n",
      "      (3): Conv2d(40, 40, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (8): to1d()\n",
      "    (9): TimeContextBlock1d(\n",
      "      (red_dim_conv): Sequential(\n",
      "        (0): Conv1d(600, 60, kernel_size=(1,), stride=(1,))\n",
      "        (1): LayerNorm(C=(60,), data_format=channels_first, eps=1e-06)\n",
      "      )\n",
      "      (tcm): Sequential(\n",
      "        (0): ConvNeXtLikeBlock(\n",
      "          (dwconvs): ModuleList(\n",
      "            (0): Conv1d(60, 60, kernel_size=(7,), stride=(1,), padding=same, groups=60)\n",
      "          )\n",
      "          (norm): BatchNorm1d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (pwconv1): Conv1d(60, 60, kernel_size=(1,), stride=(1,))\n",
      "        )\n",
      "        (1): ConvNeXtLikeBlock(\n",
      "          (dwconvs): ModuleList(\n",
      "            (0): Conv1d(60, 60, kernel_size=(19,), stride=(1,), padding=same, groups=60)\n",
      "          )\n",
      "          (norm): BatchNorm1d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (pwconv1): Conv1d(60, 60, kernel_size=(1,), stride=(1,))\n",
      "        )\n",
      "        (2): ConvNeXtLikeBlock(\n",
      "          (dwconvs): ModuleList(\n",
      "            (0): Conv1d(60, 60, kernel_size=(31,), stride=(1,), padding=same, groups=60)\n",
      "          )\n",
      "          (norm): BatchNorm1d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (pwconv1): Conv1d(60, 60, kernel_size=(1,), stride=(1,))\n",
      "        )\n",
      "        (3): ConvNeXtLikeBlock(\n",
      "          (dwconvs): ModuleList(\n",
      "            (0): Conv1d(60, 60, kernel_size=(59,), stride=(1,), padding=same, groups=60)\n",
      "          )\n",
      "          (norm): BatchNorm1d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (pwconv1): Conv1d(60, 60, kernel_size=(1,), stride=(1,))\n",
      "        )\n",
      "        (4): TransformerEncoderLayer(\n",
      "          (attention): MultiHeadAttention(\n",
      "            (k_proj): Linear(in_features=60, out_features=60, bias=True)\n",
      "            (v_proj): Linear(in_features=60, out_features=60, bias=True)\n",
      "            (q_proj): Linear(in_features=60, out_features=60, bias=True)\n",
      "            (out_proj): Linear(in_features=60, out_features=60, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm((60,), eps=1e-06, elementwise_affine=True)\n",
      "          (feed_forward): FeedForward(\n",
      "            (intermediate_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (intermediate_dense): Linear(in_features=60, out_features=60, bias=True)\n",
      "            (intermediate_act_fn): NewGELUActivation()\n",
      "            (output_dense): Linear(in_features=60, out_features=60, bias=True)\n",
      "            (output_dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (final_layer_norm): LayerNorm((60,), eps=1e-06, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (exp_dim_conv): Conv1d(60, 600, kernel_size=(1,), stride=(1,))\n",
      "    )\n",
      "  )\n",
      "  (stage4): Sequential(\n",
      "    (0): weigth1d(w=(1, 5, 600, 1),sequential=False)\n",
      "    (1): to2d(f=15,c=40)\n",
      "    (2): Conv2d(40, 40, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (3): ConvBlock2d(\n",
      "      (conv_block): ResBasicBlock(\n",
      "        (conv1): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=40, bias=False)\n",
      "        (conv1pw): Conv2d(40, 40, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (bn1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=40, bias=False)\n",
      "        (conv2pw): Conv2d(40, 40, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (bn2): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (se): Identity()\n",
      "        (downsample): Identity()\n",
      "      )\n",
      "    )\n",
      "    (4): ConvBlock2d(\n",
      "      (conv_block): ResBasicBlock(\n",
      "        (conv1): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=40, bias=False)\n",
      "        (conv1pw): Conv2d(40, 40, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (bn1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=40, bias=False)\n",
      "        (conv2pw): Conv2d(40, 40, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (bn2): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (se): Identity()\n",
      "        (downsample): Identity()\n",
      "      )\n",
      "    )\n",
      "    (5): ConvBlock2d(\n",
      "      (conv_block): ResBasicBlock(\n",
      "        (conv1): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=40, bias=False)\n",
      "        (conv1pw): Conv2d(40, 40, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (bn1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=40, bias=False)\n",
      "        (conv2pw): Conv2d(40, 40, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (bn2): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (se): Identity()\n",
      "        (downsample): Identity()\n",
      "      )\n",
      "    )\n",
      "    (6): to1d()\n",
      "    (7): TimeContextBlock1d(\n",
      "      (red_dim_conv): Sequential(\n",
      "        (0): Conv1d(600, 60, kernel_size=(1,), stride=(1,))\n",
      "        (1): LayerNorm(C=(60,), data_format=channels_first, eps=1e-06)\n",
      "      )\n",
      "      (tcm): Sequential(\n",
      "        (0): ConvNeXtLikeBlock(\n",
      "          (dwconvs): ModuleList(\n",
      "            (0): Conv1d(60, 60, kernel_size=(7,), stride=(1,), padding=same, groups=60)\n",
      "          )\n",
      "          (norm): BatchNorm1d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (pwconv1): Conv1d(60, 60, kernel_size=(1,), stride=(1,))\n",
      "        )\n",
      "        (1): ConvNeXtLikeBlock(\n",
      "          (dwconvs): ModuleList(\n",
      "            (0): Conv1d(60, 60, kernel_size=(19,), stride=(1,), padding=same, groups=60)\n",
      "          )\n",
      "          (norm): BatchNorm1d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (pwconv1): Conv1d(60, 60, kernel_size=(1,), stride=(1,))\n",
      "        )\n",
      "        (2): ConvNeXtLikeBlock(\n",
      "          (dwconvs): ModuleList(\n",
      "            (0): Conv1d(60, 60, kernel_size=(31,), stride=(1,), padding=same, groups=60)\n",
      "          )\n",
      "          (norm): BatchNorm1d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (pwconv1): Conv1d(60, 60, kernel_size=(1,), stride=(1,))\n",
      "        )\n",
      "        (3): ConvNeXtLikeBlock(\n",
      "          (dwconvs): ModuleList(\n",
      "            (0): Conv1d(60, 60, kernel_size=(59,), stride=(1,), padding=same, groups=60)\n",
      "          )\n",
      "          (norm): BatchNorm1d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (pwconv1): Conv1d(60, 60, kernel_size=(1,), stride=(1,))\n",
      "        )\n",
      "        (4): TransformerEncoderLayer(\n",
      "          (attention): MultiHeadAttention(\n",
      "            (k_proj): Linear(in_features=60, out_features=60, bias=True)\n",
      "            (v_proj): Linear(in_features=60, out_features=60, bias=True)\n",
      "            (q_proj): Linear(in_features=60, out_features=60, bias=True)\n",
      "            (out_proj): Linear(in_features=60, out_features=60, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm((60,), eps=1e-06, elementwise_affine=True)\n",
      "          (feed_forward): FeedForward(\n",
      "            (intermediate_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (intermediate_dense): Linear(in_features=60, out_features=60, bias=True)\n",
      "            (intermediate_act_fn): NewGELUActivation()\n",
      "            (output_dense): Linear(in_features=60, out_features=60, bias=True)\n",
      "            (output_dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (final_layer_norm): LayerNorm((60,), eps=1e-06, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (exp_dim_conv): Conv1d(60, 600, kernel_size=(1,), stride=(1,))\n",
      "    )\n",
      "  )\n",
      "  (fin_wght1d): weigth1d(w=(1, 6, 600, 1),sequential=False)\n",
      "  (mfa): Identity()\n",
      "  (fin_to2d): Identity()\n",
      ")\n",
      "spec => MelBanks(\n",
      "  (torchfbank): Sequential(\n",
      "    (0): Identity()\n",
      "    (1): PreEmphasis()\n",
      "    (2): MelSpectrogram(\n",
      "      (spectrogram): Spectrogram()\n",
      "      (mel_scale): MelScale()\n",
      "    )\n",
      "  )\n",
      "  (specaug): Identity()\n",
      ")\n",
      "pool => ASTP(\n",
      "  (linear1): Conv1d(1800, 128, kernel_size=(1,), stride=(1,))\n",
      "  (linear2): Conv1d(128, 600, kernel_size=(1,), stride=(1,))\n",
      ")\n",
      "bn => BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "linear => Linear(in_features=1200, out_features=192, bias=True)\n"
     ]
    }
   ],
   "source": [
    "for name, module in original_model.named_children():\n",
    "    print(name, \"=>\", module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Conv1dAs2d(nn.Module):\n",
    "    \"\"\"\n",
    "    Replace a Conv1d with an equivalent Conv2d (H = kernel, W = 1)\n",
    "    so that ONNX shows only Conv2d, which RKNN supports.\n",
    "    \"\"\"\n",
    "    def __init__(self, conv1d: nn.Conv1d):\n",
    "        super().__init__()\n",
    "\n",
    "        k, d, s, g = conv1d.kernel_size[0], conv1d.dilation[0], conv1d.stride[0], conv1d.groups\n",
    "\n",
    "        # --- numeric padding ---\n",
    "        if isinstance(conv1d.padding, str):        # \"same\" or \"valid\"\n",
    "            if conv1d.padding == \"same\":\n",
    "                pad_num = floor(d * (k - 1) / 2)\n",
    "            else:                                  # \"valid\"\n",
    "                pad_num = 0\n",
    "        else:                                      # already a tuple/int\n",
    "            pad_num = conv1d.padding[0]\n",
    "\n",
    "        # Build the Conv2d with weights copied\n",
    "        self.conv2d = nn.Conv2d(\n",
    "            in_channels  = conv1d.in_channels,\n",
    "            out_channels = conv1d.out_channels,\n",
    "            kernel_size  = (k, 1),\n",
    "            stride       = (s, 1),\n",
    "            padding      = (pad_num, 0),\n",
    "            dilation     = (d, 1),\n",
    "            groups       = g,\n",
    "            bias         = conv1d.bias is not None\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # (out, in/groups, k) → (out, in/groups, k, 1)\n",
    "            self.conv2d.weight.copy_(conv1d.weight.unsqueeze(-1))\n",
    "            if conv1d.bias is not None:\n",
    "                self.conv2d.bias.copy_(conv1d.bias)\n",
    "\n",
    "    def forward(self, x):           # x: [B, C, T]\n",
    "        #todo : pay attention to the input shape! AVI APPROVED\n",
    "        return self.conv2d(x.unsqueeze(-1)).squeeze(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# 2) Define a Model Class without MelBanks\n",
    "########################################\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ReDimNetNoMel(nn.Module):\n",
    "    \"\"\"\n",
    "    A wrapper around the original ReDimNetWrap that:\n",
    "      - Excludes the 'spec' (MelBanks) module\n",
    "      - Uses 'backbone', 'pool', 'bn', and 'linear'\n",
    "    We expect a precomputed mel spectrogram as input with shape [B, 1, n_mels, time_frames].\n",
    "    \"\"\"\n",
    "    def __init__(self, original_wrap):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Grab references to the submodules we want to keep\n",
    "        self.backbone = original_wrap.backbone\n",
    "        \n",
    "        # fix problem01\n",
    "        # list of (stage, block) indices you already know are problematic\n",
    "        TARGETS = [(0, 6), (1, 8), (2, 8), (3, 9), (4, 7)]\n",
    "        for s_idx, b_idx in TARGETS:\n",
    "            for tcm_idx in range(4):\n",
    "                block = self.backbone.__getattr__(f\"stage{s_idx}\")[b_idx].tcm[tcm_idx]\n",
    "\n",
    "                block.dwconvs[0] = Conv1dAs2d(block.dwconvs[0])\n",
    "                block.pwconv1    = Conv1dAs2d(block.pwconv1)   # 1×1 conv\n",
    "\n",
    "        \n",
    "        # Replace ASTP with RKNN-safe version:\n",
    "        self.pool = original_wrap.pool\n",
    "        self.bn = original_wrap.bn\n",
    "        self.linear = original_wrap.linear\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: shape [B, 1, n_mels, time_frames]\n",
    "        # (1) Pass through the backbone\n",
    "        x = self.backbone(x)    # shape might become [B, channels, frames] or similar\n",
    "        print(\"Backbone output shape:\", x.shape)  # ADD THIS LINE\n",
    "        # (2) Pooling\n",
    "        x = self.pool(x)        # ASTP => shape likely [B, embedding_dim]\n",
    "        # (3) BatchNorm\n",
    "        x = self.bn(x)\n",
    "        # (4) Final linear => 192-dim (if that's your embedding size)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Create an instance of our new model that skips the MelBanks front-end\n",
    "model_no_mel = ReDimNetNoMel(original_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backbone output shape: torch.Size([1, 600, 200])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.3978,  1.5166, -2.9237, -1.6767, -0.6410,  0.6147,  1.7317,  1.3453,\n",
       "          2.0658,  3.0819,  2.4785,  0.0139, -2.3835,  0.4887, -3.2520,  1.4158,\n",
       "         -2.4732,  0.0682,  1.1215, -1.5474, -0.9176,  1.6874, -1.2805,  1.3055,\n",
       "         -0.0371, -0.9979,  0.9772, -1.1284, -0.0362, -0.3448,  1.2055, -3.1566,\n",
       "         -0.5061,  2.2746, -0.7287,  2.1097,  1.7377, -1.6573, -1.0085, -2.9908,\n",
       "         -1.1353,  3.1056,  3.1658, -3.3186,  2.9330,  1.3228, -1.2465, -0.1844,\n",
       "         -1.3745,  1.2949,  0.5271, -0.7634, -2.0954,  3.0173, -3.8173, -2.1225,\n",
       "         -1.9191, -3.1836, -0.3250, -1.4265, -2.3579, -1.1529, -0.1265, -0.2899,\n",
       "          0.3035,  2.2541, -2.8337,  2.2676,  0.2988,  0.8168,  0.9460, -0.9825,\n",
       "         -3.9490, -0.9255,  0.5740, -4.0351, -1.5374,  0.1865, -5.0443, -0.6003,\n",
       "         -3.5272,  3.8754,  1.5844, -2.0722,  0.3120, -0.5477,  3.2885,  0.6553,\n",
       "         -0.5948,  1.2261,  0.5734,  1.0521,  1.9769,  3.2080,  0.7328,  1.6537,\n",
       "          1.9009,  1.4860,  2.7492,  0.1393, -1.9055, -0.1571,  0.8367, -0.4205,\n",
       "         -0.7043, -0.1983, -0.0645,  0.4768,  1.1560, -1.5002, -1.1933,  2.1007,\n",
       "          0.5130,  1.0569,  2.1791, -1.8199,  1.4225, -1.7833, -0.1899,  0.4803,\n",
       "         -1.6428, -0.2877,  2.3677, -1.7298,  0.5610,  0.7084, -1.7934, -1.3492,\n",
       "         -0.6993, -0.7780, -1.6621, -1.9518,  2.6080, -0.2119, -0.7586,  1.1428,\n",
       "          1.7514,  1.8043,  2.5042, -2.1254,  1.8767, -2.8222, -1.2654,  1.9407,\n",
       "         -1.8709,  1.4742, -0.8101,  0.5478, -1.8454,  1.4076, -0.9916,  1.3424,\n",
       "         -1.0026,  0.5647, -3.9291, -1.5981,  0.0436, -0.6273, -1.1933,  2.5296,\n",
       "         -0.2132,  1.2212, -1.1554, -0.1410,  1.6489,  0.1166, -0.4321,  0.5881,\n",
       "         -0.9651,  0.3211,  0.4214, -0.0711, -0.0387,  0.4719,  1.6275, -0.4150,\n",
       "         -1.0233,  2.3539, -1.8616, -0.9609, -0.2214,  0.8869, -3.8101,  0.4406,\n",
       "          1.5827, -1.6137, -1.9918, -1.6878,  0.7742, -1.4097, -0.4561,  1.4968]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_no_mel.eval()  # <- this line is critical!\n",
    "dummy = torch.randn(1, 1, 60, 200)\n",
    "model_no_mel(dummy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## layres debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "problematic layer tree is:\n",
    "\n",
    "```\n",
    "TimeContextBlock1d\n",
    "├── red_dim_conv (Sequential)\n",
    "│   ├── Conv1d(600 → 60, kernel_size=1)\n",
    "│   └── LayerNorm(C=60, data_format=channels_first)\n",
    "├── tcm (Sequential)\n",
    "│   ├── ConvNeXtLikeBlock (kernel=7)\n",
    "│   │   ├── dwconvs: Conv1d(60 → 60, kernel_size=7, groups=60)\n",
    "│   │   ├── norm: BatchNorm1d(60)\n",
    "│   │   ├── act: GELU\n",
    "│   │   └── pwconv1: Conv1d(60 → 60, kernel_size=1)\n",
    "│   ├── ConvNeXtLikeBlock (kernel=19)\n",
    "│   │   ├── dwconvs: Conv1d(60 → 60, kernel_size=19, groups=60)\n",
    "│   │   ├── norm: BatchNorm1d(60)\n",
    "│   │   ├── act: GELU\n",
    "│   │   └── pwconv1: Conv1d(60 → 60, kernel_size=1)\n",
    "│   ├── ConvNeXtLikeBlock (kernel=31)\n",
    "│   │   ├── dwconvs: Conv1d(60 → 60, kernel_size=31, groups=60)\n",
    "│   │   ├── norm: BatchNorm1d(60)\n",
    "│   │   ├── act: GELU\n",
    "│   │   └── pwconv1: Conv1d(60 → 60, kernel_size=1)\n",
    "│   ├── ConvNeXtLikeBlock (kernel=59)\n",
    "│   │   ├── dwconvs: Conv1d(60 → 60, kernel_size=59, groups=60)\n",
    "│   │   ├── norm: BatchNorm1d(60)\n",
    "│   │   ├── act: GELU\n",
    "│   │   └── pwconv1: Conv1d(60 → 60, kernel_size=1)\n",
    "│   └── TransformerEncoderLayer\n",
    "│       ├── attention (MultiHeadAttention)\n",
    "│       │   ├── k_proj: Linear(60 → 60)\n",
    "│       │   ├── v_proj: Linear(60 → 60)\n",
    "│       │   ├── q_proj: Linear(60 → 60)\n",
    "│       │   └── out_proj: Linear(60 → 60)\n",
    "│       ├── layer_norm: LayerNorm(60)\n",
    "│       ├── feed_forward\n",
    "│       │   ├── intermediate_dropout: Dropout(0.0)\n",
    "│       │   ├── intermediate_dense: Linear(60 → 60)\n",
    "│       │   ├── intermediate_act_fn: NewGELUActivation\n",
    "│       │   ├── output_dense: Linear(60 → 60)\n",
    "│       │   └── output_dropout: Dropout(0.0)\n",
    "│       └── final_layer_norm: LayerNorm(60)\n",
    "└── exp_dim_conv: Conv1d(60 → 600, kernel_size=1)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Still has LayerNorm at: backbone.stage0.6.tcm.4.layer_norm\n",
      "❌ Still has LayerNorm at: backbone.stage0.6.tcm.4.final_layer_norm\n",
      "❌ Still has LayerNorm at: backbone.stage1.8.tcm.4.layer_norm\n",
      "❌ Still has LayerNorm at: backbone.stage1.8.tcm.4.final_layer_norm\n",
      "❌ Still has LayerNorm at: backbone.stage2.8.tcm.4.layer_norm\n",
      "❌ Still has LayerNorm at: backbone.stage2.8.tcm.4.final_layer_norm\n",
      "❌ Still has LayerNorm at: backbone.stage3.9.tcm.4.layer_norm\n",
      "❌ Still has LayerNorm at: backbone.stage3.9.tcm.4.final_layer_norm\n",
      "❌ Still has LayerNorm at: backbone.stage4.7.tcm.4.layer_norm\n",
      "❌ Still has LayerNorm at: backbone.stage4.7.tcm.4.final_layer_norm\n"
     ]
    }
   ],
   "source": [
    "for name, module in model_no_mel.named_modules():\n",
    "    if isinstance(module, nn.LayerNorm):\n",
    "        print(\"❌ Still has LayerNorm at:\", name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stage0.6 = ConvNeXtLikeBlock(\n",
      "  (dwconvs): ModuleList(\n",
      "    (0): Conv1dAs2d(\n",
      "      (conv2d): Conv2d(60, 60, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), groups=60)\n",
      "    )\n",
      "  )\n",
      "  (norm): BatchNorm1d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (act): GELU(approximate='none')\n",
      "  (pwconv1): Conv1dAs2d(\n",
      "    (conv2d): Conv2d(60, 60, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(\"stage0.6 =\", model_no_mel.backbone.stage4[7].tcm[0]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FP16 check\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backbone output shape: torch.Size([1, 600, 200])\n",
      "safe in pure FP16? tensor(True)\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    fp16_net = copy.deepcopy(model_no_mel).half().eval()\n",
    "    ok = torch.isfinite(fp16_net(dummy.half())).all()\n",
    "    print('safe in pure FP16?', ok)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ReDimNetNoMel(\n",
       "  (backbone): ReDimNet(\n",
       "    (stem): Sequential(\n",
       "      (0): Conv2d(1, 10, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "      (1): LayerNorm(C=(10,), data_format=channels_first, eps=1e-06)\n",
       "      (2): to1d()\n",
       "    )\n",
       "    (stage0): Sequential(\n",
       "      (0): weigth1d(w=(1, 1, 1, 1),sequential=False)\n",
       "      (1): to2d(f=60,c=10)\n",
       "      (2): Conv2d(10, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (3): ConvBlock2d(\n",
       "        (conv_block): ResBasicBlock(\n",
       "          (conv1): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=10, bias=False)\n",
       "          (conv1pw): Conv2d(10, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=10, bias=False)\n",
       "          (conv2pw): Conv2d(10, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (se): Identity()\n",
       "          (downsample): Identity()\n",
       "        )\n",
       "      )\n",
       "      (4): ConvBlock2d(\n",
       "        (conv_block): ResBasicBlock(\n",
       "          (conv1): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=10, bias=False)\n",
       "          (conv1pw): Conv2d(10, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=10, bias=False)\n",
       "          (conv2pw): Conv2d(10, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (se): Identity()\n",
       "          (downsample): Identity()\n",
       "        )\n",
       "      )\n",
       "      (5): to1d()\n",
       "      (6): TimeContextBlock1d(\n",
       "        (red_dim_conv): Sequential(\n",
       "          (0): Conv1d(600, 20, kernel_size=(1,), stride=(1,))\n",
       "          (1): LayerNorm(C=(20,), data_format=channels_first, eps=1e-06)\n",
       "        )\n",
       "        (tcm): Sequential(\n",
       "          (0): ConvNeXtLikeBlock(\n",
       "            (dwconvs): ModuleList(\n",
       "              (0): Conv1dAs2d(\n",
       "                (conv2d): Conv2d(20, 20, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), groups=20)\n",
       "              )\n",
       "            )\n",
       "            (norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (pwconv1): Conv1dAs2d(\n",
       "              (conv2d): Conv2d(20, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "          )\n",
       "          (1): ConvNeXtLikeBlock(\n",
       "            (dwconvs): ModuleList(\n",
       "              (0): Conv1dAs2d(\n",
       "                (conv2d): Conv2d(20, 20, kernel_size=(19, 1), stride=(1, 1), padding=(9, 0), groups=20)\n",
       "              )\n",
       "            )\n",
       "            (norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (pwconv1): Conv1dAs2d(\n",
       "              (conv2d): Conv2d(20, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "          )\n",
       "          (2): ConvNeXtLikeBlock(\n",
       "            (dwconvs): ModuleList(\n",
       "              (0): Conv1dAs2d(\n",
       "                (conv2d): Conv2d(20, 20, kernel_size=(31, 1), stride=(1, 1), padding=(15, 0), groups=20)\n",
       "              )\n",
       "            )\n",
       "            (norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (pwconv1): Conv1dAs2d(\n",
       "              (conv2d): Conv2d(20, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "          )\n",
       "          (3): ConvNeXtLikeBlock(\n",
       "            (dwconvs): ModuleList(\n",
       "              (0): Conv1dAs2d(\n",
       "                (conv2d): Conv2d(20, 20, kernel_size=(59, 1), stride=(1, 1), padding=(29, 0), groups=20)\n",
       "              )\n",
       "            )\n",
       "            (norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (pwconv1): Conv1dAs2d(\n",
       "              (conv2d): Conv2d(20, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "          )\n",
       "          (4): TransformerEncoderLayer(\n",
       "            (attention): MultiHeadAttention(\n",
       "              (k_proj): Linear(in_features=20, out_features=20, bias=True)\n",
       "              (v_proj): Linear(in_features=20, out_features=20, bias=True)\n",
       "              (q_proj): Linear(in_features=20, out_features=20, bias=True)\n",
       "              (out_proj): Linear(in_features=20, out_features=20, bias=True)\n",
       "            )\n",
       "            (layer_norm): LayerNorm((20,), eps=1e-06, elementwise_affine=True)\n",
       "            (feed_forward): FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.0, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=20, out_features=20, bias=True)\n",
       "              (intermediate_act_fn): NewGELUActivation()\n",
       "              (output_dense): Linear(in_features=20, out_features=20, bias=True)\n",
       "              (output_dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((20,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (exp_dim_conv): Conv1d(20, 600, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "    )\n",
       "    (stage1): Sequential(\n",
       "      (0): weigth1d(w=(1, 2, 600, 1),sequential=False)\n",
       "      (1): to2d(f=60,c=10)\n",
       "      (2): Conv2d(10, 40, kernel_size=(2, 1), stride=(2, 1))\n",
       "      (3): ConvBlock2d(\n",
       "        (conv_block): ResBasicBlock(\n",
       "          (conv1): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=40, bias=False)\n",
       "          (conv1pw): Conv2d(40, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (bn1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=40, bias=False)\n",
       "          (conv2pw): Conv2d(40, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (bn2): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (se): Identity()\n",
       "          (downsample): Identity()\n",
       "        )\n",
       "      )\n",
       "      (4): ConvBlock2d(\n",
       "        (conv_block): ResBasicBlock(\n",
       "          (conv1): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=40, bias=False)\n",
       "          (conv1pw): Conv2d(40, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (bn1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=40, bias=False)\n",
       "          (conv2pw): Conv2d(40, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (bn2): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (se): Identity()\n",
       "          (downsample): Identity()\n",
       "        )\n",
       "      )\n",
       "      (5): ConvBlock2d(\n",
       "        (conv_block): ResBasicBlock(\n",
       "          (conv1): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=40, bias=False)\n",
       "          (conv1pw): Conv2d(40, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (bn1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=40, bias=False)\n",
       "          (conv2pw): Conv2d(40, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (bn2): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (se): Identity()\n",
       "          (downsample): Identity()\n",
       "        )\n",
       "      )\n",
       "      (6): Sequential(\n",
       "        (0): Conv2d(40, 20, kernel_size=(3, 3), stride=(1, 1), padding=same, groups=20)\n",
       "        (1): BatchNorm2d(20, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): GELU(approximate='none')\n",
       "        (3): Conv2d(20, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (7): to1d()\n",
       "      (8): TimeContextBlock1d(\n",
       "        (red_dim_conv): Sequential(\n",
       "          (0): Conv1d(600, 20, kernel_size=(1,), stride=(1,))\n",
       "          (1): LayerNorm(C=(20,), data_format=channels_first, eps=1e-06)\n",
       "        )\n",
       "        (tcm): Sequential(\n",
       "          (0): ConvNeXtLikeBlock(\n",
       "            (dwconvs): ModuleList(\n",
       "              (0): Conv1dAs2d(\n",
       "                (conv2d): Conv2d(20, 20, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), groups=20)\n",
       "              )\n",
       "            )\n",
       "            (norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (pwconv1): Conv1dAs2d(\n",
       "              (conv2d): Conv2d(20, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "          )\n",
       "          (1): ConvNeXtLikeBlock(\n",
       "            (dwconvs): ModuleList(\n",
       "              (0): Conv1dAs2d(\n",
       "                (conv2d): Conv2d(20, 20, kernel_size=(19, 1), stride=(1, 1), padding=(9, 0), groups=20)\n",
       "              )\n",
       "            )\n",
       "            (norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (pwconv1): Conv1dAs2d(\n",
       "              (conv2d): Conv2d(20, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "          )\n",
       "          (2): ConvNeXtLikeBlock(\n",
       "            (dwconvs): ModuleList(\n",
       "              (0): Conv1dAs2d(\n",
       "                (conv2d): Conv2d(20, 20, kernel_size=(31, 1), stride=(1, 1), padding=(15, 0), groups=20)\n",
       "              )\n",
       "            )\n",
       "            (norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (pwconv1): Conv1dAs2d(\n",
       "              (conv2d): Conv2d(20, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "          )\n",
       "          (3): ConvNeXtLikeBlock(\n",
       "            (dwconvs): ModuleList(\n",
       "              (0): Conv1dAs2d(\n",
       "                (conv2d): Conv2d(20, 20, kernel_size=(59, 1), stride=(1, 1), padding=(29, 0), groups=20)\n",
       "              )\n",
       "            )\n",
       "            (norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (pwconv1): Conv1dAs2d(\n",
       "              (conv2d): Conv2d(20, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "          )\n",
       "          (4): TransformerEncoderLayer(\n",
       "            (attention): MultiHeadAttention(\n",
       "              (k_proj): Linear(in_features=20, out_features=20, bias=True)\n",
       "              (v_proj): Linear(in_features=20, out_features=20, bias=True)\n",
       "              (q_proj): Linear(in_features=20, out_features=20, bias=True)\n",
       "              (out_proj): Linear(in_features=20, out_features=20, bias=True)\n",
       "            )\n",
       "            (layer_norm): LayerNorm((20,), eps=1e-06, elementwise_affine=True)\n",
       "            (feed_forward): FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.0, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=20, out_features=20, bias=True)\n",
       "              (intermediate_act_fn): NewGELUActivation()\n",
       "              (output_dense): Linear(in_features=20, out_features=20, bias=True)\n",
       "              (output_dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((20,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (exp_dim_conv): Conv1d(20, 600, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "    )\n",
       "    (stage2): Sequential(\n",
       "      (0): weigth1d(w=(1, 3, 600, 1),sequential=False)\n",
       "      (1): to2d(f=30,c=20)\n",
       "      (2): Conv2d(20, 60, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (3): ConvBlock2d(\n",
       "        (conv_block): ResBasicBlock(\n",
       "          (conv1): Conv2d(60, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=60, bias=False)\n",
       "          (conv1pw): Conv2d(60, 60, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (bn1): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(60, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=60, bias=False)\n",
       "          (conv2pw): Conv2d(60, 60, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (bn2): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (se): Identity()\n",
       "          (downsample): Identity()\n",
       "        )\n",
       "      )\n",
       "      (4): ConvBlock2d(\n",
       "        (conv_block): ResBasicBlock(\n",
       "          (conv1): Conv2d(60, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=60, bias=False)\n",
       "          (conv1pw): Conv2d(60, 60, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (bn1): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(60, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=60, bias=False)\n",
       "          (conv2pw): Conv2d(60, 60, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (bn2): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (se): Identity()\n",
       "          (downsample): Identity()\n",
       "        )\n",
       "      )\n",
       "      (5): ConvBlock2d(\n",
       "        (conv_block): ResBasicBlock(\n",
       "          (conv1): Conv2d(60, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=60, bias=False)\n",
       "          (conv1pw): Conv2d(60, 60, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (bn1): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(60, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=60, bias=False)\n",
       "          (conv2pw): Conv2d(60, 60, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (bn2): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (se): Identity()\n",
       "          (downsample): Identity()\n",
       "        )\n",
       "      )\n",
       "      (6): Sequential(\n",
       "        (0): Conv2d(60, 20, kernel_size=(3, 3), stride=(1, 1), padding=same, groups=20)\n",
       "        (1): BatchNorm2d(20, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): GELU(approximate='none')\n",
       "        (3): Conv2d(20, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (7): to1d()\n",
       "      (8): TimeContextBlock1d(\n",
       "        (red_dim_conv): Sequential(\n",
       "          (0): Conv1d(600, 20, kernel_size=(1,), stride=(1,))\n",
       "          (1): LayerNorm(C=(20,), data_format=channels_first, eps=1e-06)\n",
       "        )\n",
       "        (tcm): Sequential(\n",
       "          (0): ConvNeXtLikeBlock(\n",
       "            (dwconvs): ModuleList(\n",
       "              (0): Conv1dAs2d(\n",
       "                (conv2d): Conv2d(20, 20, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), groups=20)\n",
       "              )\n",
       "            )\n",
       "            (norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (pwconv1): Conv1dAs2d(\n",
       "              (conv2d): Conv2d(20, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "          )\n",
       "          (1): ConvNeXtLikeBlock(\n",
       "            (dwconvs): ModuleList(\n",
       "              (0): Conv1dAs2d(\n",
       "                (conv2d): Conv2d(20, 20, kernel_size=(19, 1), stride=(1, 1), padding=(9, 0), groups=20)\n",
       "              )\n",
       "            )\n",
       "            (norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (pwconv1): Conv1dAs2d(\n",
       "              (conv2d): Conv2d(20, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "          )\n",
       "          (2): ConvNeXtLikeBlock(\n",
       "            (dwconvs): ModuleList(\n",
       "              (0): Conv1dAs2d(\n",
       "                (conv2d): Conv2d(20, 20, kernel_size=(31, 1), stride=(1, 1), padding=(15, 0), groups=20)\n",
       "              )\n",
       "            )\n",
       "            (norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (pwconv1): Conv1dAs2d(\n",
       "              (conv2d): Conv2d(20, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "          )\n",
       "          (3): ConvNeXtLikeBlock(\n",
       "            (dwconvs): ModuleList(\n",
       "              (0): Conv1dAs2d(\n",
       "                (conv2d): Conv2d(20, 20, kernel_size=(59, 1), stride=(1, 1), padding=(29, 0), groups=20)\n",
       "              )\n",
       "            )\n",
       "            (norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (pwconv1): Conv1dAs2d(\n",
       "              (conv2d): Conv2d(20, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "          )\n",
       "          (4): TransformerEncoderLayer(\n",
       "            (attention): MultiHeadAttention(\n",
       "              (k_proj): Linear(in_features=20, out_features=20, bias=True)\n",
       "              (v_proj): Linear(in_features=20, out_features=20, bias=True)\n",
       "              (q_proj): Linear(in_features=20, out_features=20, bias=True)\n",
       "              (out_proj): Linear(in_features=20, out_features=20, bias=True)\n",
       "            )\n",
       "            (layer_norm): LayerNorm((20,), eps=1e-06, elementwise_affine=True)\n",
       "            (feed_forward): FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.0, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=20, out_features=20, bias=True)\n",
       "              (intermediate_act_fn): NewGELUActivation()\n",
       "              (output_dense): Linear(in_features=20, out_features=20, bias=True)\n",
       "              (output_dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((20,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (exp_dim_conv): Conv1d(20, 600, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "    )\n",
       "    (stage3): Sequential(\n",
       "      (0): weigth1d(w=(1, 4, 600, 1),sequential=False)\n",
       "      (1): to2d(f=30,c=20)\n",
       "      (2): Conv2d(20, 80, kernel_size=(2, 1), stride=(2, 1))\n",
       "      (3): ConvBlock2d(\n",
       "        (conv_block): ResBasicBlock(\n",
       "          (conv1): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=80, bias=False)\n",
       "          (conv1pw): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (bn1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=80, bias=False)\n",
       "          (conv2pw): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (bn2): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (se): Identity()\n",
       "          (downsample): Identity()\n",
       "        )\n",
       "      )\n",
       "      (4): ConvBlock2d(\n",
       "        (conv_block): ResBasicBlock(\n",
       "          (conv1): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=80, bias=False)\n",
       "          (conv1pw): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (bn1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=80, bias=False)\n",
       "          (conv2pw): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (bn2): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (se): Identity()\n",
       "          (downsample): Identity()\n",
       "        )\n",
       "      )\n",
       "      (5): ConvBlock2d(\n",
       "        (conv_block): ResBasicBlock(\n",
       "          (conv1): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=80, bias=False)\n",
       "          (conv1pw): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (bn1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=80, bias=False)\n",
       "          (conv2pw): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (bn2): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (se): Identity()\n",
       "          (downsample): Identity()\n",
       "        )\n",
       "      )\n",
       "      (6): ConvBlock2d(\n",
       "        (conv_block): ResBasicBlock(\n",
       "          (conv1): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=80, bias=False)\n",
       "          (conv1pw): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (bn1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=80, bias=False)\n",
       "          (conv2pw): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (bn2): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (se): Identity()\n",
       "          (downsample): Identity()\n",
       "        )\n",
       "      )\n",
       "      (7): Sequential(\n",
       "        (0): Conv2d(80, 40, kernel_size=(3, 3), stride=(1, 1), padding=same, groups=40)\n",
       "        (1): BatchNorm2d(40, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): GELU(approximate='none')\n",
       "        (3): Conv2d(40, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (8): to1d()\n",
       "      (9): TimeContextBlock1d(\n",
       "        (red_dim_conv): Sequential(\n",
       "          (0): Conv1d(600, 60, kernel_size=(1,), stride=(1,))\n",
       "          (1): LayerNorm(C=(60,), data_format=channels_first, eps=1e-06)\n",
       "        )\n",
       "        (tcm): Sequential(\n",
       "          (0): ConvNeXtLikeBlock(\n",
       "            (dwconvs): ModuleList(\n",
       "              (0): Conv1dAs2d(\n",
       "                (conv2d): Conv2d(60, 60, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), groups=60)\n",
       "              )\n",
       "            )\n",
       "            (norm): BatchNorm1d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (pwconv1): Conv1dAs2d(\n",
       "              (conv2d): Conv2d(60, 60, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "          )\n",
       "          (1): ConvNeXtLikeBlock(\n",
       "            (dwconvs): ModuleList(\n",
       "              (0): Conv1dAs2d(\n",
       "                (conv2d): Conv2d(60, 60, kernel_size=(19, 1), stride=(1, 1), padding=(9, 0), groups=60)\n",
       "              )\n",
       "            )\n",
       "            (norm): BatchNorm1d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (pwconv1): Conv1dAs2d(\n",
       "              (conv2d): Conv2d(60, 60, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "          )\n",
       "          (2): ConvNeXtLikeBlock(\n",
       "            (dwconvs): ModuleList(\n",
       "              (0): Conv1dAs2d(\n",
       "                (conv2d): Conv2d(60, 60, kernel_size=(31, 1), stride=(1, 1), padding=(15, 0), groups=60)\n",
       "              )\n",
       "            )\n",
       "            (norm): BatchNorm1d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (pwconv1): Conv1dAs2d(\n",
       "              (conv2d): Conv2d(60, 60, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "          )\n",
       "          (3): ConvNeXtLikeBlock(\n",
       "            (dwconvs): ModuleList(\n",
       "              (0): Conv1dAs2d(\n",
       "                (conv2d): Conv2d(60, 60, kernel_size=(59, 1), stride=(1, 1), padding=(29, 0), groups=60)\n",
       "              )\n",
       "            )\n",
       "            (norm): BatchNorm1d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (pwconv1): Conv1dAs2d(\n",
       "              (conv2d): Conv2d(60, 60, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "          )\n",
       "          (4): TransformerEncoderLayer(\n",
       "            (attention): MultiHeadAttention(\n",
       "              (k_proj): Linear(in_features=60, out_features=60, bias=True)\n",
       "              (v_proj): Linear(in_features=60, out_features=60, bias=True)\n",
       "              (q_proj): Linear(in_features=60, out_features=60, bias=True)\n",
       "              (out_proj): Linear(in_features=60, out_features=60, bias=True)\n",
       "            )\n",
       "            (layer_norm): LayerNorm((60,), eps=1e-06, elementwise_affine=True)\n",
       "            (feed_forward): FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.0, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=60, out_features=60, bias=True)\n",
       "              (intermediate_act_fn): NewGELUActivation()\n",
       "              (output_dense): Linear(in_features=60, out_features=60, bias=True)\n",
       "              (output_dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((60,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (exp_dim_conv): Conv1d(60, 600, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "    )\n",
       "    (stage4): Sequential(\n",
       "      (0): weigth1d(w=(1, 5, 600, 1),sequential=False)\n",
       "      (1): to2d(f=15,c=40)\n",
       "      (2): Conv2d(40, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (3): ConvBlock2d(\n",
       "        (conv_block): ResBasicBlock(\n",
       "          (conv1): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=40, bias=False)\n",
       "          (conv1pw): Conv2d(40, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (bn1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=40, bias=False)\n",
       "          (conv2pw): Conv2d(40, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (bn2): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (se): Identity()\n",
       "          (downsample): Identity()\n",
       "        )\n",
       "      )\n",
       "      (4): ConvBlock2d(\n",
       "        (conv_block): ResBasicBlock(\n",
       "          (conv1): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=40, bias=False)\n",
       "          (conv1pw): Conv2d(40, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (bn1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=40, bias=False)\n",
       "          (conv2pw): Conv2d(40, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (bn2): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (se): Identity()\n",
       "          (downsample): Identity()\n",
       "        )\n",
       "      )\n",
       "      (5): ConvBlock2d(\n",
       "        (conv_block): ResBasicBlock(\n",
       "          (conv1): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=40, bias=False)\n",
       "          (conv1pw): Conv2d(40, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (bn1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=40, bias=False)\n",
       "          (conv2pw): Conv2d(40, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (bn2): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (se): Identity()\n",
       "          (downsample): Identity()\n",
       "        )\n",
       "      )\n",
       "      (6): to1d()\n",
       "      (7): TimeContextBlock1d(\n",
       "        (red_dim_conv): Sequential(\n",
       "          (0): Conv1d(600, 60, kernel_size=(1,), stride=(1,))\n",
       "          (1): LayerNorm(C=(60,), data_format=channels_first, eps=1e-06)\n",
       "        )\n",
       "        (tcm): Sequential(\n",
       "          (0): ConvNeXtLikeBlock(\n",
       "            (dwconvs): ModuleList(\n",
       "              (0): Conv1dAs2d(\n",
       "                (conv2d): Conv2d(60, 60, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), groups=60)\n",
       "              )\n",
       "            )\n",
       "            (norm): BatchNorm1d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (pwconv1): Conv1dAs2d(\n",
       "              (conv2d): Conv2d(60, 60, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "          )\n",
       "          (1): ConvNeXtLikeBlock(\n",
       "            (dwconvs): ModuleList(\n",
       "              (0): Conv1dAs2d(\n",
       "                (conv2d): Conv2d(60, 60, kernel_size=(19, 1), stride=(1, 1), padding=(9, 0), groups=60)\n",
       "              )\n",
       "            )\n",
       "            (norm): BatchNorm1d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (pwconv1): Conv1dAs2d(\n",
       "              (conv2d): Conv2d(60, 60, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "          )\n",
       "          (2): ConvNeXtLikeBlock(\n",
       "            (dwconvs): ModuleList(\n",
       "              (0): Conv1dAs2d(\n",
       "                (conv2d): Conv2d(60, 60, kernel_size=(31, 1), stride=(1, 1), padding=(15, 0), groups=60)\n",
       "              )\n",
       "            )\n",
       "            (norm): BatchNorm1d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (pwconv1): Conv1dAs2d(\n",
       "              (conv2d): Conv2d(60, 60, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "          )\n",
       "          (3): ConvNeXtLikeBlock(\n",
       "            (dwconvs): ModuleList(\n",
       "              (0): Conv1dAs2d(\n",
       "                (conv2d): Conv2d(60, 60, kernel_size=(59, 1), stride=(1, 1), padding=(29, 0), groups=60)\n",
       "              )\n",
       "            )\n",
       "            (norm): BatchNorm1d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (pwconv1): Conv1dAs2d(\n",
       "              (conv2d): Conv2d(60, 60, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "          )\n",
       "          (4): TransformerEncoderLayer(\n",
       "            (attention): MultiHeadAttention(\n",
       "              (k_proj): Linear(in_features=60, out_features=60, bias=True)\n",
       "              (v_proj): Linear(in_features=60, out_features=60, bias=True)\n",
       "              (q_proj): Linear(in_features=60, out_features=60, bias=True)\n",
       "              (out_proj): Linear(in_features=60, out_features=60, bias=True)\n",
       "            )\n",
       "            (layer_norm): LayerNorm((60,), eps=1e-06, elementwise_affine=True)\n",
       "            (feed_forward): FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.0, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=60, out_features=60, bias=True)\n",
       "              (intermediate_act_fn): NewGELUActivation()\n",
       "              (output_dense): Linear(in_features=60, out_features=60, bias=True)\n",
       "              (output_dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((60,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (exp_dim_conv): Conv1d(60, 600, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "    )\n",
       "    (fin_wght1d): weigth1d(w=(1, 6, 600, 1),sequential=False)\n",
       "    (mfa): Identity()\n",
       "    (fin_to2d): Identity()\n",
       "  )\n",
       "  (pool): ASTP(\n",
       "    (linear1): Conv1d(1800, 128, kernel_size=(1,), stride=(1,))\n",
       "    (linear2): Conv1d(128, 600, kernel_size=(1,), stride=(1,))\n",
       "  )\n",
       "  (bn): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (linear): Linear(in_features=1200, out_features=192, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_no_mel.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backbone output shape: torch.Size([1, 600, 200])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==============================================================================================================\n",
       "Layer (type:depth-idx)                                       Output Shape              Param #\n",
       "==============================================================================================================\n",
       "ReDimNetNoMel                                                [1, 192]                  --\n",
       "├─ReDimNet: 1-1                                              [1, 600, 200]             --\n",
       "│    └─Sequential: 2-1                                       [1, 600, 200]             --\n",
       "│    │    └─Conv2d: 3-1                                      [1, 10, 60, 200]          100\n",
       "│    │    └─LayerNorm: 3-2                                   [1, 10, 60, 200]          20\n",
       "│    │    └─to1d: 3-3                                        [1, 600, 200]             --\n",
       "│    └─Sequential: 2-2                                       [1, 600, 200]             --\n",
       "│    │    └─weigth1d: 3-4                                    [1, 600, 200]             (1)\n",
       "│    │    └─to2d: 3-5                                        [1, 10, 60, 200]          --\n",
       "│    │    └─Conv2d: 3-6                                      [1, 10, 60, 200]          110\n",
       "│    │    └─ConvBlock2d: 3-7                                 [1, 10, 60, 200]          440\n",
       "│    │    └─ConvBlock2d: 3-8                                 [1, 10, 60, 200]          440\n",
       "│    │    └─to1d: 3-9                                        [1, 600, 200]             --\n",
       "│    │    └─TimeContextBlock1d: 3-10                         [1, 600, 200]             31,500\n",
       "│    └─Sequential: 2-3                                       [1, 600, 200]             --\n",
       "│    │    └─weigth1d: 3-11                                   [1, 600, 200]             1,200\n",
       "│    │    └─to2d: 3-12                                       [1, 10, 60, 200]          --\n",
       "│    │    └─Conv2d: 3-13                                     [1, 40, 30, 200]          840\n",
       "│    │    └─ConvBlock2d: 3-14                                [1, 40, 30, 200]          4,160\n",
       "│    │    └─ConvBlock2d: 3-15                                [1, 40, 30, 200]          4,160\n",
       "│    │    └─ConvBlock2d: 3-16                                [1, 40, 30, 200]          4,160\n",
       "│    │    └─Sequential: 3-17                                 [1, 20, 30, 200]          840\n",
       "│    │    └─to1d: 3-18                                       [1, 600, 200]             --\n",
       "│    │    └─TimeContextBlock1d: 3-19                         [1, 600, 200]             31,500\n",
       "│    └─Sequential: 2-4                                       [1, 600, 200]             --\n",
       "│    │    └─weigth1d: 3-20                                   [1, 600, 200]             1,800\n",
       "│    │    └─to2d: 3-21                                       [1, 20, 30, 200]          --\n",
       "│    │    └─Conv2d: 3-22                                     [1, 60, 30, 200]          1,260\n",
       "│    │    └─ConvBlock2d: 3-23                                [1, 60, 30, 200]          8,640\n",
       "│    │    └─ConvBlock2d: 3-24                                [1, 60, 30, 200]          8,640\n",
       "│    │    └─ConvBlock2d: 3-25                                [1, 60, 30, 200]          8,640\n",
       "│    │    └─Sequential: 3-26                                 [1, 20, 30, 200]          1,020\n",
       "│    │    └─to1d: 3-27                                       [1, 600, 200]             --\n",
       "│    │    └─TimeContextBlock1d: 3-28                         [1, 600, 200]             31,500\n",
       "│    └─Sequential: 2-5                                       [1, 600, 200]             --\n",
       "│    │    └─weigth1d: 3-29                                   [1, 600, 200]             2,400\n",
       "│    │    └─to2d: 3-30                                       [1, 20, 30, 200]          --\n",
       "│    │    └─Conv2d: 3-31                                     [1, 80, 15, 200]          3,280\n",
       "│    │    └─ConvBlock2d: 3-32                                [1, 80, 15, 200]          14,720\n",
       "│    │    └─ConvBlock2d: 3-33                                [1, 80, 15, 200]          14,720\n",
       "│    │    └─ConvBlock2d: 3-34                                [1, 80, 15, 200]          14,720\n",
       "│    │    └─ConvBlock2d: 3-35                                [1, 80, 15, 200]          14,720\n",
       "│    │    └─Sequential: 3-36                                 [1, 40, 15, 200]          2,480\n",
       "│    │    └─to1d: 3-37                                       [1, 600, 200]             --\n",
       "│    │    └─TimeContextBlock1d: 3-38                         [1, 600, 200]             117,300\n",
       "│    └─Sequential: 2-6                                       [1, 600, 200]             --\n",
       "│    │    └─weigth1d: 3-39                                   [1, 600, 200]             3,000\n",
       "│    │    └─to2d: 3-40                                       [1, 40, 15, 200]          --\n",
       "│    │    └─Conv2d: 3-41                                     [1, 40, 15, 200]          1,640\n",
       "│    │    └─ConvBlock2d: 3-42                                [1, 40, 15, 200]          4,160\n",
       "│    │    └─ConvBlock2d: 3-43                                [1, 40, 15, 200]          4,160\n",
       "│    │    └─ConvBlock2d: 3-44                                [1, 40, 15, 200]          4,160\n",
       "│    │    └─to1d: 3-45                                       [1, 600, 200]             --\n",
       "│    │    └─TimeContextBlock1d: 3-46                         [1, 600, 200]             117,300\n",
       "│    └─weigth1d: 2-7                                         [1, 600, 200]             3,600\n",
       "│    └─Identity: 2-8                                         [1, 600, 200]             --\n",
       "│    └─Identity: 2-9                                         [1, 600, 200]             --\n",
       "├─ASTP: 1-2                                                  [1, 1200]                 --\n",
       "│    └─Conv1d: 2-10                                          [1, 128, 200]             230,528\n",
       "│    └─Conv1d: 2-11                                          [1, 600, 200]             77,400\n",
       "├─BatchNorm1d: 1-3                                           [1, 1200]                 2,400\n",
       "├─Linear: 1-4                                                [1, 192]                  230,592\n",
       "==============================================================================================================\n",
       "Total params: 1,004,251\n",
       "Trainable params: 1,004,250\n",
       "Non-trainable params: 1\n",
       "Total mult-adds (M): 606.26\n",
       "==============================================================================================================\n",
       "Input size (MB): 0.05\n",
       "Forward/backward pass size (MB): 198.55\n",
       "Params size (MB): 4.02\n",
       "Estimated Total Size (MB): 202.62\n",
       "=============================================================================================================="
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model_no_mel, (1, 1, 60, 200))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Function for WAV -> MelSpectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------------------------------------------------------------------\n",
    "#  ReDimNet front-end settings (taken from the IDRnD repo defaults)\n",
    "#    • 16 kHz audio\n",
    "#    • pre-emphasis α = 0.97\n",
    "#    • 25 ms window  (400 samples)\n",
    "#    • 15 ms hop     (240 samples)  ➜ 134 frames for a 2-s clip\n",
    "#    • 60 Mel bins, 20 Hz → 8 kHz\n",
    "# ------------------------------------------------------------------\n",
    "_PREEMPH  = 0.97\n",
    "_SR       = 16_000\n",
    "_N_FFT    = 512\n",
    "_WIN_LEN  = 400\n",
    "_HOP      = 240\n",
    "_N_MELS   = 60\n",
    "_F_MIN    = 20.0\n",
    "_F_MAX    = 7600.0\n",
    "_EPS      = 1e-6            # numerical stability\n",
    "\n",
    "\n",
    "# Singleton MelSpectrogram so we build the kernel only once\n",
    "_mel_layer = T.MelSpectrogram(\n",
    "    sample_rate=_SR,\n",
    "    n_fft=_N_FFT,\n",
    "    win_length=_WIN_LEN,\n",
    "    hop_length=_HOP,\n",
    "    f_min=_F_MIN,\n",
    "    f_max=_F_MAX,\n",
    "    n_mels=_N_MELS,\n",
    "    power=2.0,               # the original uses power-spec → log10 later\n",
    "    center=True,\n",
    "    pad_mode=\"reflect\",\n",
    "    window_fn=torch.hamming_window\n",
    ")\n",
    "\n",
    "def _pre_emphasis(wave: torch.Tensor, alpha: float = _PREEMPH) -> torch.Tensor:\n",
    "    \"\"\"y[n] = x[n] − α·x[n−1] (first sample unchanged).\"\"\"\n",
    "    y = wave.clone()\n",
    "    y[:, 1:] = y[:, 1:] - alpha * y[:, :-1]\n",
    "    return y\n",
    "\n",
    "\n",
    "def pad_or_crop_logmel(log_mel, target_frames=200):\n",
    "    \"\"\"\n",
    "    Ensures log_mel is shaped [1, n_mels, target_frames] by:\n",
    "    - Padding with zeros on the right if too short\n",
    "    - Center-cropping if too long\n",
    "    \"\"\"\n",
    "    B, M, T = log_mel.shape\n",
    "    if T < target_frames:\n",
    "        pad_amt = target_frames - T\n",
    "        log_mel = F.pad(log_mel, (0, pad_amt))  # pad at end\n",
    "        print(f\"Padding log_mel from {T} to {target_frames} frames\")\n",
    "    elif T > target_frames:\n",
    "        start = (T - target_frames) // 2\n",
    "        log_mel = log_mel[:, :, start:start + target_frames]\n",
    "        print(f\"Cropping log_mel from {T} to {target_frames} frames\")\n",
    "    return log_mel\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def waveform_to_logmel(wave: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    wave : Tensor [B', T] | [1, T]\n",
    "        16-kHz mono waveform already trimmed / padded (32 000 samples for 2 s).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    log_mel : Tensor [B', 1, 60, frames]\n",
    "        Bit-exact front-end output expected by `model_no_mel`.\n",
    "    \"\"\"\n",
    "    # Make sure we always have a batch dimension\n",
    "    if wave.dim() == 1:      # (T,) → (1, T)\n",
    "        wave = wave.unsqueeze(0)\n",
    "    elif wave.dim() == 2 and wave.shape[0] > 1:\n",
    "        raise ValueError(\"Input must be mono; got multi-channel tensor.\")\n",
    "\n",
    "    # pre-emphasis\n",
    "    wave = _pre_emphasis(wave.float())\n",
    "\n",
    "    # Mel power-spectrogram\n",
    "    mel = _mel_layer(wave)\n",
    "    mel = torch.log(mel + 1e-6)          # → [B, 60, frames]\n",
    "\n",
    "    # log-scale (natural or log10 both work – log10 matches repo)\n",
    "    mel = mel - mel.mean(dim=-1, keepdim=True)\n",
    "\n",
    "    # pad/crop\n",
    "    mel = pad_or_crop_logmel(mel, target_frames=200)  # Ensure 200 frames\n",
    "\n",
    "    # add the dummy channel dim expected by Conv2d stem\n",
    "    mel = mel.unsqueeze(1)                # → [B, 1, 60, frames]\n",
    "\n",
    "    return mel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_inference(wav_path: str):\n",
    "    # (a) Load audio\n",
    "    waveform, sample_rate = torchaudio.load(wav_path)  # shape: [channels, time]\n",
    "    # If stereo, select one channel, or average:\n",
    "    if waveform.shape[0] > 1:\n",
    "        waveform = waveform.mean(dim=0, keepdim=True)\n",
    "\n",
    "    # Resample if needed\n",
    "    target_sample_rate=16000\n",
    "    if sample_rate != target_sample_rate:\n",
    "        resampler = T.Resample(orig_freq=sample_rate, new_freq=target_sample_rate)\n",
    "        waveform = resampler(waveform)\n",
    "\n",
    "    # (b) Convert to log-mel\n",
    "    log_mel = waveform_to_logmel(waveform)\n",
    "    print('feeding logmel shape:', log_mel.shape)\n",
    "    \n",
    "    # (c) Forward pass\n",
    "    with torch.no_grad():\n",
    "        embedding = model_no_mel(log_mel)  # shape typically [1, 192] or so\n",
    "\n",
    "    print(\"Embedding shape:\", embedding.shape)\n",
    "    #print(\"Embedding:\", embedding)\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute similarity between two embeddings\n",
    "def cosine_similarity(embedding1, embedding2):\n",
    "    return F.cosine_similarity(embedding1, embedding2).item()\n",
    "\n",
    "def cosine_similarity_numpys(emb1: np.ndarray, emb2: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Compute cosine similarity between two vectors of shape (D,) or (1, D).\n",
    "    \"\"\"\n",
    "    # If shape is (1, D), flatten to (D,)\n",
    "    v1 = emb1.flatten()\n",
    "    v2 = emb2.flatten()\n",
    "\n",
    "    # dot product\n",
    "    dot = np.dot(v1, v2)\n",
    "    # norms\n",
    "    norm1 = np.linalg.norm(v1)\n",
    "    norm2 = np.linalg.norm(v2)\n",
    "\n",
    "    # Add a small epsilon in case of very small norms\n",
    "    sim = dot / (norm1 * norm2 + 1e-8)\n",
    "    return sim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cropping log_mel from 1224 to 200 frames\n",
      "feeding logmel shape: torch.Size([1, 1, 60, 200])\n",
      "Backbone output shape: torch.Size([1, 600, 200])\n",
      "Embedding shape: torch.Size([1, 192])\n",
      "Padding log_mel from 108 to 200 frames\n",
      "feeding logmel shape: torch.Size([1, 1, 60, 200])\n",
      "Backbone output shape: torch.Size([1, 600, 200])\n",
      "Embedding shape: torch.Size([1, 192])\n",
      "Padding log_mel from 99 to 200 frames\n",
      "feeding logmel shape: torch.Size([1, 1, 60, 200])\n",
      "Backbone output shape: torch.Size([1, 600, 200])\n",
      "Embedding shape: torch.Size([1, 192])\n",
      "Padding log_mel from 118 to 200 frames\n",
      "feeding logmel shape: torch.Size([1, 1, 60, 200])\n",
      "Backbone output shape: torch.Size([1, 600, 200])\n",
      "Embedding shape: torch.Size([1, 192])\n",
      "Cropping log_mel from 222 to 200 frames\n",
      "feeding logmel shape: torch.Size([1, 1, 60, 200])\n",
      "Backbone output shape: torch.Size([1, 600, 200])\n",
      "Embedding shape: torch.Size([1, 192])\n"
     ]
    }
   ],
   "source": [
    "embed0 = example_inference(\"test000.wav\")\n",
    "embed1 = example_inference(\"testRob1.wav\")\n",
    "embed2 = example_inference(\"testRob2.wav\")\n",
    "embed3 = example_inference(\"testme1.wav\")\n",
    "embed4 = example_inference(\"testme2.wav\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity (robot to robot): 0.8525184988975525\n",
      "Similarity (robot to webvoice): 0.19565537571907043\n",
      "Similarity (robot to me1   ): 0.09306611120700836\n",
      "Similarity (robot to me2  ): 0.02307063341140747\n",
      "Similarity (me 1 to me 2  ): 0.3505307734012604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3519825/3060022475.py:14: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword\n",
      "  dot = np.dot(v1, v2)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Similarity (robot to robot): {cosine_similarity_numpys(embed1, embed2)}\")\n",
    "print(f\"Similarity (robot to webvoice): {cosine_similarity_numpys(embed0, embed1)}\")\n",
    "print(f\"Similarity (robot to me1   ): {cosine_similarity_numpys(embed1, embed3)}\")\n",
    "print(f\"Similarity (robot to me2  ): {cosine_similarity_numpys(embed1, embed4)}\")\n",
    "print(f\"Similarity (me 1 to me 2  ): {cosine_similarity_numpys(embed3, embed4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.4072,  1.5482, -1.7756, -4.7985, -5.2683, -3.2689,  0.9713,  0.6932,\n",
       "        -2.0320, -2.5734])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed1[0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NHWCWrapper(nn.Module):\n",
    "    def __init__(self, model_nchw):\n",
    "        super().__init__()\n",
    "        self.model = model_nchw\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: NHWC => NCHW\n",
    "        x = x.permute(0, 3, 1, 2).contiguous()\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "\n",
    "def export_to_onnx(model, onnx_path=\"ReDimNet_no_mel.onnx\"):\n",
    "    model.eval()\n",
    "    \n",
    "    # Create a dummy input with shape matching [B=1, 1, n_mels=72, time_frames=200] (example)\n",
    "    dummy_input = torch.randn(1, 1, 60, 200)\n",
    "\n",
    "    model_NHWC = NHWCWrapper(model)\n",
    "    ## TODO: rand clip [8/-8]\n",
    "    dummy_input_NHWC = torch.rand(1, 60, 200, 1)*16-8\n",
    "\n",
    "    \n",
    "    #  fixed-length segments \n",
    "    torch.onnx.export(\n",
    "        model_NHWC,\n",
    "        dummy_input_NHWC,\n",
    "        onnx_path,\n",
    "        input_names=[\"log_mel\"],\n",
    "        output_names=[\"embedding\"],\n",
    "        opset_version=13\n",
    "    )\n",
    "    \n",
    "    # # dynamic axes for variable time frames\n",
    "    # torch.onnx.export(\n",
    "    #     model_NHWC,\n",
    "    #     dummy_input_NHWC,\n",
    "    #     onnx_path,\n",
    "    #     input_names   = [\"log_mel\"],\n",
    "    #     output_names  = [\"embedding\"],\n",
    "    #     opset_version = 13,               # use a recent opset\n",
    "    #     dynamic_axes = {\n",
    "    #         # input  tensor : {axis_index : symbolic_name}\n",
    "    #         \"log_mel\"  : {0: \"batch\",   2: \"time\"},   # B and T now flexible\n",
    "    #         \"embedding\": {0: \"batch\"}                 # output length is fixed, batched\n",
    "    #     }\n",
    "    # ) \n",
    "    \n",
    "    print(\"Exported to\", onnx_path)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backbone output shape: torch.Size([1, 600, 200])\n",
      "Exported to ReDimNet_no_mel.onnx\n",
      "-rw-rw-r-- 1 vlad vlad 4.2M Jun 17 10:49 ReDimNet_no_mel.onnx\n"
     ]
    }
   ],
   "source": [
    "export_to_onnx(model_no_mel,onnx_path = \"ReDimNet_no_mel.onnx\")\n",
    "!ls -lah ReDimNet_no_mel.onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### store half"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backbone output shape: torch.Size([1, 600, 200])\n"
     ]
    }
   ],
   "source": [
    "model_NHWC = NHWCWrapper(model_no_mel)\n",
    "dummy_input_NHWC = torch.rand(1, 60, 200, 1)*16-8\n",
    "\n",
    "fp16_net = copy.deepcopy(model_NHWC).half().eval()\n",
    "fp16_dummy = dummy_input_NHWC.half()\n",
    "\n",
    "#  fixed-length segments \n",
    "torch.onnx.export(\n",
    "   fp16_net,\n",
    "   fp16_dummy,\n",
    "   \"ReDimNet_no_mel_half.onnx\",\n",
    "   input_names=[\"log_mel\"],\n",
    "   output_names=[\"embedding\"],\n",
    "   opset_version=13\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch's .half() is unreliable for full model precision control in ONNX. Instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/proj/voice/pyvoice_venv/lib/python3.10/site-packages/onnxconverter_common/float16.py:43: UserWarning: the float32 number 5.605193857299268e-45 will be truncated to 1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(pos_min, min_positive_val))\n",
      "/data/proj/voice/pyvoice_venv/lib/python3.10/site-packages/onnxconverter_common/float16.py:43: UserWarning: the float32 number 1.2503155177867598e-09 will be truncated to 1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(pos_min, min_positive_val))\n",
      "/data/proj/voice/pyvoice_venv/lib/python3.10/site-packages/onnxconverter_common/float16.py:43: UserWarning: the float32 number 5.4816506287478153e-14 will be truncated to 1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(pos_min, min_positive_val))\n",
      "/data/proj/voice/pyvoice_venv/lib/python3.10/site-packages/onnxconverter_common/float16.py:43: UserWarning: the float32 number 2.2374582096875482e-17 will be truncated to 1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(pos_min, min_positive_val))\n",
      "/data/proj/voice/pyvoice_venv/lib/python3.10/site-packages/onnxconverter_common/float16.py:43: UserWarning: the float32 number 4.89484008880936e-09 will be truncated to 1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(pos_min, min_positive_val))\n",
      "/data/proj/voice/pyvoice_venv/lib/python3.10/site-packages/onnxconverter_common/float16.py:43: UserWarning: the float32 number 7.13836811883084e-08 will be truncated to 1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(pos_min, min_positive_val))\n",
      "/data/proj/voice/pyvoice_venv/lib/python3.10/site-packages/onnxconverter_common/float16.py:53: UserWarning: the float32 number -1.0086018242816408e-08 will be truncated to -1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(neg_max, -min_positive_val))\n",
      "/data/proj/voice/pyvoice_venv/lib/python3.10/site-packages/onnxconverter_common/float16.py:43: UserWarning: the float32 number 1.0000000116860974e-07 will be truncated to 1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(pos_min, min_positive_val))\n"
     ]
    }
   ],
   "source": [
    "from onnxconverter_common.float16 import convert_float_to_float16\n",
    "import onnx\n",
    "\n",
    "model_fp32 = onnx.load(\"ReDimNet_no_mel.onnx\")\n",
    "model_fp16 = convert_float_to_float16(model_fp32, keep_io_types=True)\n",
    "onnx.save(model_fp16, \"ReDimNet_no_mel_fp16.onnx\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## verify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# onnx_model_path = \"ReDimNet_no_mel.onnx\"\n",
    "onnx_model_path = \"ReDimNet_no_mel_fp16.onnx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX model is valid!\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "onnx_model = onnx.load(onnx_model_path)\n",
    "onnx.checker.check_model(onnx_model)\n",
    "print(\"ONNX model is valid!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as ort\n",
    "\n",
    "def run_inference_onnx(onnx_path, wav_path):\n",
    "    \"\"\"\n",
    "    Loads an audio file, converts to log-mel, and runs inference\n",
    "    in an ONNX session. Returns the embedding as a NumPy array.\n",
    "    \"\"\"\n",
    "    #######################################\n",
    "    # 1) Load your ONNX model\n",
    "    #######################################\n",
    "    # (Optional) onnx.checker to confirm it’s valid\n",
    "    onnx_model = onnx.load(onnx_path)\n",
    "    onnx.checker.check_model(onnx_model)\n",
    "    print(f\"Loaded and checked ONNX model from: {onnx_path}\")\n",
    "\n",
    "    # Create an inference session\n",
    "    session = ort.InferenceSession(onnx_path)\n",
    "\n",
    "    # Usually we retrieve the first input & output name\n",
    "    input_name = session.get_inputs()[0].name\n",
    "    output_name = session.get_outputs()[0].name\n",
    "\n",
    "    #######################################\n",
    "    # 2) Load audio, get log-mel\n",
    "    #######################################\n",
    "    waveform, sample_rate = torchaudio.load(wav_path)\n",
    "    # If multi-channel, downmix:\n",
    "    if waveform.shape[0] > 1:\n",
    "        waveform = waveform.mean(dim=0, keepdim=True)\n",
    "        \n",
    "    # Resample if needed\n",
    "    target_sample_rate=16000\n",
    "    if sample_rate != target_sample_rate:\n",
    "        resampler = T.Resample(orig_freq=sample_rate, new_freq=target_sample_rate)\n",
    "        waveform = resampler(waveform)\n",
    "\n",
    "    log_mel = waveform_to_logmel(waveform)\n",
    "\n",
    "    #######################################\n",
    "    # 3) ONNX Inference\n",
    "    #######################################\n",
    "    # Convert to NumPy for ONNX runtime\n",
    "    log_mel_np = log_mel.cpu().numpy()\n",
    "    log_mel_np = np.transpose(log_mel_np, (0, 2, 3, 1))\n",
    "    \n",
    "    # Run inference\n",
    "    outputs = session.run([output_name], {input_name: log_mel_np})\n",
    "    # outputs is a list; typically we want the first item\n",
    "    embedding = outputs[0]  # shape is [1, embedding_dim]\n",
    "\n",
    "    # print(\"Embedding[10]: \", embedding[0:10])  # Print the 10th element of the embedding\n",
    "    print(\"Embedding shape:\", embedding.shape)\n",
    "    # print(\"Embedding data:\\n\", embedding)\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded and checked ONNX model from: ReDimNet_no_mel_fp16.onnx\n",
      "Cropping log_mel from 1224 to 200 frames\n",
      "Embedding shape: (1, 192)\n",
      "Loaded and checked ONNX model from: ReDimNet_no_mel_fp16.onnx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0;93m2025-06-17 10:50:01.221221939 [W:onnxruntime:, constant_folding.cc:268 ApplyImpl] Could not find a CPU kernel and hence can't constant fold Sub node '/model/pool/Sub_1'\u001b[m\n",
      "\u001b[0;93m2025-06-17 10:50:01.227437589 [W:onnxruntime:, constant_folding.cc:268 ApplyImpl] Could not find a CPU kernel and hence can't constant fold Sub node '/model/pool/Sub_1'\u001b[m\n",
      "\u001b[0;93m2025-06-17 10:50:01.435890446 [W:onnxruntime:, constant_folding.cc:268 ApplyImpl] Could not find a CPU kernel and hence can't constant fold Sub node '/model/pool/Sub_1'\u001b[m\n",
      "\u001b[0;93m2025-06-17 10:50:01.444913557 [W:onnxruntime:, constant_folding.cc:268 ApplyImpl] Could not find a CPU kernel and hence can't constant fold Sub node '/model/pool/Sub_1'\u001b[m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padding log_mel from 108 to 200 frames\n",
      "Embedding shape: (1, 192)\n",
      "Loaded and checked ONNX model from: ReDimNet_no_mel_fp16.onnx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0;93m2025-06-17 10:50:01.727040099 [W:onnxruntime:, constant_folding.cc:268 ApplyImpl] Could not find a CPU kernel and hence can't constant fold Sub node '/model/pool/Sub_1'\u001b[m\n",
      "\u001b[0;93m2025-06-17 10:50:01.733174707 [W:onnxruntime:, constant_folding.cc:268 ApplyImpl] Could not find a CPU kernel and hence can't constant fold Sub node '/model/pool/Sub_1'\u001b[m\n",
      "\u001b[0;93m2025-06-17 10:50:01.926956763 [W:onnxruntime:, constant_folding.cc:268 ApplyImpl] Could not find a CPU kernel and hence can't constant fold Sub node '/model/pool/Sub_1'\u001b[m\n",
      "\u001b[0;93m2025-06-17 10:50:01.932968892 [W:onnxruntime:, constant_folding.cc:268 ApplyImpl] Could not find a CPU kernel and hence can't constant fold Sub node '/model/pool/Sub_1'\u001b[m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padding log_mel from 99 to 200 frames\n",
      "Embedding shape: (1, 192)\n",
      "Loaded and checked ONNX model from: ReDimNet_no_mel_fp16.onnx\n",
      "Padding log_mel from 118 to 200 frames\n",
      "Embedding shape: (1, 192)\n",
      "Loaded and checked ONNX model from: ReDimNet_no_mel_fp16.onnx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0;93m2025-06-17 10:50:02.129455291 [W:onnxruntime:, constant_folding.cc:268 ApplyImpl] Could not find a CPU kernel and hence can't constant fold Sub node '/model/pool/Sub_1'\u001b[m\n",
      "\u001b[0;93m2025-06-17 10:50:02.135165990 [W:onnxruntime:, constant_folding.cc:268 ApplyImpl] Could not find a CPU kernel and hence can't constant fold Sub node '/model/pool/Sub_1'\u001b[m\n",
      "\u001b[0;93m2025-06-17 10:50:02.388745703 [W:onnxruntime:, constant_folding.cc:268 ApplyImpl] Could not find a CPU kernel and hence can't constant fold Sub node '/model/pool/Sub_1'\u001b[m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cropping log_mel from 222 to 200 frames\n",
      "Embedding shape: (1, 192)\n",
      "Loaded and checked ONNX model from: ReDimNet_no_mel_fp16.onnx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0;93m2025-06-17 10:50:02.397914177 [W:onnxruntime:, constant_folding.cc:268 ApplyImpl] Could not find a CPU kernel and hence can't constant fold Sub node '/model/pool/Sub_1'\u001b[m\n",
      "\u001b[0;93m2025-06-17 10:50:02.656165153 [W:onnxruntime:, constant_folding.cc:268 ApplyImpl] Could not find a CPU kernel and hence can't constant fold Sub node '/model/pool/Sub_1'\u001b[m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cropping log_mel from 275 to 200 frames\n",
      "Embedding shape: (1, 192)\n",
      "Loaded and checked ONNX model from: ReDimNet_no_mel_fp16.onnx\n",
      "Cropping log_mel from 422 to 200 frames\n",
      "Embedding shape: (1, 192)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0;93m2025-06-17 10:50:02.663016571 [W:onnxruntime:, constant_folding.cc:268 ApplyImpl] Could not find a CPU kernel and hence can't constant fold Sub node '/model/pool/Sub_1'\u001b[m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "embed0 = run_inference_onnx(onnx_model_path, \"test000.wav\")\n",
    "embed1 = run_inference_onnx(onnx_model_path, \"testRob1.wav\")\n",
    "embed2 = run_inference_onnx(onnx_model_path, \"testRob2.wav\")\n",
    "embed3 = run_inference_onnx(onnx_model_path, \"testme1.wav\")\n",
    "embed4 = run_inference_onnx(onnx_model_path, \"testme2.wav\")\n",
    "embed5 = run_inference_onnx(onnx_model_path, \"pas_1.wav\")\n",
    "embed6 = run_inference_onnx(onnx_model_path, \"pas_2.wav\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity (robot to robot): 0.8523633480072021\n",
      "Similarity (robot to webvoice): 0.19561129808425903\n",
      "Similarity (robot to me1   ): 0.09194975346326828\n",
      "Similarity (robot to me2  ): 0.02281811833381653\n",
      "Similarity (me 1 to me 2  ): 0.35111984610557556\n",
      "Similarity (pas 1 to pas 2  ): 0.6781063675880432\n"
     ]
    }
   ],
   "source": [
    "print(f\"Similarity (robot to robot): {cosine_similarity_numpys(embed1, embed2)}\")\n",
    "print(f\"Similarity (robot to webvoice): {cosine_similarity_numpys(embed0, embed1)}\")\n",
    "print(f\"Similarity (robot to me1   ): {cosine_similarity_numpys(embed1, embed3)}\")\n",
    "print(f\"Similarity (robot to me2  ): {cosine_similarity_numpys(embed1, embed4)}\")\n",
    "print(f\"Similarity (me 1 to me 2  ): {cosine_similarity_numpys(embed3, embed4)}\")\n",
    "print(f\"Similarity (pas 1 to pas 2  ): {cosine_similarity_numpys(embed5, embed6)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.40856647,  1.5462252 , -1.7751795 , -4.7960186 , -5.26892   ,\n",
       "       -3.2609475 ,  0.9575441 ,  0.68722236, -2.0291152 , -2.566957  ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed1[0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cal fake data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* run in rknn docker:\n",
    "\n",
    "``` bash\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Directory for calibration inputs\n",
    "os.makedirs(\"calib_npy\", exist_ok=True)\n",
    "\n",
    "# Create 100 dummy log-mel tensors\n",
    "for i in range(10):\n",
    "    log_mel = torch.randn(1, 1, 60, 200).numpy().astype(np.float32)\n",
    "    np.save(f\"calib_npy/sample_{i}.npy\", log_mel)\n",
    "\n",
    "# Write dataset.txt listing all paths\n",
    "with open(\"dataset.txt\", \"w\") as f:\n",
    "    for i in range(10):\n",
    "        f.write(f\"calib_npy/sample_{i}.npy\\n\")\n",
    "\n",
    "\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## converts\n",
    "\n",
    "* python convert.py ReDimNet_no_mel.onnx  rk3588 fp ReDimNet_no_mel.rknn \n",
    "* python convert.py ReDimNet_no_mel.onnx  rv1106 i8  ReDimNet_no_mel.rknn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyvoice_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
