{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# common CACL utils with visual tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0+cu124\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "## our utils\n",
    "from utils.common_import import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## audio "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def audio_wav_to_waveform(wav_path):\n",
    "    waveform, sample_rate = torchaudio.load(wav_path)  # shape: [channels, time]\n",
    "    # If stereo, select one channel, or average:\n",
    "    if waveform.shape[0] > 1:\n",
    "        waveform = waveform.mean(dim=0, keepdim=True)\n",
    "\n",
    "    # Resample if needed\n",
    "    target_sample_rate=16000\n",
    "    if sample_rate != target_sample_rate:\n",
    "        resampler = T.Resample(orig_freq=sample_rate, new_freq=target_sample_rate)\n",
    "        waveform = resampler(waveform)\n",
    "        \n",
    "    return waveform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_wave_spec_calced(test_audio_file):\n",
    "    # calc waveform and mel spectrogram\n",
    "    waveform = audio_wav_to_waveform(test_audio_file)\n",
    "    mel_spec = waveform_to_logmel(waveform)\n",
    "    print(f\"Waveform shape: {waveform.shape} , type: {type(waveform)}\")\n",
    "    ## draw mel spectrogram\n",
    "    import matplotlib.pyplot as plt\n",
    "    def plot_mel_spectrogram(mel_spec):\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        plt.imshow(mel_spec.T, aspect='auto', origin='lower', cmap='viridis')\n",
    "        plt.title('Mel Spectrogram')\n",
    "        plt.xlabel('Time')\n",
    "        plt.ylabel('Mel Frequency Bands')\n",
    "        plt.colorbar(format='%+2.0f dB')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    # squeeze to 2D for imshow\n",
    "    mel_spec_2d = mel_spec.squeeze()\n",
    "    if isinstance(mel_spec_2d, torch.Tensor):\n",
    "        mel_spec_2d = mel_spec_2d.cpu().numpy()\n",
    "    # transpose if needed (time on x-axis)\n",
    "    if mel_spec_2d.shape[0] == 60:\n",
    "        mel_spec_2d = mel_spec_2d.T\n",
    "    # plot mel spectrogram\n",
    "    plot_mel_spectrogram(mel_spec_2d)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MEL CALC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------\n",
    "#  ReDimNet front-end settings (taken from the IDRnD repo defaults)\n",
    "#    • 16 kHz audio\n",
    "#    • pre-emphasis α = 0.97\n",
    "#    • 25 ms window  (400 samples)\n",
    "#    • 15 ms hop     (240 samples)  ➜ 134 frames for a 2-s clip\n",
    "#    • 60 Mel bins, 20 Hz → 8 kHz\n",
    "# ------------------------------------------------------------------\n",
    "_PREEMPH  = 0.97\n",
    "_SR       = 16_000\n",
    "_N_FFT    = 512\n",
    "_WIN_LEN  = 400\n",
    "_HOP      = 240\n",
    "_N_MELS   = 60\n",
    "_F_MIN    = 20.0\n",
    "_F_MAX    = 7600.0\n",
    "_EPS      = 1e-6            # numerical stability\n",
    "\n",
    "\n",
    "# Singleton MelSpectrogram so we build the kernel only once\n",
    "_mel_layer = T.MelSpectrogram(\n",
    "    sample_rate=_SR,\n",
    "    n_fft=_N_FFT,\n",
    "    win_length=_WIN_LEN,\n",
    "    hop_length=_HOP,\n",
    "    f_min=_F_MIN,\n",
    "    f_max=_F_MAX,\n",
    "    n_mels=_N_MELS,\n",
    "    power=2.0,               # the original uses power-spec → log10 later\n",
    "    center=True,\n",
    "    pad_mode=\"reflect\",\n",
    "    window_fn=torch.hamming_window\n",
    ")\n",
    "\n",
    "def _pre_emphasis(wave: torch.Tensor, alpha: float = _PREEMPH) -> torch.Tensor:\n",
    "    \"\"\"y[n] = x[n] − α·x[n−1] (first sample unchanged).\"\"\"\n",
    "    y = wave.clone()\n",
    "    y[:, 1:] = y[:, 1:] - alpha * y[:, :-1]\n",
    "    return y\n",
    "\n",
    "\n",
    "def pad_or_crop_logmel(log_mel, target_frames=200):\n",
    "    \"\"\"\n",
    "    Ensures log_mel is shaped [1, n_mels, target_frames] by:\n",
    "    - Padding with zeros on the right if too short\n",
    "    - Center-cropping if too long\n",
    "    \"\"\"\n",
    "    B, M, T = log_mel.shape\n",
    "    if T < target_frames:\n",
    "        pad_amt = target_frames - T\n",
    "        log_mel = F.pad(log_mel, (0, pad_amt))  # pad at end\n",
    "        print(f\"Padding log_mel from {T} to {target_frames} frames\")\n",
    "    elif T > target_frames:\n",
    "        start = (T - target_frames) // 2\n",
    "        log_mel = log_mel[:, :, start:start + target_frames]\n",
    "        print(f\"Cropping log_mel from {T} to {target_frames} frames\")\n",
    "    return log_mel\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def waveform_to_logmel(wave: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    wave : Tensor [B', T] | [1, T]\n",
    "        16-kHz mono waveform already trimmed / padded (32 000 samples for 2 s).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    log_mel : Tensor [B', 1, 60, frames]\n",
    "        Bit-exact front-end output expected by `model_no_mel`.\n",
    "    \"\"\"\n",
    "    \n",
    "    wave = wave[:,:32000]\n",
    "    print(f\"Input waveform shape: {wave.shape}\")\n",
    "    \n",
    "    # Make sure we always have a batch dimension\n",
    "    if wave.dim() == 1:      # (T,) → (1, T)\n",
    "        wave = wave.unsqueeze(0)\n",
    "    elif wave.dim() == 2 and wave.shape[0] > 1:\n",
    "        raise ValueError(\"Input must be mono; got multi-channel tensor.\")\n",
    "\n",
    "    # pre-emphasis\n",
    "    wave = _pre_emphasis(wave.float())\n",
    "\n",
    "    # Mel power-spectrogram\n",
    "    mel = _mel_layer(wave)\n",
    "    mel = torch.log(mel + 1e-6)          # → [B, 60, frames]\n",
    "\n",
    "    # log-scale (natural or log10 both work – log10 matches repo)\n",
    "    mel = mel - mel.mean(dim=-1, keepdim=True)\n",
    "\n",
    "    # pad/crop\n",
    "    mel = pad_or_crop_logmel(mel, target_frames=134)  # Ensure 200 frames\n",
    "\n",
    "    # add the dummy channel dim expected by Conv2d stem\n",
    "    mel = mel.unsqueeze(1)                # → [B, 1, 60, frames]\n",
    "\n",
    "    return mel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute similarity between two embeddings\n",
    "def cosine_similarity(embedding1, embedding2):\n",
    "    return F.cosine_similarity(embedding1, embedding2).item()\n",
    "\n",
    "def cosine_similarity_numpys(emb1: np.ndarray, emb2: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Compute cosine similarity between two vectors of shape (D,) or (1, D).\n",
    "    \"\"\"\n",
    "    # If shape is (1, D), flatten to (D,)\n",
    "    v1 = emb1.flatten()\n",
    "    v2 = emb2.flatten()\n",
    "\n",
    "    # dot product\n",
    "    dot = np.dot(v1, v2)\n",
    "    # norms\n",
    "    norm1 = np.linalg.norm(v1)\n",
    "    norm2 = np.linalg.norm(v2)\n",
    "\n",
    "    # Add a small epsilon in case of very small norms\n",
    "    sim = dot / (norm1 * norm2 + 1e-8)\n",
    "    return sim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ONNX Func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NHWCWrapper(nn.Module):\n",
    "    def __init__(self, model_nchw):\n",
    "        super().__init__()\n",
    "        self.model = model_nchw\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: NHWC => NCHW\n",
    "        x = x.permute(0, 3, 1, 2).contiguous()\n",
    "        return self.model(x)\n",
    "    \n",
    "    \n",
    "\n",
    "def export_to_onnx(model, onnx_path=\"ReDimNet_no_mel.onnx\"):\n",
    "    model.eval()\n",
    "    \n",
    "    # Create a dummy input with shape matching [B=1, 1, n_mels=60, time_frames=134] (example)\n",
    "    dummy_input = torch.randn(1, 1, 60, 134)\n",
    "\n",
    "    #  fixed-length segments \n",
    "    torch.onnx.export(\n",
    "        model,\n",
    "        dummy_input,\n",
    "        onnx_path,\n",
    "        input_names=[\"log_mel\"],\n",
    "        output_names=[\"embedding\"],\n",
    "        opset_version=17\n",
    "    )\n",
    "    \n",
    "    # # dynamic axes for variable time frames\n",
    "    # torch.onnx.export(\n",
    "    #     model,\n",
    "    #     dummy_input,\n",
    "    #     onnx_path,\n",
    "    #     input_names   = [\"log_mel\"],\n",
    "    #     output_names  = [\"embedding\"],\n",
    "    #     opset_version = 17,               # use a recent opset\n",
    "    #     dynamic_axes = {\n",
    "    #         # input  tensor : {axis_index : symbolic_name}\n",
    "    #         \"log_mel\"  : {0: \"batch\",   2: \"time\"},   # B and T now flexible\n",
    "    #         \"embedding\": {0: \"batch\"}                 # output length is fixed, batched\n",
    "    #     }\n",
    "    # ) \n",
    "    \n",
    "    print(\"Exported to\", onnx_path)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from onnxconverter_common.float16 import convert_float_to_float16\n",
    "\n",
    "def restore_in_half_precision(onnx_path, output_path):\n",
    "    \"\"\"\n",
    "    Convert an ONNX model to half precision (FP16).\n",
    "    \"\"\"\n",
    "    model_fp32 = onnx.load(onnx_path)\n",
    "    model_fp16 = convert_float_to_float16(model_fp32, keep_io_types=True)\n",
    "    onnx.save(model_fp16, output_path)\n",
    "    print(f\"Converted {onnx_path} to half precision and saved as {output_path}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyvoice_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
