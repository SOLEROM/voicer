{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RKNN fixed version \n",
    "\n",
    "* noMel\n",
    "* conv1d + fix for pad\n",
    "* newAVact - approximation of GELU \n",
    "\n",
    "===============================================================\n",
    "\n",
    "* build new noMel model based on base line\n",
    "* replace bad layers with working layers\n",
    "    * no need to change the width of the layers\n",
    "* run voices through the model and compare to baseline\n",
    "\n",
    "==============================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0+cu124\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "## our utils\n",
    "from utils.common_import import *\n",
    "from utils.test_all_voices import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-display\n",
    "import my_utils as myUtils\n",
    "from play1_setBase_line_B0 import original_model,base_line_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## original activations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACT_TYPES = (\n",
    "    nn.ReLU, nn.ReLU6, nn.LeakyReLU, nn.ELU, nn.PReLU, nn.GELU,\n",
    "    nn.SiLU, nn.Sigmoid, nn.Tanh, nn.Hardswish\n",
    ")\n",
    "\n",
    "def list_activations(model):\n",
    "    \"\"\"Print every module whose class is in ACT_TYPES.\"\"\"\n",
    "    for name, m in model.named_modules():\n",
    "        if isinstance(m, ACT_TYPES):\n",
    "            print(f'{name:<60} {m}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "backbone.stage0.3.conv_block.relu                            ReLU(inplace=True)\n",
      "backbone.stage0.4.conv_block.relu                            ReLU(inplace=True)\n",
      "backbone.stage0.6.tcm.0.act                                  GELU(approximate='none')\n",
      "backbone.stage0.6.tcm.1.act                                  GELU(approximate='none')\n",
      "backbone.stage0.6.tcm.2.act                                  GELU(approximate='none')\n",
      "backbone.stage0.6.tcm.3.act                                  GELU(approximate='none')\n",
      "backbone.stage1.3.conv_block.relu                            ReLU(inplace=True)\n",
      "backbone.stage1.4.conv_block.relu                            ReLU(inplace=True)\n",
      "backbone.stage1.5.conv_block.relu                            ReLU(inplace=True)\n",
      "backbone.stage1.6.2                                          GELU(approximate='none')\n",
      "backbone.stage1.8.tcm.0.act                                  GELU(approximate='none')\n",
      "backbone.stage1.8.tcm.1.act                                  GELU(approximate='none')\n",
      "backbone.stage1.8.tcm.2.act                                  GELU(approximate='none')\n",
      "backbone.stage1.8.tcm.3.act                                  GELU(approximate='none')\n",
      "backbone.stage2.3.conv_block.relu                            ReLU(inplace=True)\n",
      "backbone.stage2.4.conv_block.relu                            ReLU(inplace=True)\n",
      "backbone.stage2.5.conv_block.relu                            ReLU(inplace=True)\n",
      "backbone.stage2.6.2                                          GELU(approximate='none')\n",
      "backbone.stage2.8.tcm.0.act                                  GELU(approximate='none')\n",
      "backbone.stage2.8.tcm.1.act                                  GELU(approximate='none')\n",
      "backbone.stage2.8.tcm.2.act                                  GELU(approximate='none')\n",
      "backbone.stage2.8.tcm.3.act                                  GELU(approximate='none')\n",
      "backbone.stage3.3.conv_block.relu                            ReLU(inplace=True)\n",
      "backbone.stage3.4.conv_block.relu                            ReLU(inplace=True)\n",
      "backbone.stage3.5.conv_block.relu                            ReLU(inplace=True)\n",
      "backbone.stage3.6.conv_block.relu                            ReLU(inplace=True)\n",
      "backbone.stage3.7.2                                          GELU(approximate='none')\n",
      "backbone.stage3.9.tcm.0.act                                  GELU(approximate='none')\n",
      "backbone.stage3.9.tcm.1.act                                  GELU(approximate='none')\n",
      "backbone.stage3.9.tcm.2.act                                  GELU(approximate='none')\n",
      "backbone.stage3.9.tcm.3.act                                  GELU(approximate='none')\n",
      "backbone.stage4.3.conv_block.relu                            ReLU(inplace=True)\n",
      "backbone.stage4.4.conv_block.relu                            ReLU(inplace=True)\n",
      "backbone.stage4.5.conv_block.relu                            ReLU(inplace=True)\n",
      "backbone.stage4.7.tcm.0.act                                  GELU(approximate='none')\n",
      "backbone.stage4.7.tcm.1.act                                  GELU(approximate='none')\n",
      "backbone.stage4.7.tcm.2.act                                  GELU(approximate='none')\n",
      "backbone.stage4.7.tcm.3.act                                  GELU(approximate='none')\n"
     ]
    }
   ],
   "source": [
    "list_activations(original_model)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create new model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conv1dAs2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from math import floor\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "#  Global switches\n",
    "# ---------------------------------------------------------------------\n",
    "DEBUG = True                 # False → silence all prints\n",
    "MAX_NPU_KERNEL = 10          # Rockchip depth-wise limit (stride = 1)\n",
    "\n",
    "\n",
    "def _dbg(msg: str):\n",
    "    if DEBUG:\n",
    "        print(msg)\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "# 1.  Shape-safe Conv1d → Conv2d wrapper (NO weight edits)\n",
    "# =====================================================================\n",
    "class Conv1dAs2d(nn.Module):\n",
    "    \"\"\"\n",
    "    Wrap a Conv1d as Conv2d without touching its weights:\n",
    "      * accepts [B,C,T]  or  [B,C,T,1]\n",
    "      * gives the same rank back\n",
    "      * never uses 'same' | 'valid' strings → RKNN keeps pads inside node\n",
    "    \"\"\"\n",
    "    def __init__(self, src: nn.Conv1d):\n",
    "        super().__init__()\n",
    "\n",
    "        k = src.kernel_size[0]\n",
    "        d = src.dilation[0]\n",
    "        s = src.stride[0]\n",
    "        g = src.groups\n",
    "        in_c, out_c = src.in_channels, src.out_channels\n",
    "        pad_in = src.padding                     # \"same\"/\"valid\"/int/tuple\n",
    "\n",
    "        # numeric padding (H, W)\n",
    "        if isinstance(pad_in, str):\n",
    "            pad_num = floor(d * (k - 1) / 2) if pad_in == \"same\" else 0\n",
    "        else:\n",
    "            pad_num = pad_in[0] if isinstance(pad_in, tuple) else pad_in\n",
    "        pad_hw = (pad_num, 0)\n",
    "\n",
    "        # banner ------------------------------------------------------\n",
    "        place = \"CPU (k>10 depth-wise)\" if (k > MAX_NPU_KERNEL and g == in_c) else \"NPU\"\n",
    "        _dbg(f\"[Conv1dAs2d] {in_c}→{out_c}  k={k} d={d} s={s} g={g}  pad={pad_hw}  ⇒ {place}\")\n",
    "\n",
    "        # build Conv2d -----------------------------------------------\n",
    "        self.conv2d = nn.Conv2d(\n",
    "            in_channels=in_c,\n",
    "            out_channels=out_c,\n",
    "            kernel_size=(k, 1),\n",
    "            stride=(s, 1),\n",
    "            padding=pad_hw,\n",
    "            dilation=(d, 1),\n",
    "            groups=g,\n",
    "            bias=src.bias is not None,\n",
    "        )\n",
    "\n",
    "        # copy weights verbatim\n",
    "        with torch.no_grad():\n",
    "            self.conv2d.weight.copy_(src.weight.unsqueeze(-1))\n",
    "            if src.bias is not None:\n",
    "                self.conv2d.bias.copy_(src.bias)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        add_dim = False\n",
    "        if x.dim() == 3:                   # [B,C,T]  → [B,C,T,1]\n",
    "            x, add_dim = x.unsqueeze(-1), True\n",
    "        elif not (x.dim() == 4 and x.shape[-1] == 1):\n",
    "            raise ValueError(f\"Conv1dAs2d got shape {tuple(x.shape)}\")\n",
    "\n",
    "        y = self.conv2d(x)                 # Conv2d works on 4-D\n",
    "        return y.squeeze(-1) if add_dim else y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### split >10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from math import floor\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "#  Global settings\n",
    "# ------------------------------------------------------------\n",
    "DEBUG_CONV1D_AS_2D   = True            # set False to silence all prints\n",
    "MAX_NPU_KERNEL       = 10              # HW limit (stride = 1)\n",
    "MAX_SUB_KERNEL_SPLIT = 9               # odd, so symmetric pad works\n",
    "\n",
    "def _dbg(msg: str):\n",
    "    if DEBUG_CONV1D_AS_2D:\n",
    "        print(msg)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "#  Helper: split a large receptive field into odd sub-kernels ≤ 9\n",
    "#  For stride==1 the effective RF of a cascade is:\n",
    "#        k_eff = sum(k_i) - (n_stages - 1)\n",
    "# ------------------------------------------------------------\n",
    "def _split_kernel(k: int, k_max: int = MAX_SUB_KERNEL_SPLIT):\n",
    "    \"\"\"Return a list of odd kernel sizes whose cascade reproduces k.\"\"\"\n",
    "    if k <= k_max:\n",
    "        return [k]\n",
    "\n",
    "    segments = []\n",
    "    remaining = k\n",
    "    while remaining > k_max:\n",
    "        segments.append(k_max)             # add a full-size 9\n",
    "        remaining -= (k_max - 1)           # because RF grows by k_max-1\n",
    "    if remaining % 2 == 0:                 # make it odd (8 → 7)\n",
    "        remaining -= 1\n",
    "        segments[-1] += 1                  # compensate so RF stays exact\n",
    "    segments.append(remaining)\n",
    "    assert sum(segments) - (len(segments)-1) == k, \"RF mismatch\"\n",
    "    return segments\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "#  Main wrapper\n",
    "# ------------------------------------------------------------\n",
    "class Conv1dAs2d_split(nn.Module):\n",
    "    \"\"\"\n",
    "    * k ≤ 10  → single Conv2d (weights copied, runs on NPU)\n",
    "    * k  > 10 → cascade of Conv2d layers, every sub-kernel ≤ 9 (runs on NPU)\n",
    "                (weights are *not* copied; fine-tune is required)\n",
    "    Padding is always numeric – ONNX will not emit a standalone Pad op.\n",
    "    \"\"\"\n",
    "    def __init__(self, conv1d: nn.Conv1d):\n",
    "        super().__init__()\n",
    "\n",
    "        # ---------- original 1-D parameters ----------\n",
    "        k, d, s, g = conv1d.kernel_size[0], conv1d.dilation[0], conv1d.stride[0], conv1d.groups\n",
    "        in_c, out_c = conv1d.in_channels, conv1d.out_channels\n",
    "        pad_in = conv1d.padding                         # \"same\" | \"valid\" | int/tuple\n",
    "\n",
    "        if d != 1 or s != 1:\n",
    "            raise ValueError(\"Wrapper currently supports stride=1, dilation=1 only\")\n",
    "\n",
    "        # ----------------------------------------------------\n",
    "        # Case A — kernel already NPU-friendly\n",
    "        # ----------------------------------------------------\n",
    "        if k <= MAX_NPU_KERNEL:\n",
    "            pad_num = floor((k - 1) / 2) if isinstance(pad_in, str) and pad_in == \"same\" \\\n",
    "                      else (pad_in[0] if isinstance(pad_in, tuple) else pad_in)\n",
    "            pad_arg = (pad_num, 0)\n",
    "\n",
    "            self.conv = nn.Conv2d(\n",
    "                in_channels=in_c, out_channels=out_c,\n",
    "                kernel_size=(k, 1), stride=(1, 1),\n",
    "                padding=pad_arg, dilation=(1, 1),\n",
    "                groups=g, bias=conv1d.bias is not None\n",
    "            )\n",
    "            with torch.no_grad():\n",
    "                self.conv.weight.copy_(conv1d.weight.unsqueeze(-1))\n",
    "                if conv1d.bias is not None:\n",
    "                    self.conv.bias.copy_(conv1d.bias)\n",
    "\n",
    "            _dbg(f\"[Conv1dAs2d] k={k} → single Conv2d, pad={pad_arg}, runs on NPU\")\n",
    "\n",
    "        # ----------------------------------------------------\n",
    "        # Case B — kernel too wide → split into ≤9-tap stages\n",
    "        # ----------------------------------------------------\n",
    "        else:\n",
    "            k_list = _split_kernel(k)                   # e.g. 59 → [9,9,9,9,9,5]\n",
    "            layers = []\n",
    "            for i, ks in enumerate(k_list):\n",
    "                pad_num = (ks - 1) // 2                 # symmetric\n",
    "                conv2d = nn.Conv2d(\n",
    "                    in_channels=in_c, out_channels=out_c,\n",
    "                    kernel_size=(ks, 1), stride=(1, 1),\n",
    "                    padding=(pad_num, 0), dilation=(1, 1),\n",
    "                    groups=g, bias=False                # leave bias out; easier to fine-tune later\n",
    "                )\n",
    "                layers.append(conv2d)\n",
    "                _dbg(f\"[Conv1dAs2d]  ├─ stage {i}: ks={ks}, pad={pad_num}\")\n",
    "            self.conv = nn.Sequential(*layers)\n",
    "\n",
    "            _dbg(f\"[Conv1dAs2d] k={k} split into {k_list} (cascade runs on NPU)\\n\"\n",
    "                 \"           ⚠ weights not copied — fine-tune is required\")\n",
    "\n",
    "        _dbg(\"------------------------------------------------------------\")\n",
    "\n",
    "    # forward\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:   # x: [B, C, T]\n",
    "        return self.conv(x.unsqueeze(-1)).squeeze(-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### safe pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 1. RKNN-safe Attentive Statistics Pooling that matches any channel count\n",
    "# ---------------------------------------------------------------------\n",
    "class ASTP_RKNNSafe(nn.Module):\n",
    "    \"\"\"ASTP rewritten so linear1 expects in_channels (600 here).\"\"\"\n",
    "    def __init__(self, src_pool: nn.Module, in_channels: int):\n",
    "        super().__init__()\n",
    "        mid_channels = src_pool.linear1.out_channels   # 128\n",
    "        out_channels = in_channels                     # 600\n",
    "\n",
    "        # fresh 1x1 conv layers\n",
    "        self.linear1 = nn.Conv1d(in_channels, mid_channels, kernel_size=1, bias=True)\n",
    "        self.linear2 = nn.Conv1d(mid_channels, out_channels, kernel_size=1, bias=True)\n",
    "\n",
    "        # copy original weights where dimensions allow\n",
    "        with torch.no_grad():\n",
    "            # linear1: tile or truncate old weights to fit new in_channels\n",
    "            old_w1 = src_pool.linear1.weight           # [128, 1800, 1]\n",
    "            repeat = (in_channels + old_w1.size(1) - 1) // old_w1.size(1)\n",
    "            new_w1 = old_w1.repeat(1, repeat, 1)[:, :in_channels, :]\n",
    "            self.linear1.weight.copy_(new_w1)\n",
    "            self.linear1.bias.copy_(src_pool.linear1.bias)\n",
    "\n",
    "            # linear2: out_channels is 600, just copy first 600\n",
    "            self.linear2.weight.copy_(src_pool.linear2.weight[:, :out_channels, :])\n",
    "            self.linear2.bias.copy_(src_pool.linear2.bias[:out_channels])\n",
    "\n",
    "        # make them RKNN friendly\n",
    "        self.linear1 = Conv1dAs2d(self.linear1)\n",
    "        self.linear2 = Conv1dAs2d(self.linear2)\n",
    "        self.eps = getattr(src_pool, \"eps\", 1e-12)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn = torch.softmax(self.linear2(torch.tanh(self.linear1(x))), dim=-1)\n",
    "        mean = torch.sum(attn * x, dim=-1)\n",
    "        var  = torch.sum(attn * (x - mean.unsqueeze(-1)) ** 2, dim=-1)\n",
    "        std  = torch.pow(var + self.eps, 0.5)          # RKNN keeps Pow on NPU\n",
    "        return torch.cat([mean, std], dim=1)           # [B, 2 * C]\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ASTP_RKNNSafe_cpu(nn.Module):\n",
    "    \"\"\"CPU-safe attentive statistics pooling (mean + variance).\"\"\"\n",
    "    def __init__(self, src_pool: nn.Module, in_channels: int):\n",
    "        super().__init__()\n",
    "        mid = src_pool.linear1.out_channels   # 128\n",
    "        out = in_channels                     # 600\n",
    "\n",
    "        self.linear1 = nn.Conv1d(in_channels, mid, kernel_size=1, bias=True)\n",
    "        self.linear2 = nn.Conv1d(mid,        out, kernel_size=1, bias=True)\n",
    "\n",
    "        # copy or tile pretrained weights\n",
    "        with torch.no_grad():\n",
    "            w1 = src_pool.linear1.weight                 # [128,1800,1]\n",
    "            reps = (in_channels + w1.size(1) - 1) // w1.size(1)\n",
    "            self.linear1.weight.copy_(w1.repeat(1, reps, 1)[:, :in_channels, :])\n",
    "            self.linear1.bias.copy_(src_pool.linear1.bias)\n",
    "\n",
    "            self.linear2.weight.copy_(src_pool.linear2.weight[:, :out, :])\n",
    "            self.linear2.bias.copy_(src_pool.linear2.bias[:out])\n",
    "\n",
    "        self.eps = getattr(src_pool, \"eps\", 1e-12)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [B, C, T]\n",
    "        attn  = torch.softmax(self.linear2(torch.tanh(self.linear1(x))), dim=-1)\n",
    "        mean  = (attn * x).sum(dim=-1)                    # [B, C]\n",
    "\n",
    "        diff  = x - mean.unsqueeze(-1)                    # broadcast\n",
    "        sqr   = diff * diff                              # element-wise square, no Pow\n",
    "        var   = (attn * sqr).sum(dim=-1) + self.eps       # [B, C]\n",
    "\n",
    "        return torch.cat([mean, var], dim=1)              # [B, 2C]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### activations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewGELUActivation(nn.Module):\n",
    "    \"\"\"\n",
    "    tanh-based approximation of GELU from Hendrycks & Gimpel (2016):\n",
    "        0.5 * x * (1 + tanh( √(2/π) · (x + 0.044715 x³) ))\n",
    "    \"\"\"\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1.0 + torch.tanh(\n",
    "            (torch.sqrt(torch.tensor(2.0 / torch.pi, device=x.device)) * \n",
    "             (x + 0.044715 * torch.pow(x, 3)))\n",
    "        ))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_activation_(module, old_cls=nn.GELU, new_cls=nn.ReLU, **new_kwargs):\n",
    "    \"\"\"\n",
    "    In-place, recursive swap of every instance of `old_cls`\n",
    "    with `new_cls(**new_kwargs)`.\n",
    "    \"\"\"\n",
    "    for name, child in module.named_children():\n",
    "        if isinstance(child, old_cls):\n",
    "            setattr(module, name, new_cls(**new_kwargs))\n",
    "        else:\n",
    "            replace_activation_(child, old_cls, new_cls, **new_kwargs)\n",
    "# -----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReDimNetNoMel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Conv1dAs2d] k=7 → single Conv2d, pad=(3, 0), runs on NPU\n",
      "------------------------------------------------------------\n",
      "[Conv1dAs2d] k=1 → single Conv2d, pad=(0, 0), runs on NPU\n",
      "------------------------------------------------------------\n",
      "[Conv1dAs2d]  ├─ stage 0: ks=9, pad=4\n",
      "[Conv1dAs2d]  ├─ stage 1: ks=9, pad=4\n",
      "[Conv1dAs2d]  ├─ stage 2: ks=3, pad=1\n",
      "[Conv1dAs2d] k=19 split into [9, 9, 3] (cascade runs on NPU)\n",
      "           ⚠ weights not copied — fine-tune is required\n",
      "------------------------------------------------------------\n",
      "[Conv1dAs2d] k=1 → single Conv2d, pad=(0, 0), runs on NPU\n",
      "------------------------------------------------------------\n",
      "[Conv1dAs2d]  ├─ stage 0: ks=9, pad=4\n",
      "[Conv1dAs2d]  ├─ stage 1: ks=9, pad=4\n",
      "[Conv1dAs2d]  ├─ stage 2: ks=9, pad=4\n",
      "[Conv1dAs2d]  ├─ stage 3: ks=7, pad=3\n",
      "[Conv1dAs2d] k=31 split into [9, 9, 9, 7] (cascade runs on NPU)\n",
      "           ⚠ weights not copied — fine-tune is required\n",
      "------------------------------------------------------------\n",
      "[Conv1dAs2d] k=1 → single Conv2d, pad=(0, 0), runs on NPU\n",
      "------------------------------------------------------------\n",
      "[Conv1dAs2d]  ├─ stage 0: ks=9, pad=4\n",
      "[Conv1dAs2d]  ├─ stage 1: ks=9, pad=4\n",
      "[Conv1dAs2d]  ├─ stage 2: ks=9, pad=4\n",
      "[Conv1dAs2d]  ├─ stage 3: ks=9, pad=4\n",
      "[Conv1dAs2d]  ├─ stage 4: ks=9, pad=4\n",
      "[Conv1dAs2d]  ├─ stage 5: ks=9, pad=4\n",
      "[Conv1dAs2d]  ├─ stage 6: ks=9, pad=4\n",
      "[Conv1dAs2d]  ├─ stage 7: ks=3, pad=1\n",
      "[Conv1dAs2d] k=59 split into [9, 9, 9, 9, 9, 9, 9, 3] (cascade runs on NPU)\n",
      "           ⚠ weights not copied — fine-tune is required\n",
      "------------------------------------------------------------\n",
      "[Conv1dAs2d] k=1 → single Conv2d, pad=(0, 0), runs on NPU\n",
      "------------------------------------------------------------\n",
      "[Conv1dAs2d] k=7 → single Conv2d, pad=(3, 0), runs on NPU\n",
      "------------------------------------------------------------\n",
      "[Conv1dAs2d] k=1 → single Conv2d, pad=(0, 0), runs on NPU\n",
      "------------------------------------------------------------\n",
      "[Conv1dAs2d]  ├─ stage 0: ks=9, pad=4\n",
      "[Conv1dAs2d]  ├─ stage 1: ks=9, pad=4\n",
      "[Conv1dAs2d]  ├─ stage 2: ks=3, pad=1\n",
      "[Conv1dAs2d] k=19 split into [9, 9, 3] (cascade runs on NPU)\n",
      "           ⚠ weights not copied — fine-tune is required\n",
      "------------------------------------------------------------\n",
      "[Conv1dAs2d] k=1 → single Conv2d, pad=(0, 0), runs on NPU\n",
      "------------------------------------------------------------\n",
      "[Conv1dAs2d]  ├─ stage 0: ks=9, pad=4\n",
      "[Conv1dAs2d]  ├─ stage 1: ks=9, pad=4\n",
      "[Conv1dAs2d]  ├─ stage 2: ks=9, pad=4\n",
      "[Conv1dAs2d]  ├─ stage 3: ks=7, pad=3\n",
      "[Conv1dAs2d] k=31 split into [9, 9, 9, 7] (cascade runs on NPU)\n",
      "           ⚠ weights not copied — fine-tune is required\n",
      "------------------------------------------------------------\n",
      "[Conv1dAs2d] k=1 → single Conv2d, pad=(0, 0), runs on NPU\n",
      "------------------------------------------------------------\n",
      "[Conv1dAs2d]  ├─ stage 0: ks=9, pad=4\n",
      "[Conv1dAs2d]  ├─ stage 1: ks=9, pad=4\n",
      "[Conv1dAs2d]  ├─ stage 2: ks=9, pad=4\n",
      "[Conv1dAs2d]  ├─ stage 3: ks=9, pad=4\n",
      "[Conv1dAs2d]  ├─ stage 4: ks=9, pad=4\n",
      "[Conv1dAs2d]  ├─ stage 5: ks=9, pad=4\n",
      "[Conv1dAs2d]  ├─ stage 6: ks=9, pad=4\n",
      "[Conv1dAs2d]  ├─ stage 7: ks=3, pad=1\n",
      "[Conv1dAs2d] k=59 split into [9, 9, 9, 9, 9, 9, 9, 3] (cascade runs on NPU)\n",
      "           ⚠ weights not copied — fine-tune is required\n",
      "------------------------------------------------------------\n",
      "[Conv1dAs2d] k=1 → single Conv2d, pad=(0, 0), runs on NPU\n",
      "------------------------------------------------------------\n",
      "[Conv1dAs2d] k=7 → single Conv2d, pad=(3, 0), runs on NPU\n",
      "------------------------------------------------------------\n",
      "[Conv1dAs2d] k=1 → single Conv2d, pad=(0, 0), runs on NPU\n",
      "------------------------------------------------------------\n",
      "[Conv1dAs2d]  ├─ stage 0: ks=9, pad=4\n",
      "[Conv1dAs2d]  ├─ stage 1: ks=9, pad=4\n",
      "[Conv1dAs2d]  ├─ stage 2: ks=3, pad=1\n",
      "[Conv1dAs2d] k=19 split into [9, 9, 3] (cascade runs on NPU)\n",
      "           ⚠ weights not copied — fine-tune is required\n",
      "------------------------------------------------------------\n",
      "[Conv1dAs2d] k=1 → single Conv2d, pad=(0, 0), runs on NPU\n",
      "------------------------------------------------------------\n",
      "[Conv1dAs2d]  ├─ stage 0: ks=9, pad=4\n",
      "[Conv1dAs2d]  ├─ stage 1: ks=9, pad=4\n",
      "[Conv1dAs2d]  ├─ stage 2: ks=9, pad=4\n",
      "[Conv1dAs2d]  ├─ stage 3: ks=7, pad=3\n",
      "[Conv1dAs2d] k=31 split into [9, 9, 9, 7] (cascade runs on NPU)\n",
      "           ⚠ weights not copied — fine-tune is required\n",
      "------------------------------------------------------------\n",
      "[Conv1dAs2d] k=1 → single Conv2d, pad=(0, 0), runs on NPU\n",
      "------------------------------------------------------------\n",
      "[Conv1dAs2d]  ├─ stage 0: ks=9, pad=4\n",
      "[Conv1dAs2d]  ├─ stage 1: ks=9, pad=4\n",
      "[Conv1dAs2d]  ├─ stage 2: ks=9, pad=4\n",
      "[Conv1dAs2d]  ├─ stage 3: ks=9, pad=4\n",
      "[Conv1dAs2d]  ├─ stage 4: ks=9, pad=4\n",
      "[Conv1dAs2d]  ├─ stage 5: ks=9, pad=4\n",
      "[Conv1dAs2d]  ├─ stage 6: ks=9, pad=4\n",
      "[Conv1dAs2d]  ├─ stage 7: ks=3, pad=1\n",
      "[Conv1dAs2d] k=59 split into [9, 9, 9, 9, 9, 9, 9, 3] (cascade runs on NPU)\n",
      "           ⚠ weights not copied — fine-tune is required\n",
      "------------------------------------------------------------\n",
      "[Conv1dAs2d] k=1 → single Conv2d, pad=(0, 0), runs on NPU\n",
      "------------------------------------------------------------\n",
      "[Conv1dAs2d] k=7 → single Conv2d, pad=(3, 0), runs on NPU\n",
      "------------------------------------------------------------\n",
      "[Conv1dAs2d] k=1 → single Conv2d, pad=(0, 0), runs on NPU\n",
      "------------------------------------------------------------\n",
      "[Conv1dAs2d]  ├─ stage 0: ks=9, pad=4\n",
      "[Conv1dAs2d]  ├─ stage 1: ks=9, pad=4\n",
      "[Conv1dAs2d]  ├─ stage 2: ks=3, pad=1\n",
      "[Conv1dAs2d] k=19 split into [9, 9, 3] (cascade runs on NPU)\n",
      "           ⚠ weights not copied — fine-tune is required\n",
      "------------------------------------------------------------\n",
      "[Conv1dAs2d] k=1 → single Conv2d, pad=(0, 0), runs on NPU\n",
      "------------------------------------------------------------\n",
      "[Conv1dAs2d]  ├─ stage 0: ks=9, pad=4\n",
      "[Conv1dAs2d]  ├─ stage 1: ks=9, pad=4\n",
      "[Conv1dAs2d]  ├─ stage 2: ks=9, pad=4\n",
      "[Conv1dAs2d]  ├─ stage 3: ks=7, pad=3\n",
      "[Conv1dAs2d] k=31 split into [9, 9, 9, 7] (cascade runs on NPU)\n",
      "           ⚠ weights not copied — fine-tune is required\n",
      "------------------------------------------------------------\n",
      "[Conv1dAs2d] k=1 → single Conv2d, pad=(0, 0), runs on NPU\n",
      "------------------------------------------------------------\n",
      "[Conv1dAs2d]  ├─ stage 0: ks=9, pad=4\n",
      "[Conv1dAs2d]  ├─ stage 1: ks=9, pad=4\n",
      "[Conv1dAs2d]  ├─ stage 2: ks=9, pad=4\n",
      "[Conv1dAs2d]  ├─ stage 3: ks=9, pad=4\n",
      "[Conv1dAs2d]  ├─ stage 4: ks=9, pad=4\n",
      "[Conv1dAs2d]  ├─ stage 5: ks=9, pad=4\n",
      "[Conv1dAs2d]  ├─ stage 6: ks=9, pad=4\n",
      "[Conv1dAs2d]  ├─ stage 7: ks=3, pad=1\n",
      "[Conv1dAs2d] k=59 split into [9, 9, 9, 9, 9, 9, 9, 3] (cascade runs on NPU)\n",
      "           ⚠ weights not copied — fine-tune is required\n",
      "------------------------------------------------------------\n",
      "[Conv1dAs2d] k=1 → single Conv2d, pad=(0, 0), runs on NPU\n",
      "------------------------------------------------------------\n",
      "[Conv1dAs2d] k=7 → single Conv2d, pad=(3, 0), runs on NPU\n",
      "------------------------------------------------------------\n",
      "[Conv1dAs2d] k=1 → single Conv2d, pad=(0, 0), runs on NPU\n",
      "------------------------------------------------------------\n",
      "[Conv1dAs2d]  ├─ stage 0: ks=9, pad=4\n",
      "[Conv1dAs2d]  ├─ stage 1: ks=9, pad=4\n",
      "[Conv1dAs2d]  ├─ stage 2: ks=3, pad=1\n",
      "[Conv1dAs2d] k=19 split into [9, 9, 3] (cascade runs on NPU)\n",
      "           ⚠ weights not copied — fine-tune is required\n",
      "------------------------------------------------------------\n",
      "[Conv1dAs2d] k=1 → single Conv2d, pad=(0, 0), runs on NPU\n",
      "------------------------------------------------------------\n",
      "[Conv1dAs2d]  ├─ stage 0: ks=9, pad=4\n",
      "[Conv1dAs2d]  ├─ stage 1: ks=9, pad=4\n",
      "[Conv1dAs2d]  ├─ stage 2: ks=9, pad=4\n",
      "[Conv1dAs2d]  ├─ stage 3: ks=7, pad=3\n",
      "[Conv1dAs2d] k=31 split into [9, 9, 9, 7] (cascade runs on NPU)\n",
      "           ⚠ weights not copied — fine-tune is required\n",
      "------------------------------------------------------------\n",
      "[Conv1dAs2d] k=1 → single Conv2d, pad=(0, 0), runs on NPU\n",
      "------------------------------------------------------------\n",
      "[Conv1dAs2d]  ├─ stage 0: ks=9, pad=4\n",
      "[Conv1dAs2d]  ├─ stage 1: ks=9, pad=4\n",
      "[Conv1dAs2d]  ├─ stage 2: ks=9, pad=4\n",
      "[Conv1dAs2d]  ├─ stage 3: ks=9, pad=4\n",
      "[Conv1dAs2d]  ├─ stage 4: ks=9, pad=4\n",
      "[Conv1dAs2d]  ├─ stage 5: ks=9, pad=4\n",
      "[Conv1dAs2d]  ├─ stage 6: ks=9, pad=4\n",
      "[Conv1dAs2d]  ├─ stage 7: ks=3, pad=1\n",
      "[Conv1dAs2d] k=59 split into [9, 9, 9, 9, 9, 9, 9, 3] (cascade runs on NPU)\n",
      "           ⚠ weights not copied — fine-tune is required\n",
      "------------------------------------------------------------\n",
      "[Conv1dAs2d] k=1 → single Conv2d, pad=(0, 0), runs on NPU\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "########################################\n",
    "# 2) Define a Model Class without MelBanks\n",
    "########################################\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ReDimNetNoMel(nn.Module):\n",
    "    \"\"\"\n",
    "    A wrapper around the original ReDimNetWrap that:\n",
    "      - Excludes the 'spec' (MelBanks) module\n",
    "      - Uses 'backbone', 'pool', 'bn', and 'linear'\n",
    "    We expect a precomputed mel spectrogram as input with shape [B, 1, n_mels, time_frames].\n",
    "    \"\"\"\n",
    "    def __init__(self, original_wrap):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Grab references to the submodules we want to keep\n",
    "        self.backbone = original_wrap.backbone\n",
    "        \n",
    "        # fix problem01\n",
    "        # list of (stage, block) indices you already know are problematic\n",
    "        TARGETS = [(0, 6), (1, 8), (2, 8), (3, 9), (4, 7)]\n",
    "        for s_idx, b_idx in TARGETS:\n",
    "            for tcm_idx in range(4):\n",
    "                block = self.backbone.__getattr__(f\"stage{s_idx}\")[b_idx].tcm[tcm_idx]\n",
    "\n",
    "                # # test ident\n",
    "                # block.dwconvs[0] = nn.Identity()\n",
    "                # block.pwconv1    = nn.Identity()\n",
    "\n",
    "                ## original fix\n",
    "                # block.dwconvs[0] = Conv1dAs2d(block.dwconvs[0])\n",
    "                # block.pwconv1    = Conv1dAs2d(block.pwconv1)   # 1×1 conv\n",
    "                \n",
    "                # test with split\n",
    "                block.dwconvs[0] = Conv1dAs2d_split(block.dwconvs[0])\n",
    "                block.pwconv1    = Conv1dAs2d_split(block.pwconv1)   # 1×1 conv\n",
    "                \n",
    "                \n",
    "                \n",
    "        # #orignal before\n",
    "        # self.pool   = original_wrap.pool\n",
    "        # # ------------------------------------------------------------------\n",
    "        # # 2. patch the pool (ASTP) – its two \"linear\" Conv1d layers\n",
    "        # # ------------------------------------------------------------------\n",
    "        # self.pool.linear1 = Conv1dAs2d(self.pool.linear1)   # 1 × 1 conv 1800→128\n",
    "        # self.pool.linear2 = Conv1dAs2d(self.pool.linear2)   # 1 × 1 conv 128 →600\n",
    "\n",
    "\n",
    "        # # # --- pooling --> RKNN-safe variant ------------------------------------\n",
    "        # # replace pool with the new channel-aware version\n",
    "        # with torch.no_grad():\n",
    "        #     dummy = torch.zeros(1, 1, 60, 134)\n",
    "        #     out_channels = self.backbone(dummy).shape[1]   # 600\n",
    "        # self.pool = ASTP_RKNNSafe(original_wrap.pool, out_channels)\n",
    "        \n",
    "        \n",
    "        \n",
    "         # find backbone output channels once\n",
    "        with torch.no_grad():\n",
    "            c_backbone = self.backbone(torch.zeros(1, 1, 60, 134)).shape[1]  # 600\n",
    "        # pooling block now entirely on CPU (no Conv1dAs2d)\n",
    "        self.pool = ASTP_RKNNSafe_cpu(original_wrap.pool, c_backbone)\n",
    "\n",
    "        \n",
    "        # ---------- tail ----------\n",
    "        self.bn = original_wrap.bn\n",
    "        self.linear = original_wrap.linear\n",
    "        \n",
    "        ## Replace activations in the backbone\n",
    "        replace_activation_(self, old_cls=nn.GELU, new_cls=NewGELUActivation)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: shape [B, 1, n_mels, time_frames]\n",
    "        # (1) Pass through the backbone\n",
    "        x = self.backbone(x)    # shape might become [B, channels, frames] or similar\n",
    "        # (2) Pooling\n",
    "        x = self.pool(x)        # ASTP => shape likely [B, embedding_dim]\n",
    "        # (3) BatchNorm\n",
    "        x = self.bn(x)\n",
    "        # (4) Final linear => 192-dim (if that's your embedding size)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Create an instance of our new model that skips the MelBanks front-end\n",
    "model_no_mel = ReDimNetNoMel(original_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 7.1901e-01, -1.8323e+00, -8.9389e-01, -3.8529e-02,  6.3565e-01,\n",
       "          1.0317e-01,  1.7315e+00,  1.4600e+00,  1.6053e+00,  3.4754e+00,\n",
       "          3.9074e+00, -9.1820e-01, -4.5573e+00, -3.9538e+00, -8.2337e+00,\n",
       "          3.3023e+00, -2.6103e+00, -2.8659e+00, -3.6192e-01, -5.9225e+00,\n",
       "          3.8702e+00,  4.2438e+00,  8.9908e-01,  5.8774e+00,  2.1006e+00,\n",
       "         -3.0569e+00,  2.9531e+00, -6.4767e+00, -6.7438e-01, -2.8426e+00,\n",
       "         -7.8291e+00, -4.9874e+00, -4.5475e+00,  5.8298e-01,  2.3136e+00,\n",
       "          1.8101e+00, -8.5856e-01, -1.6696e+00,  5.6260e+00, -4.0882e+00,\n",
       "         -9.6633e-01,  5.3925e+00,  3.6782e-01, -2.9767e+00,  8.6189e+00,\n",
       "          2.3005e+00, -3.6641e+00,  2.7388e+00,  6.8082e-01,  2.1704e+00,\n",
       "         -6.0969e-01, -4.9366e+00, -1.4689e+00,  8.6941e+00, -3.5627e+00,\n",
       "         -2.8654e+00, -7.9509e-01, -3.3555e+00,  2.2370e+00, -1.8435e+00,\n",
       "         -1.7426e+00, -2.6053e+00,  8.5533e-01,  3.0613e+00,  1.3642e+00,\n",
       "          4.8716e+00, -6.8409e-02,  2.1619e+00,  5.5944e+00, -3.1865e+00,\n",
       "          1.6395e+00, -5.2769e+00, -5.6568e+00, -1.5779e+00, -1.0827e+00,\n",
       "         -3.9708e+00, -1.6194e+00,  1.3419e+00, -3.2964e+00, -2.5670e+00,\n",
       "         -5.1328e+00,  6.4549e+00,  3.1026e+00, -7.5744e-01,  5.0162e+00,\n",
       "          1.1180e+00,  1.3993e+00, -1.6128e+00,  3.7755e-01,  2.7114e+00,\n",
       "         -1.5816e+00,  3.1002e+00,  5.8027e-01,  3.9632e+00, -2.3106e-01,\n",
       "          7.7671e-01,  3.6124e+00,  1.5488e+00,  7.0810e-01, -1.7092e+00,\n",
       "          8.0954e-01,  4.8299e+00, -1.6968e+00, -1.5529e+00, -1.2413e+00,\n",
       "         -3.6170e+00,  7.2956e-01,  3.8990e+00,  5.8022e+00, -5.0423e+00,\n",
       "          1.4828e+00,  1.7315e+00,  3.4658e+00, -3.4984e+00,  2.3660e+00,\n",
       "         -2.1322e+00,  3.2322e+00, -1.3439e+00, -2.8926e+00,  9.8938e-01,\n",
       "         -3.0789e+00, -1.8009e-01,  3.0137e+00, -2.3235e+00,  1.5061e+00,\n",
       "         -1.7709e+00, -4.1749e+00, -7.6285e-01,  1.5752e+00, -2.7901e+00,\n",
       "         -4.9311e+00, -1.1349e-03,  5.5243e+00, -1.2102e+00, -2.2640e+00,\n",
       "          2.3476e-02,  1.5582e+00,  5.8828e+00,  3.5915e+00, -1.6385e+00,\n",
       "          1.9939e+00, -1.4402e+00, -3.5620e+00,  1.8593e+00, -4.8196e+00,\n",
       "          2.3190e+00,  1.7585e+00,  2.9819e+00, -3.5840e+00,  2.0506e+00,\n",
       "         -3.2603e+00,  7.6529e-01,  4.6762e-01, -1.2121e+00, -2.5505e+00,\n",
       "         -5.0992e+00, -4.3935e-01,  8.5355e-01,  9.7953e-01, -3.0184e+00,\n",
       "         -3.3726e-01, -1.6627e+00, -1.6450e+00, -2.8658e+00,  1.5295e-01,\n",
       "         -1.5026e+00, -1.3192e-01,  2.1241e-01, -2.9972e+00,  2.1208e+00,\n",
       "          3.4917e+00, -3.5275e+00,  1.5874e+00, -1.5326e+00,  9.3328e-01,\n",
       "          1.7777e+00, -1.4988e+00,  4.0138e+00, -2.3262e+00, -2.4396e+00,\n",
       "         -2.2762e+00, -1.5216e+00, -6.8982e+00,  2.0553e+00,  2.4937e+00,\n",
       "         -3.4298e+00, -1.6662e+00, -2.7311e+00,  4.9105e+00, -3.4362e-01,\n",
       "          1.0027e+00,  2.6005e+00]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_no_mel.eval()  # <- this line is critical!\n",
    "dummy = torch.randn(1, 1, 60, 134)\n",
    "model_no_mel(dummy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PADS?\n",
    "\n",
    "\n",
    "Conv1dAs2d deliberately switches to padding='same' whenever\n",
    "pad_num (= (k-1)//2) > max_pad (currently 4).\n",
    "PyTorch → ONNX keeps that as the auto_pad attribute, and RKNN reacts by\n",
    "materialising an explicit Pad node that it then assigns to the CPU,\n",
    "which breaks compilation.\n",
    "\n",
    "Hardware can handle large numeric pads – it merely dislikes auto_pad.\n",
    "\n",
    "```\n",
    "       (dwconvs): ModuleList(\n",
    "              (0): Conv1dAs2d(\n",
    "                (conv2d): Conv2d(20, 20, kernel_size=(59, 1), stride=(1, 1), padding=same, groups=20)\n",
    "              )\n",
    "            )\n",
    "            (norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "            (act): NewGELUActivation()\n",
    "            (pwconv1): Conv1dAs2d(\n",
    "              (conv2d): Conv2d(20, 20, kernel_size=(1, 1), stride=(1, 1))\n",
    "            )\n",
    "\n",
    "```\n",
    "\n",
    " unsupport cpu Pad op, op name: Pad:/backbone/stage0/stage0.6/tcm/tcm.1/dwconvs.0/Unsqueeze_output_0_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Conv2d layers with padding mode \"same\" ===\n",
      "  backbone.stem.0                                               k= 3  d= 1  mode=\"same\"\n",
      "  backbone.stage1.6.0                                           k= 3  d= 1  mode=\"same\"\n",
      "  backbone.stage2.6.0                                           k= 3  d= 1  mode=\"same\"\n",
      "  backbone.stage3.7.0                                           k= 3  d= 1  mode=\"same\"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "# 4. Inside PyTorch: list every Conv2d that still uses \"padding=\\'same\\'\"\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "print('\\n=== Conv2d layers with padding mode \"same\" ===')\n",
    "for name, mod in model_no_mel.named_modules():\n",
    "    if isinstance(mod, torch.nn.Conv2d) and isinstance(mod.padding, str):\n",
    "        k, _ = mod.kernel_size\n",
    "        d, _ = mod.dilation\n",
    "        print(f'  {name:60s}  k={k:2d}  d={d:2d}  mode=\"same\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SIZE?\n",
    "\n",
    "For stride = 1 the silicon docs spell it out:\n",
    "\n",
    "    “The minimum supported kernel size is 1 and the maximum is 11 × stride – 1.”\n",
    "    dl.radxa.com\n",
    "\n",
    "With stride = 1 the ceiling is 10. Anything wider forces RKNN to split the layer into a CPU-side Pad ➜ Conv pair, and the runtime you’re using still lacks a CPU implementation of Pad, so the build fails even though the Conv itself would happily run on the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All Conv2d layers are within RKNN limits.\n"
     ]
    }
   ],
   "source": [
    "LIMIT = 10         # 11*stride - 1 with stride = 1\n",
    "\n",
    "bad = []\n",
    "for name, m in model_no_mel.named_modules():\n",
    "    if isinstance(m, torch.nn.Conv2d):\n",
    "        k, _ = m.kernel_size\n",
    "        pad, _ = (m.padding if not isinstance(m.padding, str)\n",
    "                   else (floor((k-1)/2), 0))          # same-padding case\n",
    "        if k > LIMIT or pad > 4:\n",
    "            bad.append((name, k, pad))\n",
    "\n",
    "if not bad:\n",
    "    print(\"✓ All Conv2d layers are within RKNN limits.\")\n",
    "else:\n",
    "    print(\"✗ Layers that violate RKNN limits:\")\n",
    "    for n,k,p in bad:\n",
    "        print(f\"   {n:60s}  k={k}  pad={p}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FP16 check\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "safe in pure FP16? tensor(True)\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    fp16_net = copy.deepcopy(model_no_mel).half().eval()\n",
    "    ok = torch.isfinite(fp16_net(dummy.half())).all()\n",
    "    print('safe in pure FP16?', ok)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "backbone.stage0.3.conv_block.relu                            ReLU(inplace=True)\n",
      "backbone.stage0.4.conv_block.relu                            ReLU(inplace=True)\n",
      "backbone.stage1.3.conv_block.relu                            ReLU(inplace=True)\n",
      "backbone.stage1.4.conv_block.relu                            ReLU(inplace=True)\n",
      "backbone.stage1.5.conv_block.relu                            ReLU(inplace=True)\n",
      "backbone.stage2.3.conv_block.relu                            ReLU(inplace=True)\n",
      "backbone.stage2.4.conv_block.relu                            ReLU(inplace=True)\n",
      "backbone.stage2.5.conv_block.relu                            ReLU(inplace=True)\n",
      "backbone.stage3.3.conv_block.relu                            ReLU(inplace=True)\n",
      "backbone.stage3.4.conv_block.relu                            ReLU(inplace=True)\n",
      "backbone.stage3.5.conv_block.relu                            ReLU(inplace=True)\n",
      "backbone.stage3.6.conv_block.relu                            ReLU(inplace=True)\n",
      "backbone.stage4.3.conv_block.relu                            ReLU(inplace=True)\n",
      "backbone.stage4.4.conv_block.relu                            ReLU(inplace=True)\n",
      "backbone.stage4.5.conv_block.relu                            ReLU(inplace=True)\n"
     ]
    }
   ],
   "source": [
    "list_activations(model_no_mel)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ReDimNetNoMel(\n",
       "  (backbone): ReDimNet(\n",
       "    (stem): Sequential(\n",
       "      (0): Conv2d(1, 10, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "      (1): LayerNorm(C=(10,), data_format=channels_first, eps=1e-06)\n",
       "      (2): to1d()\n",
       "    )\n",
       "    (stage0): Sequential(\n",
       "      (0): weigth1d(w=(1, 1, 1, 1),sequential=False)\n",
       "      (1): to2d(f=60,c=10)\n",
       "      (2): Conv2d(10, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (3): ConvBlock2d(\n",
       "        (conv_block): ResBasicBlock(\n",
       "          (conv1): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=10, bias=False)\n",
       "          (conv1pw): Conv2d(10, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=10, bias=False)\n",
       "          (conv2pw): Conv2d(10, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (se): Identity()\n",
       "          (downsample): Identity()\n",
       "        )\n",
       "      )\n",
       "      (4): ConvBlock2d(\n",
       "        (conv_block): ResBasicBlock(\n",
       "          (conv1): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=10, bias=False)\n",
       "          (conv1pw): Conv2d(10, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=10, bias=False)\n",
       "          (conv2pw): Conv2d(10, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (se): Identity()\n",
       "          (downsample): Identity()\n",
       "        )\n",
       "      )\n",
       "      (5): to1d()\n",
       "      (6): TimeContextBlock1d(\n",
       "        (red_dim_conv): Sequential(\n",
       "          (0): Conv1d(600, 20, kernel_size=(1,), stride=(1,))\n",
       "          (1): LayerNorm(C=(20,), data_format=channels_first, eps=1e-06)\n",
       "        )\n",
       "        (tcm): Sequential(\n",
       "          (0): ConvNeXtLikeBlock(\n",
       "            (dwconvs): ModuleList(\n",
       "              (0): Conv1dAs2d_split(\n",
       "                (conv): Conv2d(20, 20, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), groups=20)\n",
       "              )\n",
       "            )\n",
       "            (norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act): NewGELUActivation()\n",
       "            (pwconv1): Conv1dAs2d_split(\n",
       "              (conv): Conv2d(20, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "          )\n",
       "          (1): ConvNeXtLikeBlock(\n",
       "            (dwconvs): ModuleList(\n",
       "              (0): Conv1dAs2d_split(\n",
       "                (conv): Sequential(\n",
       "                  (0): Conv2d(20, 20, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), groups=20, bias=False)\n",
       "                  (1): Conv2d(20, 20, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), groups=20, bias=False)\n",
       "                  (2): Conv2d(20, 20, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), groups=20, bias=False)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act): NewGELUActivation()\n",
       "            (pwconv1): Conv1dAs2d_split(\n",
       "              (conv): Conv2d(20, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "          )\n",
       "          (2): ConvNeXtLikeBlock(\n",
       "            (dwconvs): ModuleList(\n",
       "              (0): Conv1dAs2d_split(\n",
       "                (conv): Sequential(\n",
       "                  (0): Conv2d(20, 20, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), groups=20, bias=False)\n",
       "                  (1): Conv2d(20, 20, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), groups=20, bias=False)\n",
       "                  (2): Conv2d(20, 20, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), groups=20, bias=False)\n",
       "                  (3): Conv2d(20, 20, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), groups=20, bias=False)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act): NewGELUActivation()\n",
       "            (pwconv1): Conv1dAs2d_split(\n",
       "              (conv): Conv2d(20, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "          )\n",
       "          (3): ConvNeXtLikeBlock(\n",
       "            (dwconvs): ModuleList(\n",
       "              (0): Conv1dAs2d_split(\n",
       "                (conv): Sequential(\n",
       "                  (0): Conv2d(20, 20, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), groups=20, bias=False)\n",
       "                  (1): Conv2d(20, 20, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), groups=20, bias=False)\n",
       "                  (2): Conv2d(20, 20, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), groups=20, bias=False)\n",
       "                  (3): Conv2d(20, 20, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), groups=20, bias=False)\n",
       "                  (4): Conv2d(20, 20, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), groups=20, bias=False)\n",
       "                  (5): Conv2d(20, 20, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), groups=20, bias=False)\n",
       "                  (6): Conv2d(20, 20, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), groups=20, bias=False)\n",
       "                  (7): Conv2d(20, 20, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), groups=20, bias=False)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act): NewGELUActivation()\n",
       "            (pwconv1): Conv1dAs2d_split(\n",
       "              (conv): Conv2d(20, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "          )\n",
       "          (4): TransformerEncoderLayer(\n",
       "            (attention): MultiHeadAttention(\n",
       "              (k_proj): Linear(in_features=20, out_features=20, bias=True)\n",
       "              (v_proj): Linear(in_features=20, out_features=20, bias=True)\n",
       "              (q_proj): Linear(in_features=20, out_features=20, bias=True)\n",
       "              (out_proj): Linear(in_features=20, out_features=20, bias=True)\n",
       "            )\n",
       "            (layer_norm): LayerNorm((20,), eps=1e-06, elementwise_affine=True)\n",
       "            (feed_forward): FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.0, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=20, out_features=20, bias=True)\n",
       "              (intermediate_act_fn): NewGELUActivation()\n",
       "              (output_dense): Linear(in_features=20, out_features=20, bias=True)\n",
       "              (output_dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((20,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (exp_dim_conv): Conv1d(20, 600, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "    )\n",
       "    (stage1): Sequential(\n",
       "      (0): weigth1d(w=(1, 2, 600, 1),sequential=False)\n",
       "      (1): to2d(f=60,c=10)\n",
       "      (2): Conv2d(10, 40, kernel_size=(2, 1), stride=(2, 1))\n",
       "      (3): ConvBlock2d(\n",
       "        (conv_block): ResBasicBlock(\n",
       "          (conv1): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=40, bias=False)\n",
       "          (conv1pw): Conv2d(40, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (bn1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=40, bias=False)\n",
       "          (conv2pw): Conv2d(40, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (bn2): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (se): Identity()\n",
       "          (downsample): Identity()\n",
       "        )\n",
       "      )\n",
       "      (4): ConvBlock2d(\n",
       "        (conv_block): ResBasicBlock(\n",
       "          (conv1): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=40, bias=False)\n",
       "          (conv1pw): Conv2d(40, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (bn1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=40, bias=False)\n",
       "          (conv2pw): Conv2d(40, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (bn2): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (se): Identity()\n",
       "          (downsample): Identity()\n",
       "        )\n",
       "      )\n",
       "      (5): ConvBlock2d(\n",
       "        (conv_block): ResBasicBlock(\n",
       "          (conv1): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=40, bias=False)\n",
       "          (conv1pw): Conv2d(40, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (bn1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=40, bias=False)\n",
       "          (conv2pw): Conv2d(40, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (bn2): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (se): Identity()\n",
       "          (downsample): Identity()\n",
       "        )\n",
       "      )\n",
       "      (6): Sequential(\n",
       "        (0): Conv2d(40, 20, kernel_size=(3, 3), stride=(1, 1), padding=same, groups=20)\n",
       "        (1): BatchNorm2d(20, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): NewGELUActivation()\n",
       "        (3): Conv2d(20, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (7): to1d()\n",
       "      (8): TimeContextBlock1d(\n",
       "        (red_dim_conv): Sequential(\n",
       "          (0): Conv1d(600, 20, kernel_size=(1,), stride=(1,))\n",
       "          (1): LayerNorm(C=(20,), data_format=channels_first, eps=1e-06)\n",
       "        )\n",
       "        (tcm): Sequential(\n",
       "          (0): ConvNeXtLikeBlock(\n",
       "            (dwconvs): ModuleList(\n",
       "              (0): Conv1dAs2d_split(\n",
       "                (conv): Conv2d(20, 20, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), groups=20)\n",
       "              )\n",
       "            )\n",
       "            (norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act): NewGELUActivation()\n",
       "            (pwconv1): Conv1dAs2d_split(\n",
       "              (conv): Conv2d(20, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "          )\n",
       "          (1): ConvNeXtLikeBlock(\n",
       "            (dwconvs): ModuleList(\n",
       "              (0): Conv1dAs2d_split(\n",
       "                (conv): Sequential(\n",
       "                  (0): Conv2d(20, 20, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), groups=20, bias=False)\n",
       "                  (1): Conv2d(20, 20, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), groups=20, bias=False)\n",
       "                  (2): Conv2d(20, 20, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), groups=20, bias=False)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act): NewGELUActivation()\n",
       "            (pwconv1): Conv1dAs2d_split(\n",
       "              (conv): Conv2d(20, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "          )\n",
       "          (2): ConvNeXtLikeBlock(\n",
       "            (dwconvs): ModuleList(\n",
       "              (0): Conv1dAs2d_split(\n",
       "                (conv): Sequential(\n",
       "                  (0): Conv2d(20, 20, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), groups=20, bias=False)\n",
       "                  (1): Conv2d(20, 20, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), groups=20, bias=False)\n",
       "                  (2): Conv2d(20, 20, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), groups=20, bias=False)\n",
       "                  (3): Conv2d(20, 20, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), groups=20, bias=False)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act): NewGELUActivation()\n",
       "            (pwconv1): Conv1dAs2d_split(\n",
       "              (conv): Conv2d(20, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "          )\n",
       "          (3): ConvNeXtLikeBlock(\n",
       "            (dwconvs): ModuleList(\n",
       "              (0): Conv1dAs2d_split(\n",
       "                (conv): Sequential(\n",
       "                  (0): Conv2d(20, 20, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), groups=20, bias=False)\n",
       "                  (1): Conv2d(20, 20, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), groups=20, bias=False)\n",
       "                  (2): Conv2d(20, 20, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), groups=20, bias=False)\n",
       "                  (3): Conv2d(20, 20, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), groups=20, bias=False)\n",
       "                  (4): Conv2d(20, 20, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), groups=20, bias=False)\n",
       "                  (5): Conv2d(20, 20, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), groups=20, bias=False)\n",
       "                  (6): Conv2d(20, 20, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), groups=20, bias=False)\n",
       "                  (7): Conv2d(20, 20, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), groups=20, bias=False)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act): NewGELUActivation()\n",
       "            (pwconv1): Conv1dAs2d_split(\n",
       "              (conv): Conv2d(20, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "          )\n",
       "          (4): TransformerEncoderLayer(\n",
       "            (attention): MultiHeadAttention(\n",
       "              (k_proj): Linear(in_features=20, out_features=20, bias=True)\n",
       "              (v_proj): Linear(in_features=20, out_features=20, bias=True)\n",
       "              (q_proj): Linear(in_features=20, out_features=20, bias=True)\n",
       "              (out_proj): Linear(in_features=20, out_features=20, bias=True)\n",
       "            )\n",
       "            (layer_norm): LayerNorm((20,), eps=1e-06, elementwise_affine=True)\n",
       "            (feed_forward): FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.0, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=20, out_features=20, bias=True)\n",
       "              (intermediate_act_fn): NewGELUActivation()\n",
       "              (output_dense): Linear(in_features=20, out_features=20, bias=True)\n",
       "              (output_dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((20,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (exp_dim_conv): Conv1d(20, 600, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "    )\n",
       "    (stage2): Sequential(\n",
       "      (0): weigth1d(w=(1, 3, 600, 1),sequential=False)\n",
       "      (1): to2d(f=30,c=20)\n",
       "      (2): Conv2d(20, 60, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (3): ConvBlock2d(\n",
       "        (conv_block): ResBasicBlock(\n",
       "          (conv1): Conv2d(60, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=60, bias=False)\n",
       "          (conv1pw): Conv2d(60, 60, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (bn1): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(60, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=60, bias=False)\n",
       "          (conv2pw): Conv2d(60, 60, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (bn2): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (se): Identity()\n",
       "          (downsample): Identity()\n",
       "        )\n",
       "      )\n",
       "      (4): ConvBlock2d(\n",
       "        (conv_block): ResBasicBlock(\n",
       "          (conv1): Conv2d(60, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=60, bias=False)\n",
       "          (conv1pw): Conv2d(60, 60, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (bn1): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(60, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=60, bias=False)\n",
       "          (conv2pw): Conv2d(60, 60, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (bn2): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (se): Identity()\n",
       "          (downsample): Identity()\n",
       "        )\n",
       "      )\n",
       "      (5): ConvBlock2d(\n",
       "        (conv_block): ResBasicBlock(\n",
       "          (conv1): Conv2d(60, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=60, bias=False)\n",
       "          (conv1pw): Conv2d(60, 60, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (bn1): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(60, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=60, bias=False)\n",
       "          (conv2pw): Conv2d(60, 60, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (bn2): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (se): Identity()\n",
       "          (downsample): Identity()\n",
       "        )\n",
       "      )\n",
       "      (6): Sequential(\n",
       "        (0): Conv2d(60, 20, kernel_size=(3, 3), stride=(1, 1), padding=same, groups=20)\n",
       "        (1): BatchNorm2d(20, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): NewGELUActivation()\n",
       "        (3): Conv2d(20, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (7): to1d()\n",
       "      (8): TimeContextBlock1d(\n",
       "        (red_dim_conv): Sequential(\n",
       "          (0): Conv1d(600, 20, kernel_size=(1,), stride=(1,))\n",
       "          (1): LayerNorm(C=(20,), data_format=channels_first, eps=1e-06)\n",
       "        )\n",
       "        (tcm): Sequential(\n",
       "          (0): ConvNeXtLikeBlock(\n",
       "            (dwconvs): ModuleList(\n",
       "              (0): Conv1dAs2d_split(\n",
       "                (conv): Conv2d(20, 20, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), groups=20)\n",
       "              )\n",
       "            )\n",
       "            (norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act): NewGELUActivation()\n",
       "            (pwconv1): Conv1dAs2d_split(\n",
       "              (conv): Conv2d(20, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "          )\n",
       "          (1): ConvNeXtLikeBlock(\n",
       "            (dwconvs): ModuleList(\n",
       "              (0): Conv1dAs2d_split(\n",
       "                (conv): Sequential(\n",
       "                  (0): Conv2d(20, 20, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), groups=20, bias=False)\n",
       "                  (1): Conv2d(20, 20, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), groups=20, bias=False)\n",
       "                  (2): Conv2d(20, 20, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), groups=20, bias=False)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act): NewGELUActivation()\n",
       "            (pwconv1): Conv1dAs2d_split(\n",
       "              (conv): Conv2d(20, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "          )\n",
       "          (2): ConvNeXtLikeBlock(\n",
       "            (dwconvs): ModuleList(\n",
       "              (0): Conv1dAs2d_split(\n",
       "                (conv): Sequential(\n",
       "                  (0): Conv2d(20, 20, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), groups=20, bias=False)\n",
       "                  (1): Conv2d(20, 20, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), groups=20, bias=False)\n",
       "                  (2): Conv2d(20, 20, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), groups=20, bias=False)\n",
       "                  (3): Conv2d(20, 20, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), groups=20, bias=False)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act): NewGELUActivation()\n",
       "            (pwconv1): Conv1dAs2d_split(\n",
       "              (conv): Conv2d(20, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "          )\n",
       "          (3): ConvNeXtLikeBlock(\n",
       "            (dwconvs): ModuleList(\n",
       "              (0): Conv1dAs2d_split(\n",
       "                (conv): Sequential(\n",
       "                  (0): Conv2d(20, 20, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), groups=20, bias=False)\n",
       "                  (1): Conv2d(20, 20, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), groups=20, bias=False)\n",
       "                  (2): Conv2d(20, 20, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), groups=20, bias=False)\n",
       "                  (3): Conv2d(20, 20, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), groups=20, bias=False)\n",
       "                  (4): Conv2d(20, 20, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), groups=20, bias=False)\n",
       "                  (5): Conv2d(20, 20, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), groups=20, bias=False)\n",
       "                  (6): Conv2d(20, 20, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), groups=20, bias=False)\n",
       "                  (7): Conv2d(20, 20, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), groups=20, bias=False)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act): NewGELUActivation()\n",
       "            (pwconv1): Conv1dAs2d_split(\n",
       "              (conv): Conv2d(20, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "          )\n",
       "          (4): TransformerEncoderLayer(\n",
       "            (attention): MultiHeadAttention(\n",
       "              (k_proj): Linear(in_features=20, out_features=20, bias=True)\n",
       "              (v_proj): Linear(in_features=20, out_features=20, bias=True)\n",
       "              (q_proj): Linear(in_features=20, out_features=20, bias=True)\n",
       "              (out_proj): Linear(in_features=20, out_features=20, bias=True)\n",
       "            )\n",
       "            (layer_norm): LayerNorm((20,), eps=1e-06, elementwise_affine=True)\n",
       "            (feed_forward): FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.0, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=20, out_features=20, bias=True)\n",
       "              (intermediate_act_fn): NewGELUActivation()\n",
       "              (output_dense): Linear(in_features=20, out_features=20, bias=True)\n",
       "              (output_dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((20,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (exp_dim_conv): Conv1d(20, 600, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "    )\n",
       "    (stage3): Sequential(\n",
       "      (0): weigth1d(w=(1, 4, 600, 1),sequential=False)\n",
       "      (1): to2d(f=30,c=20)\n",
       "      (2): Conv2d(20, 80, kernel_size=(2, 1), stride=(2, 1))\n",
       "      (3): ConvBlock2d(\n",
       "        (conv_block): ResBasicBlock(\n",
       "          (conv1): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=80, bias=False)\n",
       "          (conv1pw): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (bn1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=80, bias=False)\n",
       "          (conv2pw): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (bn2): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (se): Identity()\n",
       "          (downsample): Identity()\n",
       "        )\n",
       "      )\n",
       "      (4): ConvBlock2d(\n",
       "        (conv_block): ResBasicBlock(\n",
       "          (conv1): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=80, bias=False)\n",
       "          (conv1pw): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (bn1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=80, bias=False)\n",
       "          (conv2pw): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (bn2): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (se): Identity()\n",
       "          (downsample): Identity()\n",
       "        )\n",
       "      )\n",
       "      (5): ConvBlock2d(\n",
       "        (conv_block): ResBasicBlock(\n",
       "          (conv1): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=80, bias=False)\n",
       "          (conv1pw): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (bn1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=80, bias=False)\n",
       "          (conv2pw): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (bn2): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (se): Identity()\n",
       "          (downsample): Identity()\n",
       "        )\n",
       "      )\n",
       "      (6): ConvBlock2d(\n",
       "        (conv_block): ResBasicBlock(\n",
       "          (conv1): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=80, bias=False)\n",
       "          (conv1pw): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (bn1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=80, bias=False)\n",
       "          (conv2pw): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (bn2): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (se): Identity()\n",
       "          (downsample): Identity()\n",
       "        )\n",
       "      )\n",
       "      (7): Sequential(\n",
       "        (0): Conv2d(80, 40, kernel_size=(3, 3), stride=(1, 1), padding=same, groups=40)\n",
       "        (1): BatchNorm2d(40, eps=1e-06, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): NewGELUActivation()\n",
       "        (3): Conv2d(40, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (8): to1d()\n",
       "      (9): TimeContextBlock1d(\n",
       "        (red_dim_conv): Sequential(\n",
       "          (0): Conv1d(600, 60, kernel_size=(1,), stride=(1,))\n",
       "          (1): LayerNorm(C=(60,), data_format=channels_first, eps=1e-06)\n",
       "        )\n",
       "        (tcm): Sequential(\n",
       "          (0): ConvNeXtLikeBlock(\n",
       "            (dwconvs): ModuleList(\n",
       "              (0): Conv1dAs2d_split(\n",
       "                (conv): Conv2d(60, 60, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), groups=60)\n",
       "              )\n",
       "            )\n",
       "            (norm): BatchNorm1d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act): NewGELUActivation()\n",
       "            (pwconv1): Conv1dAs2d_split(\n",
       "              (conv): Conv2d(60, 60, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "          )\n",
       "          (1): ConvNeXtLikeBlock(\n",
       "            (dwconvs): ModuleList(\n",
       "              (0): Conv1dAs2d_split(\n",
       "                (conv): Sequential(\n",
       "                  (0): Conv2d(60, 60, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), groups=60, bias=False)\n",
       "                  (1): Conv2d(60, 60, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), groups=60, bias=False)\n",
       "                  (2): Conv2d(60, 60, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), groups=60, bias=False)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (norm): BatchNorm1d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act): NewGELUActivation()\n",
       "            (pwconv1): Conv1dAs2d_split(\n",
       "              (conv): Conv2d(60, 60, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "          )\n",
       "          (2): ConvNeXtLikeBlock(\n",
       "            (dwconvs): ModuleList(\n",
       "              (0): Conv1dAs2d_split(\n",
       "                (conv): Sequential(\n",
       "                  (0): Conv2d(60, 60, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), groups=60, bias=False)\n",
       "                  (1): Conv2d(60, 60, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), groups=60, bias=False)\n",
       "                  (2): Conv2d(60, 60, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), groups=60, bias=False)\n",
       "                  (3): Conv2d(60, 60, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), groups=60, bias=False)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (norm): BatchNorm1d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act): NewGELUActivation()\n",
       "            (pwconv1): Conv1dAs2d_split(\n",
       "              (conv): Conv2d(60, 60, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "          )\n",
       "          (3): ConvNeXtLikeBlock(\n",
       "            (dwconvs): ModuleList(\n",
       "              (0): Conv1dAs2d_split(\n",
       "                (conv): Sequential(\n",
       "                  (0): Conv2d(60, 60, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), groups=60, bias=False)\n",
       "                  (1): Conv2d(60, 60, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), groups=60, bias=False)\n",
       "                  (2): Conv2d(60, 60, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), groups=60, bias=False)\n",
       "                  (3): Conv2d(60, 60, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), groups=60, bias=False)\n",
       "                  (4): Conv2d(60, 60, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), groups=60, bias=False)\n",
       "                  (5): Conv2d(60, 60, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), groups=60, bias=False)\n",
       "                  (6): Conv2d(60, 60, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), groups=60, bias=False)\n",
       "                  (7): Conv2d(60, 60, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), groups=60, bias=False)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (norm): BatchNorm1d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act): NewGELUActivation()\n",
       "            (pwconv1): Conv1dAs2d_split(\n",
       "              (conv): Conv2d(60, 60, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "          )\n",
       "          (4): TransformerEncoderLayer(\n",
       "            (attention): MultiHeadAttention(\n",
       "              (k_proj): Linear(in_features=60, out_features=60, bias=True)\n",
       "              (v_proj): Linear(in_features=60, out_features=60, bias=True)\n",
       "              (q_proj): Linear(in_features=60, out_features=60, bias=True)\n",
       "              (out_proj): Linear(in_features=60, out_features=60, bias=True)\n",
       "            )\n",
       "            (layer_norm): LayerNorm((60,), eps=1e-06, elementwise_affine=True)\n",
       "            (feed_forward): FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.0, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=60, out_features=60, bias=True)\n",
       "              (intermediate_act_fn): NewGELUActivation()\n",
       "              (output_dense): Linear(in_features=60, out_features=60, bias=True)\n",
       "              (output_dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((60,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (exp_dim_conv): Conv1d(60, 600, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "    )\n",
       "    (stage4): Sequential(\n",
       "      (0): weigth1d(w=(1, 5, 600, 1),sequential=False)\n",
       "      (1): to2d(f=15,c=40)\n",
       "      (2): Conv2d(40, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (3): ConvBlock2d(\n",
       "        (conv_block): ResBasicBlock(\n",
       "          (conv1): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=40, bias=False)\n",
       "          (conv1pw): Conv2d(40, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (bn1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=40, bias=False)\n",
       "          (conv2pw): Conv2d(40, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (bn2): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (se): Identity()\n",
       "          (downsample): Identity()\n",
       "        )\n",
       "      )\n",
       "      (4): ConvBlock2d(\n",
       "        (conv_block): ResBasicBlock(\n",
       "          (conv1): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=40, bias=False)\n",
       "          (conv1pw): Conv2d(40, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (bn1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=40, bias=False)\n",
       "          (conv2pw): Conv2d(40, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (bn2): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (se): Identity()\n",
       "          (downsample): Identity()\n",
       "        )\n",
       "      )\n",
       "      (5): ConvBlock2d(\n",
       "        (conv_block): ResBasicBlock(\n",
       "          (conv1): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=40, bias=False)\n",
       "          (conv1pw): Conv2d(40, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (bn1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=40, bias=False)\n",
       "          (conv2pw): Conv2d(40, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (bn2): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (se): Identity()\n",
       "          (downsample): Identity()\n",
       "        )\n",
       "      )\n",
       "      (6): to1d()\n",
       "      (7): TimeContextBlock1d(\n",
       "        (red_dim_conv): Sequential(\n",
       "          (0): Conv1d(600, 60, kernel_size=(1,), stride=(1,))\n",
       "          (1): LayerNorm(C=(60,), data_format=channels_first, eps=1e-06)\n",
       "        )\n",
       "        (tcm): Sequential(\n",
       "          (0): ConvNeXtLikeBlock(\n",
       "            (dwconvs): ModuleList(\n",
       "              (0): Conv1dAs2d_split(\n",
       "                (conv): Conv2d(60, 60, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), groups=60)\n",
       "              )\n",
       "            )\n",
       "            (norm): BatchNorm1d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act): NewGELUActivation()\n",
       "            (pwconv1): Conv1dAs2d_split(\n",
       "              (conv): Conv2d(60, 60, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "          )\n",
       "          (1): ConvNeXtLikeBlock(\n",
       "            (dwconvs): ModuleList(\n",
       "              (0): Conv1dAs2d_split(\n",
       "                (conv): Sequential(\n",
       "                  (0): Conv2d(60, 60, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), groups=60, bias=False)\n",
       "                  (1): Conv2d(60, 60, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), groups=60, bias=False)\n",
       "                  (2): Conv2d(60, 60, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), groups=60, bias=False)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (norm): BatchNorm1d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act): NewGELUActivation()\n",
       "            (pwconv1): Conv1dAs2d_split(\n",
       "              (conv): Conv2d(60, 60, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "          )\n",
       "          (2): ConvNeXtLikeBlock(\n",
       "            (dwconvs): ModuleList(\n",
       "              (0): Conv1dAs2d_split(\n",
       "                (conv): Sequential(\n",
       "                  (0): Conv2d(60, 60, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), groups=60, bias=False)\n",
       "                  (1): Conv2d(60, 60, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), groups=60, bias=False)\n",
       "                  (2): Conv2d(60, 60, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), groups=60, bias=False)\n",
       "                  (3): Conv2d(60, 60, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), groups=60, bias=False)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (norm): BatchNorm1d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act): NewGELUActivation()\n",
       "            (pwconv1): Conv1dAs2d_split(\n",
       "              (conv): Conv2d(60, 60, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "          )\n",
       "          (3): ConvNeXtLikeBlock(\n",
       "            (dwconvs): ModuleList(\n",
       "              (0): Conv1dAs2d_split(\n",
       "                (conv): Sequential(\n",
       "                  (0): Conv2d(60, 60, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), groups=60, bias=False)\n",
       "                  (1): Conv2d(60, 60, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), groups=60, bias=False)\n",
       "                  (2): Conv2d(60, 60, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), groups=60, bias=False)\n",
       "                  (3): Conv2d(60, 60, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), groups=60, bias=False)\n",
       "                  (4): Conv2d(60, 60, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), groups=60, bias=False)\n",
       "                  (5): Conv2d(60, 60, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), groups=60, bias=False)\n",
       "                  (6): Conv2d(60, 60, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), groups=60, bias=False)\n",
       "                  (7): Conv2d(60, 60, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), groups=60, bias=False)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (norm): BatchNorm1d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act): NewGELUActivation()\n",
       "            (pwconv1): Conv1dAs2d_split(\n",
       "              (conv): Conv2d(60, 60, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "          )\n",
       "          (4): TransformerEncoderLayer(\n",
       "            (attention): MultiHeadAttention(\n",
       "              (k_proj): Linear(in_features=60, out_features=60, bias=True)\n",
       "              (v_proj): Linear(in_features=60, out_features=60, bias=True)\n",
       "              (q_proj): Linear(in_features=60, out_features=60, bias=True)\n",
       "              (out_proj): Linear(in_features=60, out_features=60, bias=True)\n",
       "            )\n",
       "            (layer_norm): LayerNorm((60,), eps=1e-06, elementwise_affine=True)\n",
       "            (feed_forward): FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.0, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=60, out_features=60, bias=True)\n",
       "              (intermediate_act_fn): NewGELUActivation()\n",
       "              (output_dense): Linear(in_features=60, out_features=60, bias=True)\n",
       "              (output_dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((60,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (exp_dim_conv): Conv1d(60, 600, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "    )\n",
       "    (fin_wght1d): weigth1d(w=(1, 6, 600, 1),sequential=False)\n",
       "    (mfa): Identity()\n",
       "    (fin_to2d): Identity()\n",
       "  )\n",
       "  (pool): ASTP_RKNNSafe_cpu(\n",
       "    (linear1): Conv1d(600, 128, kernel_size=(1,), stride=(1,))\n",
       "    (linear2): Conv1d(128, 600, kernel_size=(1,), stride=(1,))\n",
       "  )\n",
       "  (bn): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (linear): Linear(in_features=1200, out_features=192, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_no_mel.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==============================================================================================================\n",
       "Layer (type:depth-idx)                                       Output Shape              Param #\n",
       "==============================================================================================================\n",
       "ReDimNetNoMel                                                [1, 192]                  --\n",
       "├─ReDimNet: 1-1                                              [1, 600, 134]             --\n",
       "│    └─Sequential: 2-1                                       [1, 600, 134]             --\n",
       "│    │    └─Conv2d: 3-1                                      [1, 10, 60, 134]          100\n",
       "│    │    └─LayerNorm: 3-2                                   [1, 10, 60, 134]          20\n",
       "│    │    └─to1d: 3-3                                        [1, 600, 134]             --\n",
       "│    └─Sequential: 2-2                                       [1, 600, 134]             --\n",
       "│    │    └─weigth1d: 3-4                                    [1, 600, 134]             (1)\n",
       "│    │    └─to2d: 3-5                                        [1, 10, 60, 134]          --\n",
       "│    │    └─Conv2d: 3-6                                      [1, 10, 60, 134]          110\n",
       "│    │    └─ConvBlock2d: 3-7                                 [1, 10, 60, 134]          440\n",
       "│    │    └─ConvBlock2d: 3-8                                 [1, 10, 60, 134]          440\n",
       "│    │    └─to1d: 3-9                                        [1, 600, 134]             --\n",
       "│    │    └─TimeContextBlock1d: 3-10                         [1, 600, 134]             31,680\n",
       "│    └─Sequential: 2-3                                       [1, 600, 134]             --\n",
       "│    │    └─weigth1d: 3-11                                   [1, 600, 134]             1,200\n",
       "│    │    └─to2d: 3-12                                       [1, 10, 60, 134]          --\n",
       "│    │    └─Conv2d: 3-13                                     [1, 40, 30, 134]          840\n",
       "│    │    └─ConvBlock2d: 3-14                                [1, 40, 30, 134]          4,160\n",
       "│    │    └─ConvBlock2d: 3-15                                [1, 40, 30, 134]          4,160\n",
       "│    │    └─ConvBlock2d: 3-16                                [1, 40, 30, 134]          4,160\n",
       "│    │    └─Sequential: 3-17                                 [1, 20, 30, 134]          840\n",
       "│    │    └─to1d: 3-18                                       [1, 600, 134]             --\n",
       "│    │    └─TimeContextBlock1d: 3-19                         [1, 600, 134]             31,680\n",
       "│    └─Sequential: 2-4                                       [1, 600, 134]             --\n",
       "│    │    └─weigth1d: 3-20                                   [1, 600, 134]             1,800\n",
       "│    │    └─to2d: 3-21                                       [1, 20, 30, 134]          --\n",
       "│    │    └─Conv2d: 3-22                                     [1, 60, 30, 134]          1,260\n",
       "│    │    └─ConvBlock2d: 3-23                                [1, 60, 30, 134]          8,640\n",
       "│    │    └─ConvBlock2d: 3-24                                [1, 60, 30, 134]          8,640\n",
       "│    │    └─ConvBlock2d: 3-25                                [1, 60, 30, 134]          8,640\n",
       "│    │    └─Sequential: 3-26                                 [1, 20, 30, 134]          1,020\n",
       "│    │    └─to1d: 3-27                                       [1, 600, 134]             --\n",
       "│    │    └─TimeContextBlock1d: 3-28                         [1, 600, 134]             31,680\n",
       "│    └─Sequential: 2-5                                       [1, 600, 134]             --\n",
       "│    │    └─weigth1d: 3-29                                   [1, 600, 134]             2,400\n",
       "│    │    └─to2d: 3-30                                       [1, 20, 30, 134]          --\n",
       "│    │    └─Conv2d: 3-31                                     [1, 80, 15, 134]          3,280\n",
       "│    │    └─ConvBlock2d: 3-32                                [1, 80, 15, 134]          14,720\n",
       "│    │    └─ConvBlock2d: 3-33                                [1, 80, 15, 134]          14,720\n",
       "│    │    └─ConvBlock2d: 3-34                                [1, 80, 15, 134]          14,720\n",
       "│    │    └─ConvBlock2d: 3-35                                [1, 80, 15, 134]          14,720\n",
       "│    │    └─Sequential: 3-36                                 [1, 40, 15, 134]          2,480\n",
       "│    │    └─to1d: 3-37                                       [1, 600, 134]             --\n",
       "│    │    └─TimeContextBlock1d: 3-38                         [1, 600, 134]             117,840\n",
       "│    └─Sequential: 2-6                                       [1, 600, 134]             --\n",
       "│    │    └─weigth1d: 3-39                                   [1, 600, 134]             3,000\n",
       "│    │    └─to2d: 3-40                                       [1, 40, 15, 134]          --\n",
       "│    │    └─Conv2d: 3-41                                     [1, 40, 15, 134]          1,640\n",
       "│    │    └─ConvBlock2d: 3-42                                [1, 40, 15, 134]          4,160\n",
       "│    │    └─ConvBlock2d: 3-43                                [1, 40, 15, 134]          4,160\n",
       "│    │    └─ConvBlock2d: 3-44                                [1, 40, 15, 134]          4,160\n",
       "│    │    └─to1d: 3-45                                       [1, 600, 134]             --\n",
       "│    │    └─TimeContextBlock1d: 3-46                         [1, 600, 134]             117,840\n",
       "│    └─weigth1d: 2-7                                         [1, 600, 134]             3,600\n",
       "│    └─Identity: 2-8                                         [1, 600, 134]             --\n",
       "│    └─Identity: 2-9                                         [1, 600, 134]             --\n",
       "├─ASTP_RKNNSafe_cpu: 1-2                                     [1, 1200]                 --\n",
       "│    └─Conv1d: 2-10                                          [1, 128, 134]             76,928\n",
       "│    └─Conv1d: 2-11                                          [1, 600, 134]             77,400\n",
       "├─BatchNorm1d: 1-3                                           [1, 1200]                 2,400\n",
       "├─Linear: 1-4                                                [1, 192]                  230,592\n",
       "==============================================================================================================\n",
       "Total params: 852,271\n",
       "Trainable params: 852,270\n",
       "Non-trainable params: 1\n",
       "Total mult-adds (M): 385.92\n",
       "==============================================================================================================\n",
       "Input size (MB): 0.03\n",
       "Forward/backward pass size (MB): 135.35\n",
       "Params size (MB): 3.41\n",
       "Estimated Total Size (MB): 138.79\n",
       "=============================================================================================================="
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model_no_mel, (1, 1, 60, 134))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TORCH SIDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_inference(wav_path: str):\n",
    "    # (a) Load audio\n",
    "    waveform, sample_rate = torchaudio.load(wav_path)  # shape: [channels, time]\n",
    "    # If stereo, select one channel, or average:\n",
    "    if waveform.shape[0] > 1:\n",
    "        waveform = waveform.mean(dim=0, keepdim=True)\n",
    "\n",
    "    # Resample if needed\n",
    "    target_sample_rate=16000\n",
    "    if sample_rate != target_sample_rate:\n",
    "        resampler = T.Resample(orig_freq=sample_rate, new_freq=target_sample_rate)\n",
    "        waveform = resampler(waveform)\n",
    "\n",
    "    # (b) Convert to log-mel\n",
    "    log_mel = myUtils.waveform_to_logmel(waveform)\n",
    "    print('feeding logmel shape:', log_mel.shape)\n",
    "    \n",
    "    # (c) Forward pass\n",
    "    with torch.no_grad():\n",
    "        embedding = model_no_mel(log_mel)  # shape typically [1, 192] or so\n",
    "\n",
    "    print(\"Embedding shape:\", embedding.shape)\n",
    "    #print(\"Embedding:\", embedding)\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* run test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input waveform shape: torch.Size([1, 32000])\n",
      "feeding logmel shape: torch.Size([1, 1, 60, 134])\n",
      "Embedding shape: torch.Size([1, 192])\n",
      "Input waveform shape: torch.Size([1, 25776])\n",
      "Padding log_mel from 108 to 134 frames\n",
      "feeding logmel shape: torch.Size([1, 1, 60, 134])\n",
      "Embedding shape: torch.Size([1, 192])\n",
      "Input waveform shape: torch.Size([1, 23570])\n",
      "Padding log_mel from 99 to 134 frames\n",
      "feeding logmel shape: torch.Size([1, 1, 60, 134])\n",
      "Embedding shape: torch.Size([1, 192])\n",
      "Input waveform shape: torch.Size([1, 32000])\n",
      "feeding logmel shape: torch.Size([1, 1, 60, 134])\n",
      "Embedding shape: torch.Size([1, 192])\n",
      "Input waveform shape: torch.Size([1, 32000])\n",
      "feeding logmel shape: torch.Size([1, 1, 60, 134])\n",
      "Embedding shape: torch.Size([1, 192])\n",
      "Input waveform shape: torch.Size([1, 28126])\n",
      "Padding log_mel from 118 to 134 frames\n",
      "feeding logmel shape: torch.Size([1, 1, 60, 134])\n",
      "Embedding shape: torch.Size([1, 192])\n",
      "Input waveform shape: torch.Size([1, 32000])\n",
      "feeding logmel shape: torch.Size([1, 1, 60, 134])\n",
      "Embedding shape: torch.Size([1, 192])\n",
      "**************************************************************************\n",
      "*************************   compare summary ******************************\n",
      "**************************************************************************\n",
      "====>>>> should be similar:\n",
      "Similarity (robot1 to robot2 ): 0.8906651735305786\n",
      "Similarity (human1 to human1 ): 0.8243448734283447\n",
      "Similarity (human2 to human2 ): 0.6441124677658081\n",
      "====>>>> should be differnet:\n",
      "Similarity (robot to human1  ): 0.4088161587715149\n",
      "Similarity (robot to human2  ): 0.4169830083847046\n",
      "Similarity (human1 to human2 ): 0.5724265575408936\n"
     ]
    }
   ],
   "source": [
    "torch_embedding = test_all_voices(\n",
    "    extract_speaker_embedding_function = torch_inference,\n",
    "    cosine_similarity_function = myUtils.cosine_similarity\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## compare to baseline\n",
    "\n",
    "* test embedding compare of voice in the currnet model with baseline model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity embde0: 0.3593041002750397\n",
      "Similarity embde1: 0.6002203226089478\n",
      "Similarity embde2: 0.550667941570282\n",
      "Similarity embde3: 0.3370029926300049\n",
      "Similarity embde4: 0.28784728050231934\n",
      "Similarity embde5: 0.4517865777015686\n",
      "Similarity embde6: 0.35602834820747375\n"
     ]
    }
   ],
   "source": [
    "print(f\"Similarity embde0: {myUtils.cosine_similarity(base_line_embedding['embed0'], torch_embedding['embed0'])}\")\n",
    "print(f\"Similarity embde1: {myUtils.cosine_similarity(base_line_embedding['embed1'], torch_embedding['embed1'])}\")\n",
    "print(f\"Similarity embde2: {myUtils.cosine_similarity(base_line_embedding['embed2'], torch_embedding['embed2'])}\")\n",
    "print(f\"Similarity embde3: {myUtils.cosine_similarity(base_line_embedding['embed3'], torch_embedding['embed3'])}\")\n",
    "print(f\"Similarity embde4: {myUtils.cosine_similarity(base_line_embedding['embed4'], torch_embedding['embed4'])}\")\n",
    "print(f\"Similarity embde5: {myUtils.cosine_similarity(base_line_embedding['embed5'], torch_embedding['embed5'])}\")\n",
    "print(f\"Similarity embde6: {myUtils.cosine_similarity(base_line_embedding['embed6'], torch_embedding['embed6'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ONNX SIDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3744139/991109579.py:8: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  (torch.sqrt(torch.tensor(2.0 / torch.pi, device=x.device)) *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported NHWC model to ReDimNet_no_mel_nhwc.onnx\n",
      "Exported to ReDimNet_no_mel.onnx\n",
      "-rw-rw-r-- 1 vlad vlad 3.6M Jun 25 14:38 ReDimNet_no_mel.onnx\n"
     ]
    }
   ],
   "source": [
    "myUtils.export_to_onnx(model_no_mel,onnx_path = \"ReDimNet_no_mel.onnx\")\n",
    "!ls -lah ReDimNet_no_mel.onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### store half"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/proj/voice/pyvoice_venv/lib/python3.10/site-packages/onnxconverter_common/float16.py:43: UserWarning: the float32 number 5.605193857299268e-45 will be truncated to 1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(pos_min, min_positive_val))\n",
      "/data/proj/voice/pyvoice_venv/lib/python3.10/site-packages/onnxconverter_common/float16.py:43: UserWarning: the float32 number 1.2503155177867598e-09 will be truncated to 1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(pos_min, min_positive_val))\n",
      "/data/proj/voice/pyvoice_venv/lib/python3.10/site-packages/onnxconverter_common/float16.py:43: UserWarning: the float32 number 5.4816506287478153e-14 will be truncated to 1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(pos_min, min_positive_val))\n",
      "/data/proj/voice/pyvoice_venv/lib/python3.10/site-packages/onnxconverter_common/float16.py:43: UserWarning: the float32 number 2.2374582096875482e-17 will be truncated to 1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(pos_min, min_positive_val))\n",
      "/data/proj/voice/pyvoice_venv/lib/python3.10/site-packages/onnxconverter_common/float16.py:43: UserWarning: the float32 number 4.89484008880936e-09 will be truncated to 1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(pos_min, min_positive_val))\n",
      "/data/proj/voice/pyvoice_venv/lib/python3.10/site-packages/onnxconverter_common/float16.py:53: UserWarning: the float32 number -1.0086018242816408e-08 will be truncated to -1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(neg_max, -min_positive_val))\n",
      "/data/proj/voice/pyvoice_venv/lib/python3.10/site-packages/onnxconverter_common/float16.py:43: UserWarning: the float32 number 9.999999960041972e-13 will be truncated to 1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(pos_min, min_positive_val))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted ReDimNet_no_mel.onnx to half precision and saved as ReDimNet_no_mel_fp16.onnx\n",
      "Converted ReDimNet_no_mel_nhwc.onnx to half precision and saved as ReDimNet_no_mel_nhwc_fp16.onnx\n"
     ]
    }
   ],
   "source": [
    "myUtils.restore_in_half_precision('ReDimNet_no_mel.onnx','ReDimNet_no_mel_fp16.onnx')\n",
    "myUtils.restore_in_half_precision('ReDimNet_no_mel_nhwc.onnx','ReDimNet_no_mel_nhwc_fp16.onnx')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## verify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# onnx_path = \"ReDimNet_no_mel.onnx\"\n",
    "onnx_path = \"ReDimNet_no_mel_fp16.onnx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX model is valid!\n"
     ]
    }
   ],
   "source": [
    "onnx_model = onnx.load(onnx_path)\n",
    "onnx.checker.check_model(onnx_model)\n",
    "print(\"ONNX model is valid!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Conv nodes with auto_pad = 'SAME_UPPER' or 'SAME_LOWER' ===\n",
      "── /backbone/stem/stem.0/Conv\n",
      "   auto_pad : SAME_UPPER\n",
      "   pads     : auto\n",
      "   kernel   : k=3  (C_in=1, C_out=10)\n",
      "   input    : graph_input_cast_0\n",
      "   output   : /backbone/stem/stem.0/Conv_output_0\n",
      "── /backbone/stage1/stage1.6/stage1.6.0/Conv\n",
      "   auto_pad : SAME_UPPER\n",
      "   pads     : auto\n",
      "   kernel   : k=3  (C_in=2, C_out=20)\n",
      "   input    : /backbone/stage1/stage1.5/conv_block/relu_1/Relu_output_0\n",
      "   output   : /backbone/stage1/stage1.6/stage1.6.0/Conv_output_0\n",
      "── /backbone/stage2/stage2.6/stage2.6.0/Conv\n",
      "   auto_pad : SAME_UPPER\n",
      "   pads     : auto\n",
      "   kernel   : k=3  (C_in=3, C_out=20)\n",
      "   input    : /backbone/stage2/stage2.5/conv_block/relu_1/Relu_output_0\n",
      "   output   : /backbone/stage2/stage2.6/stage2.6.0/Conv_output_0\n",
      "── /backbone/stage3/stage3.7/stage3.7.0/Conv\n",
      "   auto_pad : SAME_UPPER\n",
      "   pads     : auto\n",
      "   kernel   : k=3  (C_in=2, C_out=40)\n",
      "   input    : /backbone/stage3/stage3.6/conv_block/relu_1/Relu_output_0\n",
      "   output   : /backbone/stage3/stage3.7/stage3.7.0/Conv_output_0\n"
     ]
    }
   ],
   "source": [
    "import torch, onnx, textwrap, json\n",
    "from pathlib import Path\n",
    "\n",
    "m= onnx_model\n",
    "\n",
    "print(\"\\n=== Conv nodes with auto_pad = 'SAME_UPPER' or 'SAME_LOWER' ===\")\n",
    "for n in m.graph.node:\n",
    "    if n.op_type != 'Conv':\n",
    "        continue\n",
    "\n",
    "    auto_attr = next((a for a in n.attribute if a.name == 'auto_pad'), None)\n",
    "    if auto_attr and auto_attr.s.decode() != 'NOTSET':\n",
    "        # gather some extra context -------------------------------------------\n",
    "        # kernel size & in/out channels\n",
    "        w_init = next(i for i in m.graph.initializer if i.name == n.input[1])\n",
    "        C_out, C_in_div_g, kH, kW = w_init.dims\n",
    "        pads_attr = next((a for a in n.attribute if a.name == 'pads'), None)\n",
    "        pads = json.loads(str(list(pads_attr.ints))) if pads_attr else 'auto'\n",
    "\n",
    "        print(textwrap.dedent(f\"\"\"\\\n",
    "            ── {n.name}\n",
    "               auto_pad : {auto_attr.s.decode()}\n",
    "               pads     : {pads}\n",
    "               kernel   : k={kH}  (C_in={C_in_div_g}, C_out={C_out})\n",
    "               input    : {n.input[0]}\n",
    "               output   : {n.output[0]}\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "───────── Gather nodes in model_cf_false_op11.onnx ─────────\n",
      "\n",
      "───────── Gather nodes in model_cf_false_op11.onnx ─────────\n"
     ]
    }
   ],
   "source": [
    "from onnx import AttributeProto, numpy_helper\n",
    "\n",
    "def walk_graph(g, scope=\"\"):\n",
    "    for n in g.node:\n",
    "        if n.op_type == \"Gather\":\n",
    "            axis = next((a.i for a in n.attribute if a.name==\"axis\"), \"?\")\n",
    "            print(f\"\\n🔹 {scope}{n.name or '(unnamed)'}   axis={axis}\")\n",
    "            print(f\"   inputs : {n.input}\")\n",
    "            print(f\"   outputs: {n.output}\")\n",
    "        # dive into sub-graphs (Loop/If/etc.)\n",
    "        for a in n.attribute:\n",
    "            if a.type == AttributeProto.GRAPH:\n",
    "                walk_graph(a.g, scope + n.name + \"/\")\n",
    "\n",
    "\n",
    "m= onnx_model\n",
    "\n",
    "print(\"\\n───────── Gather nodes in model_cf_false_op11.onnx ─────────\")\n",
    "walk_graph(m.graph)\n",
    "print(\"\\n───────── Gather nodes in model_cf_false_op11.onnx ─────────\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pow /backbone/stem/stem.1/Pow\n",
      "Sqrt /backbone/stem/stem.1/Sqrt\n",
      "Transpose /backbone/stem/stem.2/Transpose\n",
      "Transpose /backbone/stage0/stage0.1/Transpose\n",
      "Transpose /backbone/stage0/stage0.5/Transpose\n",
      "Pow /backbone/stage0/stage0.6/red_dim_conv/red_dim_conv.1/Pow\n",
      "Sqrt /backbone/stage0/stage0.6/red_dim_conv/red_dim_conv.1/Sqrt\n",
      "Pow /backbone/stage0/stage0.6/tcm/tcm.0/act/Pow\n",
      "Pow /backbone/stage0/stage0.6/tcm/tcm.1/act/Pow\n",
      "Pow /backbone/stage0/stage0.6/tcm/tcm.2/act/Pow\n",
      "Pow /backbone/stage0/stage0.6/tcm/tcm.3/act/Pow\n",
      "Transpose /backbone/stage0/stage0.6/tcm/tcm.4/Transpose\n",
      "Transpose /backbone/stage0/stage0.6/tcm/tcm.4/attention/Transpose\n",
      "Transpose /backbone/stage0/stage0.6/tcm/tcm.4/attention/Transpose_1\n",
      "Transpose /backbone/stage0/stage0.6/tcm/tcm.4/attention/Transpose_2\n",
      "Transpose /backbone/stage0/stage0.6/tcm/tcm.4/attention/Transpose_3\n",
      "Transpose /backbone/stage0/stage0.6/tcm/tcm.4/attention/Transpose_4\n",
      "Pow /backbone/stage0/stage0.6/tcm/tcm.4/feed_forward/intermediate_act_fn/Pow\n",
      "Transpose /backbone/stage0/stage0.6/tcm/tcm.4/Transpose_1\n",
      "Transpose /backbone/stage1/stage1.1/Transpose\n",
      "Pow /backbone/stage1/stage1.6/stage1.6.2/Pow\n",
      "Transpose /backbone/stage1/stage1.7/Transpose\n",
      "Pow /backbone/stage1/stage1.8/red_dim_conv/red_dim_conv.1/Pow\n",
      "Sqrt /backbone/stage1/stage1.8/red_dim_conv/red_dim_conv.1/Sqrt\n",
      "Pow /backbone/stage1/stage1.8/tcm/tcm.0/act/Pow\n",
      "Pow /backbone/stage1/stage1.8/tcm/tcm.1/act/Pow\n",
      "Pow /backbone/stage1/stage1.8/tcm/tcm.2/act/Pow\n",
      "Pow /backbone/stage1/stage1.8/tcm/tcm.3/act/Pow\n",
      "Transpose /backbone/stage1/stage1.8/tcm/tcm.4/Transpose\n",
      "Transpose /backbone/stage1/stage1.8/tcm/tcm.4/attention/Transpose\n",
      "Transpose /backbone/stage1/stage1.8/tcm/tcm.4/attention/Transpose_1\n",
      "Transpose /backbone/stage1/stage1.8/tcm/tcm.4/attention/Transpose_2\n",
      "Transpose /backbone/stage1/stage1.8/tcm/tcm.4/attention/Transpose_3\n",
      "Transpose /backbone/stage1/stage1.8/tcm/tcm.4/attention/Transpose_4\n",
      "Pow /backbone/stage1/stage1.8/tcm/tcm.4/feed_forward/intermediate_act_fn/Pow\n",
      "Transpose /backbone/stage1/stage1.8/tcm/tcm.4/Transpose_1\n",
      "Transpose /backbone/stage2/stage2.1/Transpose\n",
      "Pow /backbone/stage2/stage2.6/stage2.6.2/Pow\n",
      "Transpose /backbone/stage2/stage2.7/Transpose\n",
      "Pow /backbone/stage2/stage2.8/red_dim_conv/red_dim_conv.1/Pow\n",
      "Sqrt /backbone/stage2/stage2.8/red_dim_conv/red_dim_conv.1/Sqrt\n",
      "Pow /backbone/stage2/stage2.8/tcm/tcm.0/act/Pow\n",
      "Pow /backbone/stage2/stage2.8/tcm/tcm.1/act/Pow\n",
      "Pow /backbone/stage2/stage2.8/tcm/tcm.2/act/Pow\n",
      "Pow /backbone/stage2/stage2.8/tcm/tcm.3/act/Pow\n",
      "Transpose /backbone/stage2/stage2.8/tcm/tcm.4/Transpose\n",
      "Transpose /backbone/stage2/stage2.8/tcm/tcm.4/attention/Transpose\n",
      "Transpose /backbone/stage2/stage2.8/tcm/tcm.4/attention/Transpose_1\n",
      "Transpose /backbone/stage2/stage2.8/tcm/tcm.4/attention/Transpose_2\n",
      "Transpose /backbone/stage2/stage2.8/tcm/tcm.4/attention/Transpose_3\n",
      "Transpose /backbone/stage2/stage2.8/tcm/tcm.4/attention/Transpose_4\n",
      "Pow /backbone/stage2/stage2.8/tcm/tcm.4/feed_forward/intermediate_act_fn/Pow\n",
      "Transpose /backbone/stage2/stage2.8/tcm/tcm.4/Transpose_1\n",
      "Transpose /backbone/stage3/stage3.1/Transpose\n",
      "Pow /backbone/stage3/stage3.7/stage3.7.2/Pow\n",
      "Transpose /backbone/stage3/stage3.8/Transpose\n",
      "Pow /backbone/stage3/stage3.9/red_dim_conv/red_dim_conv.1/Pow\n",
      "Sqrt /backbone/stage3/stage3.9/red_dim_conv/red_dim_conv.1/Sqrt\n",
      "Pow /backbone/stage3/stage3.9/tcm/tcm.0/act/Pow\n",
      "Pow /backbone/stage3/stage3.9/tcm/tcm.1/act/Pow\n",
      "Pow /backbone/stage3/stage3.9/tcm/tcm.2/act/Pow\n",
      "Pow /backbone/stage3/stage3.9/tcm/tcm.3/act/Pow\n",
      "Transpose /backbone/stage3/stage3.9/tcm/tcm.4/Transpose\n",
      "Transpose /backbone/stage3/stage3.9/tcm/tcm.4/attention/Transpose\n",
      "Transpose /backbone/stage3/stage3.9/tcm/tcm.4/attention/Transpose_1\n",
      "Transpose /backbone/stage3/stage3.9/tcm/tcm.4/attention/Transpose_2\n",
      "Transpose /backbone/stage3/stage3.9/tcm/tcm.4/attention/Transpose_3\n",
      "Transpose /backbone/stage3/stage3.9/tcm/tcm.4/attention/Transpose_4\n",
      "Pow /backbone/stage3/stage3.9/tcm/tcm.4/feed_forward/intermediate_act_fn/Pow\n",
      "Transpose /backbone/stage3/stage3.9/tcm/tcm.4/Transpose_1\n",
      "Transpose /backbone/stage4/stage4.1/Transpose\n",
      "Transpose /backbone/stage4/stage4.6/Transpose\n",
      "Pow /backbone/stage4/stage4.7/red_dim_conv/red_dim_conv.1/Pow\n",
      "Sqrt /backbone/stage4/stage4.7/red_dim_conv/red_dim_conv.1/Sqrt\n",
      "Pow /backbone/stage4/stage4.7/tcm/tcm.0/act/Pow\n",
      "Pow /backbone/stage4/stage4.7/tcm/tcm.1/act/Pow\n",
      "Pow /backbone/stage4/stage4.7/tcm/tcm.2/act/Pow\n",
      "Pow /backbone/stage4/stage4.7/tcm/tcm.3/act/Pow\n",
      "Transpose /backbone/stage4/stage4.7/tcm/tcm.4/Transpose\n",
      "Transpose /backbone/stage4/stage4.7/tcm/tcm.4/attention/Transpose\n",
      "Transpose /backbone/stage4/stage4.7/tcm/tcm.4/attention/Transpose_1\n",
      "Transpose /backbone/stage4/stage4.7/tcm/tcm.4/attention/Transpose_2\n",
      "Transpose /backbone/stage4/stage4.7/tcm/tcm.4/attention/Transpose_3\n",
      "Transpose /backbone/stage4/stage4.7/tcm/tcm.4/attention/Transpose_4\n",
      "Pow /backbone/stage4/stage4.7/tcm/tcm.4/feed_forward/intermediate_act_fn/Pow\n",
      "Transpose /backbone/stage4/stage4.7/tcm/tcm.4/Transpose_1\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "m= onnx_model\n",
    "bad_ops = {\"Gather\", \"Pow\", \"Sqrt\", \"Log\", \"Exp\", \"Transpose\"}  # add more if needed\n",
    "for n in m.graph.node:\n",
    "    if n.op_type in bad_ops:\n",
    "        print(n.op_type, n.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_onnx(wav_path):\n",
    "    \"\"\"\n",
    "    Loads an audio file, converts to log-mel, and runs inference\n",
    "    in an ONNX session. Returns the embedding as a NumPy array.\n",
    "    \"\"\"\n",
    "    print(\"===================================================\")\n",
    "    print(\"===========   run_inference_onnx   ================\")\n",
    "    print(\"===================================================\")\n",
    "    #######################################\n",
    "    # 1) Load your ONNX model\n",
    "    #######################################\n",
    "    # (Optional) onnx.checker to confirm it’s valid\n",
    "    onnx_model = onnx.load(onnx_path)\n",
    "    onnx.checker.check_model(onnx_model)\n",
    "    print(f\"Loaded and checked ONNX model from: {onnx_path}\")\n",
    "\n",
    "    # Create an inference session\n",
    "    session = ort.InferenceSession(onnx_path)\n",
    "\n",
    "    # Usually we retrieve the first input & output name\n",
    "    input_name = session.get_inputs()[0].name\n",
    "    output_name = session.get_outputs()[0].name\n",
    "\n",
    "    #######################################\n",
    "    # 2) Load audio, get log-mel\n",
    "    #######################################\n",
    "    print(\"loading audio from:\", wav_path)\n",
    "    waveform, sample_rate = torchaudio.load(wav_path)\n",
    "    print(f\"...Waveform rate {sample_rate}  ; shape : {waveform.shape}\")\n",
    "\n",
    "    \n",
    "    # If multi-channel, downmix:\n",
    "    if waveform.shape[0] > 1:\n",
    "        waveform = waveform.mean(dim=0, keepdim=True)\n",
    "        \n",
    "    # Resample if needed\n",
    "    target_sample_rate=16000\n",
    "    if sample_rate != target_sample_rate:\n",
    "        resampler = T.Resample(orig_freq=sample_rate, new_freq=target_sample_rate)\n",
    "        waveform = resampler(waveform)\n",
    "        # save resampled waveform to files with suffix \"_resampled_16.wav\"\n",
    "        # torchaudio.save(wav_path.replace(\".wav\", \"_resampled_16.wav\"), waveform, target_sample_rate)\n",
    "\n",
    "    log_mel =  myUtils.waveform_to_logmel(waveform)\n",
    "    \n",
    "    #######################################\n",
    "    # 3) ONNX Inference\n",
    "    #######################################\n",
    "    # Convert to NumPy for ONNX runtime\n",
    "    log_mel_np = log_mel.cpu().numpy()\n",
    "    \n",
    "    ## save log_mel_np to file with suffix \"_logmel.npy\" to check later\n",
    "    print(\"logmelshape : \", log_mel_np.shape)\n",
    "    log_mel_fp16 = log_mel_np.astype(np.float16)  # → half precision\n",
    "    orig_name = os.path.splitext(os.path.basename(wav_path))[0]\n",
    "    folder = os.path.dirname(wav_path)\n",
    "    out_path = os.path.join(folder, f\"logmel_{orig_name}.npy\")\n",
    "    np.save(out_path, log_mel_fp16)\n",
    "    \n",
    "    # Run inference\n",
    "    outputs = session.run([output_name], {input_name: log_mel_np})\n",
    "    # outputs is a list; typically we want the first item\n",
    "    embedding = outputs[0]  # shape is [1, embedding_dim]\n",
    "\n",
    "    # print(\"Embedding[10]: \", embedding[0:10])  # Print the 10th element of the embedding\n",
    "    print(\"Embedding shape:\", embedding.shape)\n",
    "    # print(\"Embedding data:\\n\", embedding)\n",
    "    return embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================================\n",
      "===========   run_inference_onnx   ================\n",
      "===================================================\n",
      "Loaded and checked ONNX model from: ReDimNet_no_mel_fp16.onnx\n",
      "loading audio from: /data/proj/voice/redimnet/wrkB0/utils/../audio/test000.wav\n",
      "...Waveform rate 16000  ; shape : torch.Size([1, 293699])\n",
      "Input waveform shape: torch.Size([1, 32000])\n",
      "logmelshape :  (1, 1, 60, 134)\n",
      "Embedding shape: (1, 192)\n",
      "===================================================\n",
      "===========   run_inference_onnx   ================\n",
      "===================================================\n",
      "Loaded and checked ONNX model from: ReDimNet_no_mel_fp16.onnx\n",
      "loading audio from: /data/proj/voice/redimnet/wrkB0/utils/../audio/testRob1.wav\n",
      "...Waveform rate 22050  ; shape : torch.Size([1, 35522])\n",
      "Input waveform shape: torch.Size([1, 25776])\n",
      "Padding log_mel from 108 to 134 frames\n",
      "logmelshape :  (1, 1, 60, 134)\n",
      "Embedding shape: (1, 192)\n",
      "===================================================\n",
      "===========   run_inference_onnx   ================\n",
      "===================================================\n",
      "Loaded and checked ONNX model from: ReDimNet_no_mel_fp16.onnx\n",
      "loading audio from: /data/proj/voice/redimnet/wrkB0/utils/../audio/testRob2.wav\n",
      "...Waveform rate 22050  ; shape : torch.Size([1, 32482])\n",
      "Input waveform shape: torch.Size([1, 23570])\n",
      "Padding log_mel from 99 to 134 frames\n",
      "logmelshape :  (1, 1, 60, 134)\n",
      "Embedding shape: (1, 192)\n",
      "===================================================\n",
      "===========   run_inference_onnx   ================\n",
      "===================================================\n",
      "Loaded and checked ONNX model from: ReDimNet_no_mel_fp16.onnx\n",
      "loading audio from: /data/proj/voice/redimnet/wrkB0/utils/../audio/test_human1_1.wav\n",
      "...Waveform rate 16000  ; shape : torch.Size([1, 65867])\n",
      "Input waveform shape: torch.Size([1, 32000])\n",
      "logmelshape :  (1, 1, 60, 134)\n",
      "Embedding shape: (1, 192)\n",
      "===================================================\n",
      "===========   run_inference_onnx   ================\n",
      "===================================================\n",
      "Loaded and checked ONNX model from: ReDimNet_no_mel_fp16.onnx\n",
      "loading audio from: /data/proj/voice/redimnet/wrkB0/utils/../audio/test_human1_2.wav\n",
      "...Waveform rate 16000  ; shape : torch.Size([1, 101189])\n",
      "Input waveform shape: torch.Size([1, 32000])\n",
      "logmelshape :  (1, 1, 60, 134)\n",
      "Embedding shape: (1, 192)\n",
      "===================================================\n",
      "===========   run_inference_onnx   ================\n",
      "===================================================\n",
      "Loaded and checked ONNX model from: ReDimNet_no_mel_fp16.onnx\n",
      "loading audio from: /data/proj/voice/redimnet/wrkB0/utils/../audio/test_human2_1.wav\n",
      "...Waveform rate 48000  ; shape : torch.Size([1, 84376])\n",
      "Input waveform shape: torch.Size([1, 28126])\n",
      "Padding log_mel from 118 to 134 frames\n",
      "logmelshape :  (1, 1, 60, 134)\n",
      "Embedding shape: (1, 192)\n",
      "===================================================\n",
      "===========   run_inference_onnx   ================\n",
      "===================================================\n",
      "Loaded and checked ONNX model from: ReDimNet_no_mel_fp16.onnx\n",
      "loading audio from: /data/proj/voice/redimnet/wrkB0/utils/../audio/test_human2_2.wav\n",
      "...Waveform rate 48000  ; shape : torch.Size([1, 159256])\n",
      "Input waveform shape: torch.Size([1, 32000])\n",
      "logmelshape :  (1, 1, 60, 134)\n",
      "Embedding shape: (1, 192)\n",
      "**************************************************************************\n",
      "*************************   compare summary ******************************\n",
      "**************************************************************************\n",
      "====>>>> should be similar:\n",
      "Similarity (robot1 to robot2 ): 0.8907718062400818\n",
      "Similarity (human1 to human1 ): 0.8241685628890991\n",
      "Similarity (human2 to human2 ): 0.6442629098892212\n",
      "====>>>> should be differnet:\n",
      "Similarity (robot to human1  ): 0.40877649188041687\n",
      "Similarity (robot to human2  ): 0.4167248010635376\n",
      "Similarity (human1 to human2 ): 0.5727478861808777\n"
     ]
    }
   ],
   "source": [
    "onnx_embedding = test_all_voices(\n",
    "    extract_speaker_embedding_function = inference_onnx,\n",
    "    cosine_similarity_function = myUtils.cosine_similarity_numpys,\n",
    "    save_embeddings=True,  # Save embeddings to files\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### compare onnx with torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity embde0: 0.9999986290931702\n",
      "Similarity embde1: 0.9999992251396179\n",
      "Similarity embde2: 0.9999989867210388\n",
      "Similarity embde3: 0.9999982714653015\n",
      "Similarity embde4: 0.9999988079071045\n",
      "Similarity embde5: 0.9999988675117493\n",
      "Similarity embde6: 0.9999987483024597\n"
     ]
    }
   ],
   "source": [
    "print(f\"Similarity embde0: {myUtils.cosine_similarity_numpys(torch_embedding['embed0'], onnx_embedding['embed0'])}\")\n",
    "print(f\"Similarity embde1: {myUtils.cosine_similarity_numpys(torch_embedding['embed1'], onnx_embedding['embed1'])}\")\n",
    "print(f\"Similarity embde2: {myUtils.cosine_similarity_numpys(torch_embedding['embed2'], onnx_embedding['embed2'])}\")\n",
    "print(f\"Similarity embde3: {myUtils.cosine_similarity_numpys(torch_embedding['embed3'], onnx_embedding['embed3'])}\")\n",
    "print(f\"Similarity embde4: {myUtils.cosine_similarity_numpys(torch_embedding['embed4'], onnx_embedding['embed4'])}\")\n",
    "print(f\"Similarity embde5: {myUtils.cosine_similarity_numpys(torch_embedding['embed5'], onnx_embedding['embed5'])}\")\n",
    "print(f\"Similarity embde6: {myUtils.cosine_similarity_numpys(torch_embedding['embed6'], onnx_embedding['embed6'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### compare onnx with base line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity embde0: 0.35939159989356995\n",
      "Similarity embde1: 0.6001739501953125\n",
      "Similarity embde2: 0.5507152080535889\n",
      "Similarity embde3: 0.33716222643852234\n",
      "Similarity embde4: 0.287955105304718\n",
      "Similarity embde5: 0.4518096148967743\n",
      "Similarity embde6: 0.35607239603996277\n"
     ]
    }
   ],
   "source": [
    "print(f\"Similarity embde0: {myUtils.cosine_similarity_numpys(base_line_embedding['embed0'], onnx_embedding['embed0'])}\")\n",
    "print(f\"Similarity embde1: {myUtils.cosine_similarity_numpys(base_line_embedding['embed1'], onnx_embedding['embed1'])}\")\n",
    "print(f\"Similarity embde2: {myUtils.cosine_similarity_numpys(base_line_embedding['embed2'], onnx_embedding['embed2'])}\")\n",
    "print(f\"Similarity embde3: {myUtils.cosine_similarity_numpys(base_line_embedding['embed3'], onnx_embedding['embed3'])}\")\n",
    "print(f\"Similarity embde4: {myUtils.cosine_similarity_numpys(base_line_embedding['embed4'], onnx_embedding['embed4'])}\")\n",
    "print(f\"Similarity embde5: {myUtils.cosine_similarity_numpys(base_line_embedding['embed5'], onnx_embedding['embed5'])}\")\n",
    "print(f\"Similarity embde6: {myUtils.cosine_similarity_numpys(base_line_embedding['embed6'], onnx_embedding['embed6'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cal fake data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* dummy for nchw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import numpy as np\n",
    "# import torch\n",
    "\n",
    "# # Directory for calibration inputs\n",
    "# os.makedirs(\"calib_npy\", exist_ok=True)\n",
    "\n",
    "# # Create 100 dummy log-mel tensors\n",
    "# for i in range(10):\n",
    "#     log_mel = torch.randn(1, 1, 60, 134).numpy().astype(np.float16)\n",
    "#     np.save(f\"calib_npy/sample_{i}.npy\", log_mel)\n",
    "\n",
    "# # Write dataset.txt listing all paths\n",
    "# with open(\"dataset.txt\", \"w\") as f:\n",
    "#     for i in range(10):\n",
    "#         f.write(f\"calib_npy/sample_{i}.npy\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* dummy for nchw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import numpy as np\n",
    "# import torch\n",
    "\n",
    "# # Directory for calibration inputs\n",
    "# os.makedirs(\"calib_npy\", exist_ok=True)\n",
    "\n",
    "# # Create 100 dummy log-mel tensors\n",
    "# for i in range(2):\n",
    "#     log_mel = torch.randn(1, 60, 134,1).numpy().astype(np.float16)\n",
    "#     np.save(f\"calib_npy/sample_{i}.npy\", log_mel)\n",
    "\n",
    "# # Write dataset.txt listing all paths\n",
    "# with open(\"dataset.txt\", \"w\") as f:\n",
    "#     for i in range(10):\n",
    "#         f.write(f\"calib_npy/sample_{i}.npy\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# converts\n",
    "\n",
    "```\n",
    "python convert.py \\\n",
    "       ../wrkB0/ReDimNet_no_mel_fp16.onnx rk3588 fp ReDimNet_no_mel.rknn \\\n",
    "       ../wrkB0/audio/logmel_testRob1.npy  ../wrkB0/audio/embedding_testRob1.torch\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyvoice_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
