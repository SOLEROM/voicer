{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RKNN fixed version \n",
    "\n",
    "* noMel\n",
    "* conv1d + fix for pad\n",
    "* newAVact - approximation of GELU \n",
    "\n",
    "===============================================================\n",
    "\n",
    "* build new noMel model based on base line\n",
    "* replace bad layers with working layers\n",
    "    * no need to change the width of the layers\n",
    "* run voices through the model and compare to baseline\n",
    "\n",
    "==============================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0+cu124\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "## our utils\n",
    "from utils.common_import import *\n",
    "from utils.test_all_voices import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-display\n",
    "import my_utils as myUtils\n",
    "from play1_setBase_line_B2 import original_model,base_line_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## original activations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACT_TYPES = (\n",
    "    nn.ReLU, nn.ReLU6, nn.LeakyReLU, nn.ELU, nn.PReLU, nn.GELU,\n",
    "    nn.SiLU, nn.Sigmoid, nn.Tanh, nn.Hardswish\n",
    ")\n",
    "\n",
    "def list_activations(model):\n",
    "    \"\"\"Print every module whose class is in ACT_TYPES.\"\"\"\n",
    "    for name, m in model.named_modules():\n",
    "        if isinstance(m, ACT_TYPES):\n",
    "            print(f'{name:<60} {m}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "backbone.stage0.3.conv_block.act                             GELU(approximate='none')\n",
      "backbone.stage0.4.conv_block.act                             GELU(approximate='none')\n",
      "backbone.stage0.6.tcm.0.act                                  GELU(approximate='none')\n",
      "backbone.stage0.6.tcm.1.act                                  GELU(approximate='none')\n",
      "backbone.stage0.6.tcm.2.act                                  GELU(approximate='none')\n",
      "backbone.stage0.6.tcm.3.act                                  GELU(approximate='none')\n",
      "backbone.stage1.3.conv_block.act                             GELU(approximate='none')\n",
      "backbone.stage1.4.conv_block.act                             GELU(approximate='none')\n",
      "backbone.stage1.6.tcm.0.act                                  GELU(approximate='none')\n",
      "backbone.stage1.6.tcm.1.act                                  GELU(approximate='none')\n",
      "backbone.stage1.6.tcm.2.act                                  GELU(approximate='none')\n",
      "backbone.stage1.6.tcm.3.act                                  GELU(approximate='none')\n",
      "backbone.stage2.3.conv_block.act                             GELU(approximate='none')\n",
      "backbone.stage2.4.conv_block.act                             GELU(approximate='none')\n",
      "backbone.stage2.5.conv_block.act                             GELU(approximate='none')\n",
      "backbone.stage2.7.tcm.0.act                                  GELU(approximate='none')\n",
      "backbone.stage2.7.tcm.1.act                                  GELU(approximate='none')\n",
      "backbone.stage2.7.tcm.2.act                                  GELU(approximate='none')\n",
      "backbone.stage2.7.tcm.3.act                                  GELU(approximate='none')\n",
      "backbone.stage3.3.conv_block.act                             GELU(approximate='none')\n",
      "backbone.stage3.4.conv_block.act                             GELU(approximate='none')\n",
      "backbone.stage3.5.conv_block.act                             GELU(approximate='none')\n",
      "backbone.stage3.6.conv_block.act                             GELU(approximate='none')\n",
      "backbone.stage3.8.tcm.0.act                                  GELU(approximate='none')\n",
      "backbone.stage3.8.tcm.1.act                                  GELU(approximate='none')\n",
      "backbone.stage3.8.tcm.2.act                                  GELU(approximate='none')\n",
      "backbone.stage3.8.tcm.3.act                                  GELU(approximate='none')\n",
      "backbone.stage4.3.conv_block.act                             GELU(approximate='none')\n",
      "backbone.stage4.4.conv_block.act                             GELU(approximate='none')\n",
      "backbone.stage4.5.conv_block.act                             GELU(approximate='none')\n",
      "backbone.stage4.6.conv_block.act                             GELU(approximate='none')\n",
      "backbone.stage4.8.tcm.0.act                                  GELU(approximate='none')\n",
      "backbone.stage4.8.tcm.1.act                                  GELU(approximate='none')\n",
      "backbone.stage4.8.tcm.2.act                                  GELU(approximate='none')\n",
      "backbone.stage4.8.tcm.3.act                                  GELU(approximate='none')\n",
      "backbone.stage5.3.conv_block.act                             GELU(approximate='none')\n",
      "backbone.stage5.4.conv_block.act                             GELU(approximate='none')\n",
      "backbone.stage5.5.conv_block.act                             GELU(approximate='none')\n",
      "backbone.stage5.6.conv_block.act                             GELU(approximate='none')\n",
      "backbone.stage5.8.tcm.0.act                                  GELU(approximate='none')\n",
      "backbone.stage5.8.tcm.1.act                                  GELU(approximate='none')\n",
      "backbone.stage5.8.tcm.2.act                                  GELU(approximate='none')\n",
      "backbone.stage5.8.tcm.3.act                                  GELU(approximate='none')\n"
     ]
    }
   ],
   "source": [
    "list_activations(original_model)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create new model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conv1dAs2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from math import floor\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "#  Global switches\n",
    "# ---------------------------------------------------------------------\n",
    "DEBUG = True                 # False → silence all prints\n",
    "MAX_NPU_KERNEL = 10          # Rockchip depth-wise limit (stride = 1)\n",
    "\n",
    "\n",
    "def _dbg(msg: str):\n",
    "    if DEBUG:\n",
    "        print(msg)\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "# 1.  Shape-safe Conv1d → Conv2d wrapper (NO weight edits)\n",
    "# =====================================================================\n",
    "class Conv1dAs2d(nn.Module):\n",
    "    \"\"\"\n",
    "    Wrap a Conv1d as Conv2d without touching its weights:\n",
    "      * accepts [B,C,T]  or  [B,C,T,1]\n",
    "      * gives the same rank back\n",
    "      * never uses 'same' | 'valid' strings → RKNN keeps pads inside node\n",
    "    \"\"\"\n",
    "    def __init__(self, src: nn.Conv1d):\n",
    "        super().__init__()\n",
    "\n",
    "        k = src.kernel_size[0]\n",
    "        d = src.dilation[0]\n",
    "        s = src.stride[0]\n",
    "        g = src.groups\n",
    "        in_c, out_c = src.in_channels, src.out_channels\n",
    "        pad_in = src.padding                     # \"same\"/\"valid\"/int/tuple\n",
    "\n",
    "        # numeric padding (H, W)\n",
    "        if isinstance(pad_in, str):\n",
    "            pad_num = floor(d * (k - 1) / 2) if pad_in == \"same\" else 0\n",
    "        else:\n",
    "            pad_num = pad_in[0] if isinstance(pad_in, tuple) else pad_in\n",
    "        pad_hw = (pad_num, 0)\n",
    "\n",
    "        # banner ------------------------------------------------------\n",
    "        place = \"CPU (k>10 depth-wise)\" if (k > MAX_NPU_KERNEL and g == in_c) else \"NPU\"\n",
    "        _dbg(f\"[Conv1dAs2d] {in_c}→{out_c}  k={k} d={d} s={s} g={g}  pad={pad_hw}  ⇒ {place}\")\n",
    "\n",
    "        # build Conv2d -----------------------------------------------\n",
    "        self.conv2d = nn.Conv2d(\n",
    "            in_channels=in_c,\n",
    "            out_channels=out_c,\n",
    "            kernel_size=(k, 1),\n",
    "            stride=(s, 1),\n",
    "            padding=pad_hw,\n",
    "            dilation=(d, 1),\n",
    "            groups=g,\n",
    "            bias=src.bias is not None,\n",
    "        )\n",
    "\n",
    "        # copy weights verbatim\n",
    "        with torch.no_grad():\n",
    "            self.conv2d.weight.copy_(src.weight.unsqueeze(-1))\n",
    "            if src.bias is not None:\n",
    "                self.conv2d.bias.copy_(src.bias)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        add_dim = False\n",
    "        if x.dim() == 3:                   # [B,C,T]  → [B,C,T,1]\n",
    "            x, add_dim = x.unsqueeze(-1), True\n",
    "        elif not (x.dim() == 4 and x.shape[-1] == 1):\n",
    "            raise ValueError(f\"Conv1dAs2d got shape {tuple(x.shape)}\")\n",
    "\n",
    "        y = self.conv2d(x)                 # Conv2d works on 4-D\n",
    "        return y.squeeze(-1) if add_dim else y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### split >10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from math import floor\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "#  Global settings\n",
    "# ------------------------------------------------------------\n",
    "DEBUG_CONV1D_AS_2D   = True            # set False to silence all prints\n",
    "MAX_NPU_KERNEL       = 10              # HW limit (stride = 1)\n",
    "MAX_SUB_KERNEL_SPLIT = 9               # odd, so symmetric pad works\n",
    "\n",
    "def _dbg(msg: str):\n",
    "    if DEBUG_CONV1D_AS_2D:\n",
    "        print(msg)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "#  Helper: split a large receptive field into odd sub-kernels ≤ 9\n",
    "#  For stride==1 the effective RF of a cascade is:\n",
    "#        k_eff = sum(k_i) - (n_stages - 1)\n",
    "# ------------------------------------------------------------\n",
    "def _split_kernel(k: int, k_max: int = MAX_SUB_KERNEL_SPLIT):\n",
    "    \"\"\"Return a list of odd kernel sizes whose cascade reproduces k.\"\"\"\n",
    "    if k <= k_max:\n",
    "        return [k]\n",
    "\n",
    "    segments = []\n",
    "    remaining = k\n",
    "    while remaining > k_max:\n",
    "        segments.append(k_max)             # add a full-size 9\n",
    "        remaining -= (k_max - 1)           # because RF grows by k_max-1\n",
    "    if remaining % 2 == 0:                 # make it odd (8 → 7)\n",
    "        remaining -= 1\n",
    "        segments[-1] += 1                  # compensate so RF stays exact\n",
    "    segments.append(remaining)\n",
    "    assert sum(segments) - (len(segments)-1) == k, \"RF mismatch\"\n",
    "    return segments\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "#  Main wrapper\n",
    "# ------------------------------------------------------------\n",
    "class Conv1dAs2d_split(nn.Module):\n",
    "    \"\"\"\n",
    "    * k ≤ 10  → single Conv2d (weights copied, runs on NPU)\n",
    "    * k  > 10 → cascade of Conv2d layers, every sub-kernel ≤ 9 (runs on NPU)\n",
    "                (weights are *not* copied; fine-tune is required)\n",
    "    Padding is always numeric – ONNX will not emit a standalone Pad op.\n",
    "    \"\"\"\n",
    "    def __init__(self, conv1d: nn.Conv1d):\n",
    "        super().__init__()\n",
    "\n",
    "        # ---------- original 1-D parameters ----------\n",
    "        k, d, s, g = conv1d.kernel_size[0], conv1d.dilation[0], conv1d.stride[0], conv1d.groups\n",
    "        in_c, out_c = conv1d.in_channels, conv1d.out_channels\n",
    "        pad_in = conv1d.padding                         # \"same\" | \"valid\" | int/tuple\n",
    "\n",
    "        if d != 1 or s != 1:\n",
    "            raise ValueError(\"Wrapper currently supports stride=1, dilation=1 only\")\n",
    "\n",
    "        # ----------------------------------------------------\n",
    "        # Case A — kernel already NPU-friendly\n",
    "        # ----------------------------------------------------\n",
    "        if k <= MAX_NPU_KERNEL:\n",
    "            pad_num = floor((k - 1) / 2) if isinstance(pad_in, str) and pad_in == \"same\" \\\n",
    "                      else (pad_in[0] if isinstance(pad_in, tuple) else pad_in)\n",
    "            pad_arg = (pad_num, 0)\n",
    "\n",
    "            self.conv = nn.Conv2d(\n",
    "                in_channels=in_c, out_channels=out_c,\n",
    "                kernel_size=(k, 1), stride=(1, 1),\n",
    "                padding=pad_arg, dilation=(1, 1),\n",
    "                groups=g, bias=conv1d.bias is not None\n",
    "            )\n",
    "            with torch.no_grad():\n",
    "                self.conv.weight.copy_(conv1d.weight.unsqueeze(-1))\n",
    "                if conv1d.bias is not None:\n",
    "                    self.conv.bias.copy_(conv1d.bias)\n",
    "\n",
    "            _dbg(f\"[Conv1dAs2d] k={k} → single Conv2d, pad={pad_arg}, runs on NPU\")\n",
    "\n",
    "        # ----------------------------------------------------\n",
    "        # Case B — kernel too wide → split into ≤9-tap stages\n",
    "        # ----------------------------------------------------\n",
    "        else:\n",
    "            k_list = _split_kernel(k)                   # e.g. 59 → [9,9,9,9,9,5]\n",
    "            layers = []\n",
    "            for i, ks in enumerate(k_list):\n",
    "                pad_num = (ks - 1) // 2                 # symmetric\n",
    "                conv2d = nn.Conv2d(\n",
    "                    in_channels=in_c, out_channels=out_c,\n",
    "                    kernel_size=(ks, 1), stride=(1, 1),\n",
    "                    padding=(pad_num, 0), dilation=(1, 1),\n",
    "                    groups=g, bias=False                # leave bias out; easier to fine-tune later\n",
    "                )\n",
    "                layers.append(conv2d)\n",
    "                _dbg(f\"[Conv1dAs2d]  ├─ stage {i}: ks={ks}, pad={pad_num}\")\n",
    "            self.conv = nn.Sequential(*layers)\n",
    "\n",
    "            _dbg(f\"[Conv1dAs2d] k={k} split into {k_list} (cascade runs on NPU)\\n\"\n",
    "                 \"           ⚠ weights not copied — fine-tune is required\")\n",
    "\n",
    "        _dbg(\"------------------------------------------------------------\")\n",
    "\n",
    "    # forward\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:   # x: [B, C, T]\n",
    "        return self.conv(x.unsqueeze(-1)).squeeze(-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### safe pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 1. RKNN-safe Attentive Statistics Pooling that matches any channel count\n",
    "# ---------------------------------------------------------------------\n",
    "class ASTP_RKNNSafe(nn.Module):\n",
    "    \"\"\"ASTP rewritten so linear1 expects in_channels (600 here).\"\"\"\n",
    "    def __init__(self, src_pool: nn.Module, in_channels: int):\n",
    "        super().__init__()\n",
    "        mid_channels = src_pool.linear1.out_channels   # 128\n",
    "        out_channels = in_channels                     # 600\n",
    "\n",
    "        # fresh 1x1 conv layers\n",
    "        self.linear1 = nn.Conv1d(in_channels, mid_channels, kernel_size=1, bias=True)\n",
    "        self.linear2 = nn.Conv1d(mid_channels, out_channels, kernel_size=1, bias=True)\n",
    "\n",
    "        # copy original weights where dimensions allow\n",
    "        with torch.no_grad():\n",
    "            # linear1: tile or truncate old weights to fit new in_channels\n",
    "            old_w1 = src_pool.linear1.weight           # [128, 1800, 1]\n",
    "            repeat = (in_channels + old_w1.size(1) - 1) // old_w1.size(1)\n",
    "            new_w1 = old_w1.repeat(1, repeat, 1)[:, :in_channels, :]\n",
    "            self.linear1.weight.copy_(new_w1)\n",
    "            self.linear1.bias.copy_(src_pool.linear1.bias)\n",
    "\n",
    "            # linear2: out_channels is 600, just copy first 600\n",
    "            self.linear2.weight.copy_(src_pool.linear2.weight[:, :out_channels, :])\n",
    "            self.linear2.bias.copy_(src_pool.linear2.bias[:out_channels])\n",
    "\n",
    "        # make them RKNN friendly\n",
    "        self.linear1 = Conv1dAs2d(self.linear1)\n",
    "        self.linear2 = Conv1dAs2d(self.linear2)\n",
    "        self.eps = getattr(src_pool, \"eps\", 1e-12)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn = torch.softmax(self.linear2(torch.tanh(self.linear1(x))), dim=-1)\n",
    "        mean = torch.sum(attn * x, dim=-1)\n",
    "        var  = torch.sum(attn * (x - mean.unsqueeze(-1)) ** 2, dim=-1)\n",
    "        std  = torch.pow(var + self.eps, 0.5)          # RKNN keeps Pow on NPU\n",
    "        return torch.cat([mean, std], dim=1)           # [B, 2 * C]\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ASTP_RKNNSafe_cpu(nn.Module):\n",
    "    \"\"\"CPU-safe attentive statistics pooling (mean + variance).\"\"\"\n",
    "    def __init__(self, src_pool: nn.Module, in_channels: int):\n",
    "        super().__init__()\n",
    "        mid = src_pool.linear1.out_channels   # 128\n",
    "        out = in_channels                     # 600\n",
    "\n",
    "        self.linear1 = nn.Conv1d(in_channels, mid, kernel_size=1, bias=True)\n",
    "        self.linear2 = nn.Conv1d(mid,        out, kernel_size=1, bias=True)\n",
    "\n",
    "        # copy or tile pretrained weights\n",
    "        with torch.no_grad():\n",
    "            w1 = src_pool.linear1.weight                 # [128,1800,1]\n",
    "            reps = (in_channels + w1.size(1) - 1) // w1.size(1)\n",
    "            self.linear1.weight.copy_(w1.repeat(1, reps, 1)[:, :in_channels, :])\n",
    "            self.linear1.bias.copy_(src_pool.linear1.bias)\n",
    "\n",
    "            self.linear2.weight.copy_(src_pool.linear2.weight[:, :out, :])\n",
    "            self.linear2.bias.copy_(src_pool.linear2.bias[:out])\n",
    "\n",
    "        self.eps = getattr(src_pool, \"eps\", 1e-12)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [B, C, T]\n",
    "        attn  = torch.softmax(self.linear2(torch.tanh(self.linear1(x))), dim=-1)\n",
    "        mean  = (attn * x).sum(dim=-1)                    # [B, C]\n",
    "\n",
    "        diff  = x - mean.unsqueeze(-1)                    # broadcast\n",
    "        sqr   = diff * diff                              # element-wise square, no Pow\n",
    "        var   = (attn * sqr).sum(dim=-1) + self.eps       # [B, C]\n",
    "\n",
    "        return torch.cat([mean, var], dim=1)              # [B, 2C]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### activations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewGELUActivation(nn.Module):\n",
    "    \"\"\"\n",
    "    tanh-based approximation of GELU from Hendrycks & Gimpel (2016):\n",
    "        0.5 * x * (1 + tanh( √(2/π) · (x + 0.044715 x³) ))\n",
    "    \"\"\"\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1.0 + torch.tanh(\n",
    "            (torch.sqrt(torch.tensor(2.0 / torch.pi, device=x.device)) * \n",
    "             (x + 0.044715 * torch.pow(x, 3)))\n",
    "        ))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_activation_(module, old_cls=nn.GELU, new_cls=nn.ReLU, **new_kwargs):\n",
    "    \"\"\"\n",
    "    In-place, recursive swap of every instance of `old_cls`\n",
    "    with `new_cls(**new_kwargs)`.\n",
    "    \"\"\"\n",
    "    for name, child in module.named_children():\n",
    "        if isinstance(child, old_cls):\n",
    "            setattr(module, name, new_cls(**new_kwargs))\n",
    "        else:\n",
    "            replace_activation_(child, old_cls, new_cls, **new_kwargs)\n",
    "# -----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReDimNetNoMel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Conv1dAs2d] k=7 → single Conv2d, pad=(3, 0), runs on NPU\n",
      "------------------------------------------------------------\n",
      "[Conv1dAs2d] k=1 → single Conv2d, pad=(0, 0), runs on NPU\n",
      "------------------------------------------------------------\n",
      "[Conv1dAs2d]  ├─ stage 0: ks=9, pad=4\n",
      "[Conv1dAs2d]  ├─ stage 1: ks=9, pad=4\n",
      "[Conv1dAs2d]  ├─ stage 2: ks=3, pad=1\n",
      "[Conv1dAs2d] k=19 split into [9, 9, 3] (cascade runs on NPU)\n",
      "           ⚠ weights not copied — fine-tune is required\n",
      "------------------------------------------------------------\n",
      "[Conv1dAs2d] k=1 → single Conv2d, pad=(0, 0), runs on NPU\n",
      "------------------------------------------------------------\n",
      "[Conv1dAs2d]  ├─ stage 0: ks=9, pad=4\n",
      "[Conv1dAs2d]  ├─ stage 1: ks=9, pad=4\n",
      "[Conv1dAs2d]  ├─ stage 2: ks=9, pad=4\n",
      "[Conv1dAs2d]  ├─ stage 3: ks=7, pad=3\n",
      "[Conv1dAs2d] k=31 split into [9, 9, 9, 7] (cascade runs on NPU)\n",
      "           ⚠ weights not copied — fine-tune is required\n",
      "------------------------------------------------------------\n",
      "[Conv1dAs2d] k=1 → single Conv2d, pad=(0, 0), runs on NPU\n",
      "------------------------------------------------------------\n",
      "[Conv1dAs2d]  ├─ stage 0: ks=9, pad=4\n",
      "[Conv1dAs2d]  ├─ stage 1: ks=9, pad=4\n",
      "[Conv1dAs2d]  ├─ stage 2: ks=9, pad=4\n",
      "[Conv1dAs2d]  ├─ stage 3: ks=9, pad=4\n",
      "[Conv1dAs2d]  ├─ stage 4: ks=9, pad=4\n",
      "[Conv1dAs2d]  ├─ stage 5: ks=9, pad=4\n",
      "[Conv1dAs2d]  ├─ stage 6: ks=9, pad=4\n",
      "[Conv1dAs2d]  ├─ stage 7: ks=3, pad=1\n",
      "[Conv1dAs2d] k=59 split into [9, 9, 9, 9, 9, 9, 9, 3] (cascade runs on NPU)\n",
      "           ⚠ weights not copied — fine-tune is required\n",
      "------------------------------------------------------------\n",
      "[Conv1dAs2d] k=1 → single Conv2d, pad=(0, 0), runs on NPU\n",
      "------------------------------------------------------------\n",
      "[Conv1dAs2d] k=7 → single Conv2d, pad=(3, 0), runs on NPU\n",
      "------------------------------------------------------------\n",
      "[Conv1dAs2d] k=1 → single Conv2d, pad=(0, 0), runs on NPU\n",
      "------------------------------------------------------------\n",
      "[Conv1dAs2d]  ├─ stage 0: ks=9, pad=4\n",
      "[Conv1dAs2d]  ├─ stage 1: ks=9, pad=4\n",
      "[Conv1dAs2d]  ├─ stage 2: ks=3, pad=1\n",
      "[Conv1dAs2d] k=19 split into [9, 9, 3] (cascade runs on NPU)\n",
      "           ⚠ weights not copied — fine-tune is required\n",
      "------------------------------------------------------------\n",
      "[Conv1dAs2d] k=1 → single Conv2d, pad=(0, 0), runs on NPU\n",
      "------------------------------------------------------------\n",
      "[Conv1dAs2d]  ├─ stage 0: ks=9, pad=4\n",
      "[Conv1dAs2d]  ├─ stage 1: ks=9, pad=4\n",
      "[Conv1dAs2d]  ├─ stage 2: ks=9, pad=4\n",
      "[Conv1dAs2d]  ├─ stage 3: ks=7, pad=3\n",
      "[Conv1dAs2d] k=31 split into [9, 9, 9, 7] (cascade runs on NPU)\n",
      "           ⚠ weights not copied — fine-tune is required\n",
      "------------------------------------------------------------\n",
      "[Conv1dAs2d] k=1 → single Conv2d, pad=(0, 0), runs on NPU\n",
      "------------------------------------------------------------\n",
      "[Conv1dAs2d]  ├─ stage 0: ks=9, pad=4\n",
      "[Conv1dAs2d]  ├─ stage 1: ks=9, pad=4\n",
      "[Conv1dAs2d]  ├─ stage 2: ks=9, pad=4\n",
      "[Conv1dAs2d]  ├─ stage 3: ks=9, pad=4\n",
      "[Conv1dAs2d]  ├─ stage 4: ks=9, pad=4\n",
      "[Conv1dAs2d]  ├─ stage 5: ks=9, pad=4\n",
      "[Conv1dAs2d]  ├─ stage 6: ks=9, pad=4\n",
      "[Conv1dAs2d]  ├─ stage 7: ks=3, pad=1\n",
      "[Conv1dAs2d] k=59 split into [9, 9, 9, 9, 9, 9, 9, 3] (cascade runs on NPU)\n",
      "           ⚠ weights not copied — fine-tune is required\n",
      "------------------------------------------------------------\n",
      "[Conv1dAs2d] k=1 → single Conv2d, pad=(0, 0), runs on NPU\n",
      "------------------------------------------------------------\n",
      "[Conv1dAs2d] k=7 → single Conv2d, pad=(3, 0), runs on NPU\n",
      "------------------------------------------------------------\n",
      "[Conv1dAs2d] k=1 → single Conv2d, pad=(0, 0), runs on NPU\n",
      "------------------------------------------------------------\n",
      "[Conv1dAs2d]  ├─ stage 0: ks=9, pad=4\n",
      "[Conv1dAs2d]  ├─ stage 1: ks=9, pad=4\n",
      "[Conv1dAs2d]  ├─ stage 2: ks=3, pad=1\n",
      "[Conv1dAs2d] k=19 split into [9, 9, 3] (cascade runs on NPU)\n",
      "           ⚠ weights not copied — fine-tune is required\n",
      "------------------------------------------------------------\n",
      "[Conv1dAs2d] k=1 → single Conv2d, pad=(0, 0), runs on NPU\n",
      "------------------------------------------------------------\n",
      "[Conv1dAs2d]  ├─ stage 0: ks=9, pad=4\n",
      "[Conv1dAs2d]  ├─ stage 1: ks=9, pad=4\n",
      "[Conv1dAs2d]  ├─ stage 2: ks=9, pad=4\n",
      "[Conv1dAs2d]  ├─ stage 3: ks=7, pad=3\n",
      "[Conv1dAs2d] k=31 split into [9, 9, 9, 7] (cascade runs on NPU)\n",
      "           ⚠ weights not copied — fine-tune is required\n",
      "------------------------------------------------------------\n",
      "[Conv1dAs2d] k=1 → single Conv2d, pad=(0, 0), runs on NPU\n",
      "------------------------------------------------------------\n",
      "[Conv1dAs2d]  ├─ stage 0: ks=9, pad=4\n",
      "[Conv1dAs2d]  ├─ stage 1: ks=9, pad=4\n",
      "[Conv1dAs2d]  ├─ stage 2: ks=9, pad=4\n",
      "[Conv1dAs2d]  ├─ stage 3: ks=9, pad=4\n",
      "[Conv1dAs2d]  ├─ stage 4: ks=9, pad=4\n",
      "[Conv1dAs2d]  ├─ stage 5: ks=9, pad=4\n",
      "[Conv1dAs2d]  ├─ stage 6: ks=9, pad=4\n",
      "[Conv1dAs2d]  ├─ stage 7: ks=3, pad=1\n",
      "[Conv1dAs2d] k=59 split into [9, 9, 9, 9, 9, 9, 9, 3] (cascade runs on NPU)\n",
      "           ⚠ weights not copied — fine-tune is required\n",
      "------------------------------------------------------------\n",
      "[Conv1dAs2d] k=1 → single Conv2d, pad=(0, 0), runs on NPU\n",
      "------------------------------------------------------------\n",
      "[Conv1dAs2d] k=7 → single Conv2d, pad=(3, 0), runs on NPU\n",
      "------------------------------------------------------------\n",
      "[Conv1dAs2d] k=1 → single Conv2d, pad=(0, 0), runs on NPU\n",
      "------------------------------------------------------------\n",
      "[Conv1dAs2d]  ├─ stage 0: ks=9, pad=4\n",
      "[Conv1dAs2d]  ├─ stage 1: ks=9, pad=4\n",
      "[Conv1dAs2d]  ├─ stage 2: ks=3, pad=1\n",
      "[Conv1dAs2d] k=19 split into [9, 9, 3] (cascade runs on NPU)\n",
      "           ⚠ weights not copied — fine-tune is required\n",
      "------------------------------------------------------------\n",
      "[Conv1dAs2d] k=1 → single Conv2d, pad=(0, 0), runs on NPU\n",
      "------------------------------------------------------------\n",
      "[Conv1dAs2d]  ├─ stage 0: ks=9, pad=4\n",
      "[Conv1dAs2d]  ├─ stage 1: ks=9, pad=4\n",
      "[Conv1dAs2d]  ├─ stage 2: ks=9, pad=4\n",
      "[Conv1dAs2d]  ├─ stage 3: ks=7, pad=3\n",
      "[Conv1dAs2d] k=31 split into [9, 9, 9, 7] (cascade runs on NPU)\n",
      "           ⚠ weights not copied — fine-tune is required\n",
      "------------------------------------------------------------\n",
      "[Conv1dAs2d] k=1 → single Conv2d, pad=(0, 0), runs on NPU\n",
      "------------------------------------------------------------\n",
      "[Conv1dAs2d]  ├─ stage 0: ks=9, pad=4\n",
      "[Conv1dAs2d]  ├─ stage 1: ks=9, pad=4\n",
      "[Conv1dAs2d]  ├─ stage 2: ks=9, pad=4\n",
      "[Conv1dAs2d]  ├─ stage 3: ks=9, pad=4\n",
      "[Conv1dAs2d]  ├─ stage 4: ks=9, pad=4\n",
      "[Conv1dAs2d]  ├─ stage 5: ks=9, pad=4\n",
      "[Conv1dAs2d]  ├─ stage 6: ks=9, pad=4\n",
      "[Conv1dAs2d]  ├─ stage 7: ks=3, pad=1\n",
      "[Conv1dAs2d] k=59 split into [9, 9, 9, 9, 9, 9, 9, 3] (cascade runs on NPU)\n",
      "           ⚠ weights not copied — fine-tune is required\n",
      "------------------------------------------------------------\n",
      "[Conv1dAs2d] k=1 → single Conv2d, pad=(0, 0), runs on NPU\n",
      "------------------------------------------------------------\n",
      "[Conv1dAs2d] k=7 → single Conv2d, pad=(3, 0), runs on NPU\n",
      "------------------------------------------------------------\n",
      "[Conv1dAs2d] k=1 → single Conv2d, pad=(0, 0), runs on NPU\n",
      "------------------------------------------------------------\n",
      "[Conv1dAs2d]  ├─ stage 0: ks=9, pad=4\n",
      "[Conv1dAs2d]  ├─ stage 1: ks=9, pad=4\n",
      "[Conv1dAs2d]  ├─ stage 2: ks=3, pad=1\n",
      "[Conv1dAs2d] k=19 split into [9, 9, 3] (cascade runs on NPU)\n",
      "           ⚠ weights not copied — fine-tune is required\n",
      "------------------------------------------------------------\n",
      "[Conv1dAs2d] k=1 → single Conv2d, pad=(0, 0), runs on NPU\n",
      "------------------------------------------------------------\n",
      "[Conv1dAs2d]  ├─ stage 0: ks=9, pad=4\n",
      "[Conv1dAs2d]  ├─ stage 1: ks=9, pad=4\n",
      "[Conv1dAs2d]  ├─ stage 2: ks=9, pad=4\n",
      "[Conv1dAs2d]  ├─ stage 3: ks=7, pad=3\n",
      "[Conv1dAs2d] k=31 split into [9, 9, 9, 7] (cascade runs on NPU)\n",
      "           ⚠ weights not copied — fine-tune is required\n",
      "------------------------------------------------------------\n",
      "[Conv1dAs2d] k=1 → single Conv2d, pad=(0, 0), runs on NPU\n",
      "------------------------------------------------------------\n",
      "[Conv1dAs2d]  ├─ stage 0: ks=9, pad=4\n",
      "[Conv1dAs2d]  ├─ stage 1: ks=9, pad=4\n",
      "[Conv1dAs2d]  ├─ stage 2: ks=9, pad=4\n",
      "[Conv1dAs2d]  ├─ stage 3: ks=9, pad=4\n",
      "[Conv1dAs2d]  ├─ stage 4: ks=9, pad=4\n",
      "[Conv1dAs2d]  ├─ stage 5: ks=9, pad=4\n",
      "[Conv1dAs2d]  ├─ stage 6: ks=9, pad=4\n",
      "[Conv1dAs2d]  ├─ stage 7: ks=3, pad=1\n",
      "[Conv1dAs2d] k=59 split into [9, 9, 9, 9, 9, 9, 9, 3] (cascade runs on NPU)\n",
      "           ⚠ weights not copied — fine-tune is required\n",
      "------------------------------------------------------------\n",
      "[Conv1dAs2d] k=1 → single Conv2d, pad=(0, 0), runs on NPU\n",
      "------------------------------------------------------------\n",
      "[Conv1dAs2d] k=7 → single Conv2d, pad=(3, 0), runs on NPU\n",
      "------------------------------------------------------------\n",
      "[Conv1dAs2d] k=1 → single Conv2d, pad=(0, 0), runs on NPU\n",
      "------------------------------------------------------------\n",
      "[Conv1dAs2d]  ├─ stage 0: ks=9, pad=4\n",
      "[Conv1dAs2d]  ├─ stage 1: ks=9, pad=4\n",
      "[Conv1dAs2d]  ├─ stage 2: ks=3, pad=1\n",
      "[Conv1dAs2d] k=19 split into [9, 9, 3] (cascade runs on NPU)\n",
      "           ⚠ weights not copied — fine-tune is required\n",
      "------------------------------------------------------------\n",
      "[Conv1dAs2d] k=1 → single Conv2d, pad=(0, 0), runs on NPU\n",
      "------------------------------------------------------------\n",
      "[Conv1dAs2d]  ├─ stage 0: ks=9, pad=4\n",
      "[Conv1dAs2d]  ├─ stage 1: ks=9, pad=4\n",
      "[Conv1dAs2d]  ├─ stage 2: ks=9, pad=4\n",
      "[Conv1dAs2d]  ├─ stage 3: ks=7, pad=3\n",
      "[Conv1dAs2d] k=31 split into [9, 9, 9, 7] (cascade runs on NPU)\n",
      "           ⚠ weights not copied — fine-tune is required\n",
      "------------------------------------------------------------\n",
      "[Conv1dAs2d] k=1 → single Conv2d, pad=(0, 0), runs on NPU\n",
      "------------------------------------------------------------\n",
      "[Conv1dAs2d]  ├─ stage 0: ks=9, pad=4\n",
      "[Conv1dAs2d]  ├─ stage 1: ks=9, pad=4\n",
      "[Conv1dAs2d]  ├─ stage 2: ks=9, pad=4\n",
      "[Conv1dAs2d]  ├─ stage 3: ks=9, pad=4\n",
      "[Conv1dAs2d]  ├─ stage 4: ks=9, pad=4\n",
      "[Conv1dAs2d]  ├─ stage 5: ks=9, pad=4\n",
      "[Conv1dAs2d]  ├─ stage 6: ks=9, pad=4\n",
      "[Conv1dAs2d]  ├─ stage 7: ks=3, pad=1\n",
      "[Conv1dAs2d] k=59 split into [9, 9, 9, 9, 9, 9, 9, 3] (cascade runs on NPU)\n",
      "           ⚠ weights not copied — fine-tune is required\n",
      "------------------------------------------------------------\n",
      "[Conv1dAs2d] k=1 → single Conv2d, pad=(0, 0), runs on NPU\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "########################################\n",
    "# 2) Define a Model Class without MelBanks\n",
    "########################################\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ReDimNetNoMel(nn.Module):\n",
    "    \"\"\"\n",
    "    A wrapper around the original ReDimNetWrap that:\n",
    "      - Excludes the 'spec' (MelBanks) module\n",
    "      - Uses 'backbone', 'pool', 'bn', and 'linear'\n",
    "    We expect a precomputed mel spectrogram as input with shape [B, 1, n_mels, time_frames].\n",
    "    \"\"\"\n",
    "    def __init__(self, original_wrap):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Grab references to the submodules we want to keep\n",
    "        self.backbone = original_wrap.backbone\n",
    "        \n",
    "        # fix problem01\n",
    "        # list of (stage, block) indices you already know are problematic\n",
    "        TARGETS = [(0, 6), (1, 6), (2, 7), (3, 8), (4, 8) , (5, 8)]\n",
    "        \n",
    "        for s_idx, b_idx in TARGETS:\n",
    "            for tcm_idx in range(4):\n",
    "                block = self.backbone.__getattr__(f\"stage{s_idx}\")[b_idx].tcm[tcm_idx]\n",
    "\n",
    "                # # test ident\n",
    "                # block.dwconvs[0] = nn.Identity()\n",
    "                # block.pwconv1    = nn.Identity()\n",
    "\n",
    "                ## original fix\n",
    "                # block.dwconvs[0] = Conv1dAs2d(block.dwconvs[0])\n",
    "                # block.pwconv1    = Conv1dAs2d(block.pwconv1)   # 1×1 conv\n",
    "                \n",
    "                # test with split\n",
    "                block.dwconvs[0] = Conv1dAs2d_split(block.dwconvs[0])\n",
    "                block.pwconv1    = Conv1dAs2d_split(block.pwconv1)   # 1×1 conv\n",
    "                \n",
    "                \n",
    "                \n",
    "        # #orignal before\n",
    "        # self.pool   = original_wrap.pool\n",
    "        # # ------------------------------------------------------------------\n",
    "        # # 2. patch the pool (ASTP) – its two \"linear\" Conv1d layers\n",
    "        # # ------------------------------------------------------------------\n",
    "        # self.pool.linear1 = Conv1dAs2d(self.pool.linear1)   # 1 × 1 conv 1800→128\n",
    "        # self.pool.linear2 = Conv1dAs2d(self.pool.linear2)   # 1 × 1 conv 128 →600\n",
    "\n",
    "\n",
    "        # # # --- pooling --> RKNN-safe variant ------------------------------------\n",
    "        # # replace pool with the new channel-aware version\n",
    "        # with torch.no_grad():\n",
    "        #     dummy = torch.zeros(1, 1, 72, 134)\n",
    "        #     out_channels = self.backbone(dummy).shape[1]   # 600\n",
    "        # self.pool = ASTP_RKNNSafe(original_wrap.pool, out_channels)\n",
    "        \n",
    "        \n",
    "        \n",
    "         # find backbone output channels once\n",
    "        with torch.no_grad():\n",
    "            c_backbone = self.backbone(torch.zeros(1, 1, 72, 134)).shape[1]  # 600\n",
    "        # pooling block now entirely on CPU (no Conv1dAs2d)\n",
    "        self.pool = ASTP_RKNNSafe_cpu(original_wrap.pool, c_backbone)\n",
    "\n",
    "        \n",
    "        # ---------- tail ----------\n",
    "        self.bn = original_wrap.bn\n",
    "        self.linear = original_wrap.linear\n",
    "        \n",
    "        ## Replace activations in the backbone\n",
    "        replace_activation_(self, old_cls=nn.GELU, new_cls=NewGELUActivation)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: shape [B, 1, n_mels, time_frames]\n",
    "        # (1) Pass through the backbone\n",
    "        x = self.backbone(x)    # shape might become [B, channels, frames] or similar\n",
    "        # (2) Pooling\n",
    "        x = self.pool(x)        # ASTP => shape likely [B, embedding_dim]\n",
    "        # (3) BatchNorm\n",
    "        x = self.bn(x)\n",
    "        # (4) Final linear => 192-dim (if that's your embedding size)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Create an instance of our new model that skips the MelBanks front-end\n",
    "model_no_mel = ReDimNetNoMel(original_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.7342, -0.3765,  0.1065, -2.1669,  0.4111,  0.9082, -0.3310,  1.1415,\n",
       "          1.6039,  0.0577,  0.7036, -0.9166, -1.4775, -0.8100,  0.3512, -1.4892,\n",
       "          2.5740, -1.9717,  0.2193, -1.0464,  0.5683, -0.8797,  0.1442, -2.7725,\n",
       "          1.3930, -3.4797,  1.2009,  0.8568, -2.2360,  0.7037,  1.4462,  0.9154,\n",
       "         -0.6945, -0.6774,  0.0602, -0.0771, -0.4021, -1.9023,  1.0491, -1.7664,\n",
       "          0.2396, -0.3444, -3.8919, -1.0772,  0.4447, -0.6326,  0.2721,  0.4667,\n",
       "         -1.7621, -1.3656,  2.1652,  2.7714,  1.3198, -1.8804,  0.0269,  2.1040,\n",
       "         -0.2017,  1.0771,  1.5558,  0.0683, -0.7983,  1.2724,  2.2012,  0.5660,\n",
       "         -2.4000,  0.5101, -0.6607,  2.3502, -0.1457, -1.7068, -0.9537, -0.6582,\n",
       "         -2.0329,  1.4163,  1.3181, -1.0567, -0.0096,  0.7868,  3.0657, -1.1313,\n",
       "          1.6675, -0.7146,  0.8327,  2.5079,  0.4475, -1.4926,  0.7370, -1.2295,\n",
       "         -0.5195, -0.0219,  1.7682,  1.3294, -0.5187,  0.1664, -0.2296, -1.9450,\n",
       "         -0.6819, -2.1093, -1.0391,  0.3794,  0.0801, -1.0070, -0.3550, -0.7180,\n",
       "          1.5232,  1.9601, -1.0796,  1.4092, -0.2120, -1.5870, -0.2201,  1.0883,\n",
       "         -1.1489, -1.2086, -1.0229,  1.7616,  0.6955, -0.8572, -3.2987,  0.3327,\n",
       "          2.0324,  1.1141, -2.8918,  0.1826, -1.5908, -1.2931, -0.9198, -0.7867,\n",
       "         -0.5653, -0.5182, -1.3793, -0.1199, -1.3746,  1.9947,  0.9216,  0.1225,\n",
       "          0.3082,  0.6524, -1.5248,  2.1617,  0.1305,  3.0826, -1.4634,  0.5446,\n",
       "          0.8405,  0.5633, -0.5499, -0.3396, -1.1017, -0.9032,  0.9512, -0.1330,\n",
       "         -1.9567,  1.0763, -0.0553, -0.9598, -0.4670, -0.0736, -1.5771,  0.3620,\n",
       "          0.2016,  0.6330,  0.1002,  0.0477, -1.4069,  0.8260, -1.3011, -2.9745,\n",
       "          1.5338, -1.3028, -0.9414, -0.8468,  0.2296,  0.3900, -2.0946,  0.7144,\n",
       "         -0.1423,  0.7452, -0.4293, -0.4210,  0.1808,  0.2778,  0.3857,  0.0737,\n",
       "         -1.1068,  0.3007, -1.4893, -0.1829, -2.7878,  1.8405,  1.3273, -1.3061]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_no_mel.eval()  # <- this line is critical!\n",
    "dummy = torch.randn(1, 1, 72, 134)\n",
    "model_no_mel(dummy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PADS?\n",
    "\n",
    "\n",
    "Conv1dAs2d deliberately switches to padding='same' whenever\n",
    "pad_num (= (k-1)//2) > max_pad (currently 4).\n",
    "PyTorch → ONNX keeps that as the auto_pad attribute, and RKNN reacts by\n",
    "materialising an explicit Pad node that it then assigns to the CPU,\n",
    "which breaks compilation.\n",
    "\n",
    "Hardware can handle large numeric pads – it merely dislikes auto_pad.\n",
    "\n",
    "```\n",
    "       (dwconvs): ModuleList(\n",
    "              (0): Conv1dAs2d(\n",
    "                (conv2d): Conv2d(20, 20, kernel_size=(59, 1), stride=(1, 1), padding=same, groups=20)\n",
    "              )\n",
    "            )\n",
    "            (norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "            (act): NewGELUActivation()\n",
    "            (pwconv1): Conv1dAs2d(\n",
    "              (conv2d): Conv2d(20, 20, kernel_size=(1, 1), stride=(1, 1))\n",
    "            )\n",
    "\n",
    "```\n",
    "\n",
    " unsupport cpu Pad op, op name: Pad:/backbone/stage0/stage0.6/tcm/tcm.1/dwconvs.0/Unsqueeze_output_0_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Conv2d layers with padding mode \"same\" ===\n",
      "  backbone.stem.0                                               k= 3  d= 1  mode=\"same\"\n",
      "  backbone.stage0.3.conv_block.dwconvs.0                        k= 3  d= 1  mode=\"same\"\n",
      "  backbone.stage0.4.conv_block.dwconvs.0                        k= 3  d= 1  mode=\"same\"\n",
      "  backbone.stage1.3.conv_block.dwconvs.0                        k= 3  d= 1  mode=\"same\"\n",
      "  backbone.stage1.4.conv_block.dwconvs.0                        k= 3  d= 1  mode=\"same\"\n",
      "  backbone.stage2.3.conv_block.dwconvs.0                        k= 3  d= 1  mode=\"same\"\n",
      "  backbone.stage2.4.conv_block.dwconvs.0                        k= 3  d= 1  mode=\"same\"\n",
      "  backbone.stage2.5.conv_block.dwconvs.0                        k= 3  d= 1  mode=\"same\"\n",
      "  backbone.stage3.3.conv_block.dwconvs.0                        k= 3  d= 1  mode=\"same\"\n",
      "  backbone.stage3.4.conv_block.dwconvs.0                        k= 3  d= 1  mode=\"same\"\n",
      "  backbone.stage3.5.conv_block.dwconvs.0                        k= 3  d= 1  mode=\"same\"\n",
      "  backbone.stage3.6.conv_block.dwconvs.0                        k= 3  d= 1  mode=\"same\"\n",
      "  backbone.stage4.3.conv_block.dwconvs.0                        k= 3  d= 1  mode=\"same\"\n",
      "  backbone.stage4.4.conv_block.dwconvs.0                        k= 3  d= 1  mode=\"same\"\n",
      "  backbone.stage4.5.conv_block.dwconvs.0                        k= 3  d= 1  mode=\"same\"\n",
      "  backbone.stage4.6.conv_block.dwconvs.0                        k= 3  d= 1  mode=\"same\"\n",
      "  backbone.stage5.3.conv_block.dwconvs.0                        k= 3  d= 1  mode=\"same\"\n",
      "  backbone.stage5.4.conv_block.dwconvs.0                        k= 3  d= 1  mode=\"same\"\n",
      "  backbone.stage5.5.conv_block.dwconvs.0                        k= 3  d= 1  mode=\"same\"\n",
      "  backbone.stage5.6.conv_block.dwconvs.0                        k= 3  d= 1  mode=\"same\"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "# 4. Inside PyTorch: list every Conv2d that still uses \"padding=\\'same\\'\"\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "print('\\n=== Conv2d layers with padding mode \"same\" ===')\n",
    "for name, mod in model_no_mel.named_modules():\n",
    "    if isinstance(mod, torch.nn.Conv2d) and isinstance(mod.padding, str):\n",
    "        k, _ = mod.kernel_size\n",
    "        d, _ = mod.dilation\n",
    "        print(f'  {name:60s}  k={k:2d}  d={d:2d}  mode=\"same\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SIZE?\n",
    "\n",
    "For stride = 1 the silicon docs spell it out:\n",
    "\n",
    "    “The minimum supported kernel size is 1 and the maximum is 11 × stride – 1.”\n",
    "    dl.radxa.com\n",
    "\n",
    "With stride = 1 the ceiling is 10. Anything wider forces RKNN to split the layer into a CPU-side Pad ➜ Conv pair, and the runtime you’re using still lacks a CPU implementation of Pad, so the build fails even though the Conv itself would happily run on the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All Conv2d layers are within RKNN limits.\n"
     ]
    }
   ],
   "source": [
    "LIMIT = 10         # 11*stride - 1 with stride = 1\n",
    "\n",
    "bad = []\n",
    "for name, m in model_no_mel.named_modules():\n",
    "    if isinstance(m, torch.nn.Conv2d):\n",
    "        k, _ = m.kernel_size\n",
    "        pad, _ = (m.padding if not isinstance(m.padding, str)\n",
    "                   else (floor((k-1)/2), 0))          # same-padding case\n",
    "        if k > LIMIT or pad > 4:\n",
    "            bad.append((name, k, pad))\n",
    "\n",
    "if not bad:\n",
    "    print(\"✓ All Conv2d layers are within RKNN limits.\")\n",
    "else:\n",
    "    print(\"✗ Layers that violate RKNN limits:\")\n",
    "    for n,k,p in bad:\n",
    "        print(f\"   {n:60s}  k={k}  pad={p}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FP16 check\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "safe in pure FP16? tensor(True)\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    fp16_net = copy.deepcopy(model_no_mel).half().eval()\n",
    "    ok = torch.isfinite(fp16_net(dummy.half())).all()\n",
    "    print('safe in pure FP16?', ok)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_activations(model_no_mel)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ReDimNetNoMel(\n",
       "  (backbone): ReDimNet(\n",
       "    (stem): Sequential(\n",
       "      (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "      (1): LayerNorm(C=(16,), data_format=channels_first, eps=1e-06)\n",
       "      (2): to1d()\n",
       "    )\n",
       "    (stage0): Sequential(\n",
       "      (0): weigth1d(w=(1, 1, 1, 1),sequential=False)\n",
       "      (1): to2d(f=72,c=16)\n",
       "      (2): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (3): ConvBlock2d(\n",
       "        (conv_block): ConvNeXtLikeBlock(\n",
       "          (dwconvs): ModuleList(\n",
       "            (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=same, groups=4)\n",
       "          )\n",
       "          (norm): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (pwconv1): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (4): ConvBlock2d(\n",
       "        (conv_block): ConvNeXtLikeBlock(\n",
       "          (dwconvs): ModuleList(\n",
       "            (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=same, groups=4)\n",
       "          )\n",
       "          (norm): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (pwconv1): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (5): to1d()\n",
       "      (6): TimeContextBlock1d(\n",
       "        (red_dim_conv): Sequential(\n",
       "          (0): Conv1d(1152, 96, kernel_size=(1,), stride=(1,))\n",
       "          (1): LayerNorm(C=(96,), data_format=channels_first, eps=1e-06)\n",
       "        )\n",
       "        (tcm): Sequential(\n",
       "          (0): ConvNeXtLikeBlock(\n",
       "            (dwconvs): ModuleList(\n",
       "              (0): Conv1dAs2d_split(\n",
       "                (conv): Conv2d(96, 96, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), groups=96)\n",
       "              )\n",
       "            )\n",
       "            (norm): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act): NewGELUActivation()\n",
       "            (pwconv1): Conv1dAs2d_split(\n",
       "              (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "          )\n",
       "          (1): ConvNeXtLikeBlock(\n",
       "            (dwconvs): ModuleList(\n",
       "              (0): Conv1dAs2d_split(\n",
       "                (conv): Sequential(\n",
       "                  (0): Conv2d(96, 96, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), groups=96, bias=False)\n",
       "                  (1): Conv2d(96, 96, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), groups=96, bias=False)\n",
       "                  (2): Conv2d(96, 96, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), groups=96, bias=False)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (norm): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act): NewGELUActivation()\n",
       "            (pwconv1): Conv1dAs2d_split(\n",
       "              (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "          )\n",
       "          (2): ConvNeXtLikeBlock(\n",
       "            (dwconvs): ModuleList(\n",
       "              (0): Conv1dAs2d_split(\n",
       "                (conv): Sequential(\n",
       "                  (0): Conv2d(96, 96, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), groups=96, bias=False)\n",
       "                  (1): Conv2d(96, 96, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), groups=96, bias=False)\n",
       "                  (2): Conv2d(96, 96, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), groups=96, bias=False)\n",
       "                  (3): Conv2d(96, 96, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), groups=96, bias=False)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (norm): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act): NewGELUActivation()\n",
       "            (pwconv1): Conv1dAs2d_split(\n",
       "              (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "          )\n",
       "          (3): ConvNeXtLikeBlock(\n",
       "            (dwconvs): ModuleList(\n",
       "              (0): Conv1dAs2d_split(\n",
       "                (conv): Sequential(\n",
       "                  (0): Conv2d(96, 96, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), groups=96, bias=False)\n",
       "                  (1): Conv2d(96, 96, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), groups=96, bias=False)\n",
       "                  (2): Conv2d(96, 96, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), groups=96, bias=False)\n",
       "                  (3): Conv2d(96, 96, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), groups=96, bias=False)\n",
       "                  (4): Conv2d(96, 96, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), groups=96, bias=False)\n",
       "                  (5): Conv2d(96, 96, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), groups=96, bias=False)\n",
       "                  (6): Conv2d(96, 96, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), groups=96, bias=False)\n",
       "                  (7): Conv2d(96, 96, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), groups=96, bias=False)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (norm): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act): NewGELUActivation()\n",
       "            (pwconv1): Conv1dAs2d_split(\n",
       "              (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "          )\n",
       "          (4): TransformerEncoderLayer(\n",
       "            (attention): MultiHeadAttention(\n",
       "              (k_proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "              (v_proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "              (q_proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "              (out_proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "            )\n",
       "            (layer_norm): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
       "            (feed_forward): FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.0, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=96, out_features=96, bias=True)\n",
       "              (intermediate_act_fn): NewGELUActivation()\n",
       "              (output_dense): Linear(in_features=96, out_features=96, bias=True)\n",
       "              (output_dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (exp_dim_conv): Conv1d(96, 1152, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "    )\n",
       "    (stage1): Sequential(\n",
       "      (0): weigth1d(w=(1, 2, 1152, 1),sequential=False)\n",
       "      (1): to2d(f=72,c=16)\n",
       "      (2): Conv2d(16, 32, kernel_size=(2, 1), stride=(2, 1))\n",
       "      (3): ConvBlock2d(\n",
       "        (conv_block): ConvNeXtLikeBlock(\n",
       "          (dwconvs): ModuleList(\n",
       "            (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=same, groups=8)\n",
       "          )\n",
       "          (norm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (pwconv1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (4): ConvBlock2d(\n",
       "        (conv_block): ConvNeXtLikeBlock(\n",
       "          (dwconvs): ModuleList(\n",
       "            (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=same, groups=8)\n",
       "          )\n",
       "          (norm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (pwconv1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (5): to1d()\n",
       "      (6): TimeContextBlock1d(\n",
       "        (red_dim_conv): Sequential(\n",
       "          (0): Conv1d(1152, 96, kernel_size=(1,), stride=(1,))\n",
       "          (1): LayerNorm(C=(96,), data_format=channels_first, eps=1e-06)\n",
       "        )\n",
       "        (tcm): Sequential(\n",
       "          (0): ConvNeXtLikeBlock(\n",
       "            (dwconvs): ModuleList(\n",
       "              (0): Conv1dAs2d_split(\n",
       "                (conv): Conv2d(96, 96, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), groups=96)\n",
       "              )\n",
       "            )\n",
       "            (norm): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act): NewGELUActivation()\n",
       "            (pwconv1): Conv1dAs2d_split(\n",
       "              (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "          )\n",
       "          (1): ConvNeXtLikeBlock(\n",
       "            (dwconvs): ModuleList(\n",
       "              (0): Conv1dAs2d_split(\n",
       "                (conv): Sequential(\n",
       "                  (0): Conv2d(96, 96, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), groups=96, bias=False)\n",
       "                  (1): Conv2d(96, 96, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), groups=96, bias=False)\n",
       "                  (2): Conv2d(96, 96, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), groups=96, bias=False)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (norm): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act): NewGELUActivation()\n",
       "            (pwconv1): Conv1dAs2d_split(\n",
       "              (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "          )\n",
       "          (2): ConvNeXtLikeBlock(\n",
       "            (dwconvs): ModuleList(\n",
       "              (0): Conv1dAs2d_split(\n",
       "                (conv): Sequential(\n",
       "                  (0): Conv2d(96, 96, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), groups=96, bias=False)\n",
       "                  (1): Conv2d(96, 96, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), groups=96, bias=False)\n",
       "                  (2): Conv2d(96, 96, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), groups=96, bias=False)\n",
       "                  (3): Conv2d(96, 96, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), groups=96, bias=False)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (norm): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act): NewGELUActivation()\n",
       "            (pwconv1): Conv1dAs2d_split(\n",
       "              (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "          )\n",
       "          (3): ConvNeXtLikeBlock(\n",
       "            (dwconvs): ModuleList(\n",
       "              (0): Conv1dAs2d_split(\n",
       "                (conv): Sequential(\n",
       "                  (0): Conv2d(96, 96, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), groups=96, bias=False)\n",
       "                  (1): Conv2d(96, 96, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), groups=96, bias=False)\n",
       "                  (2): Conv2d(96, 96, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), groups=96, bias=False)\n",
       "                  (3): Conv2d(96, 96, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), groups=96, bias=False)\n",
       "                  (4): Conv2d(96, 96, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), groups=96, bias=False)\n",
       "                  (5): Conv2d(96, 96, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), groups=96, bias=False)\n",
       "                  (6): Conv2d(96, 96, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), groups=96, bias=False)\n",
       "                  (7): Conv2d(96, 96, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), groups=96, bias=False)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (norm): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act): NewGELUActivation()\n",
       "            (pwconv1): Conv1dAs2d_split(\n",
       "              (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "          )\n",
       "          (4): TransformerEncoderLayer(\n",
       "            (attention): MultiHeadAttention(\n",
       "              (k_proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "              (v_proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "              (q_proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "              (out_proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "            )\n",
       "            (layer_norm): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
       "            (feed_forward): FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.0, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=96, out_features=96, bias=True)\n",
       "              (intermediate_act_fn): NewGELUActivation()\n",
       "              (output_dense): Linear(in_features=96, out_features=96, bias=True)\n",
       "              (output_dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (exp_dim_conv): Conv1d(96, 1152, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "    )\n",
       "    (stage2): Sequential(\n",
       "      (0): weigth1d(w=(1, 3, 1152, 1),sequential=False)\n",
       "      (1): to2d(f=36,c=32)\n",
       "      (2): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (3): ConvBlock2d(\n",
       "        (conv_block): ConvNeXtLikeBlock(\n",
       "          (dwconvs): ModuleList(\n",
       "            (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=same, groups=8)\n",
       "          )\n",
       "          (norm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (pwconv1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (4): ConvBlock2d(\n",
       "        (conv_block): ConvNeXtLikeBlock(\n",
       "          (dwconvs): ModuleList(\n",
       "            (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=same, groups=8)\n",
       "          )\n",
       "          (norm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (pwconv1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (5): ConvBlock2d(\n",
       "        (conv_block): ConvNeXtLikeBlock(\n",
       "          (dwconvs): ModuleList(\n",
       "            (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=same, groups=8)\n",
       "          )\n",
       "          (norm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (pwconv1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (6): to1d()\n",
       "      (7): TimeContextBlock1d(\n",
       "        (red_dim_conv): Sequential(\n",
       "          (0): Conv1d(1152, 96, kernel_size=(1,), stride=(1,))\n",
       "          (1): LayerNorm(C=(96,), data_format=channels_first, eps=1e-06)\n",
       "        )\n",
       "        (tcm): Sequential(\n",
       "          (0): ConvNeXtLikeBlock(\n",
       "            (dwconvs): ModuleList(\n",
       "              (0): Conv1dAs2d_split(\n",
       "                (conv): Conv2d(96, 96, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), groups=96)\n",
       "              )\n",
       "            )\n",
       "            (norm): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act): NewGELUActivation()\n",
       "            (pwconv1): Conv1dAs2d_split(\n",
       "              (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "          )\n",
       "          (1): ConvNeXtLikeBlock(\n",
       "            (dwconvs): ModuleList(\n",
       "              (0): Conv1dAs2d_split(\n",
       "                (conv): Sequential(\n",
       "                  (0): Conv2d(96, 96, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), groups=96, bias=False)\n",
       "                  (1): Conv2d(96, 96, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), groups=96, bias=False)\n",
       "                  (2): Conv2d(96, 96, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), groups=96, bias=False)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (norm): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act): NewGELUActivation()\n",
       "            (pwconv1): Conv1dAs2d_split(\n",
       "              (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "          )\n",
       "          (2): ConvNeXtLikeBlock(\n",
       "            (dwconvs): ModuleList(\n",
       "              (0): Conv1dAs2d_split(\n",
       "                (conv): Sequential(\n",
       "                  (0): Conv2d(96, 96, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), groups=96, bias=False)\n",
       "                  (1): Conv2d(96, 96, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), groups=96, bias=False)\n",
       "                  (2): Conv2d(96, 96, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), groups=96, bias=False)\n",
       "                  (3): Conv2d(96, 96, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), groups=96, bias=False)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (norm): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act): NewGELUActivation()\n",
       "            (pwconv1): Conv1dAs2d_split(\n",
       "              (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "          )\n",
       "          (3): ConvNeXtLikeBlock(\n",
       "            (dwconvs): ModuleList(\n",
       "              (0): Conv1dAs2d_split(\n",
       "                (conv): Sequential(\n",
       "                  (0): Conv2d(96, 96, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), groups=96, bias=False)\n",
       "                  (1): Conv2d(96, 96, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), groups=96, bias=False)\n",
       "                  (2): Conv2d(96, 96, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), groups=96, bias=False)\n",
       "                  (3): Conv2d(96, 96, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), groups=96, bias=False)\n",
       "                  (4): Conv2d(96, 96, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), groups=96, bias=False)\n",
       "                  (5): Conv2d(96, 96, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), groups=96, bias=False)\n",
       "                  (6): Conv2d(96, 96, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), groups=96, bias=False)\n",
       "                  (7): Conv2d(96, 96, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), groups=96, bias=False)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (norm): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act): NewGELUActivation()\n",
       "            (pwconv1): Conv1dAs2d_split(\n",
       "              (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "          )\n",
       "          (4): TransformerEncoderLayer(\n",
       "            (attention): MultiHeadAttention(\n",
       "              (k_proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "              (v_proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "              (q_proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "              (out_proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "            )\n",
       "            (layer_norm): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
       "            (feed_forward): FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.0, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=96, out_features=96, bias=True)\n",
       "              (intermediate_act_fn): NewGELUActivation()\n",
       "              (output_dense): Linear(in_features=96, out_features=96, bias=True)\n",
       "              (output_dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (exp_dim_conv): Conv1d(96, 1152, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "    )\n",
       "    (stage3): Sequential(\n",
       "      (0): weigth1d(w=(1, 4, 1152, 1),sequential=False)\n",
       "      (1): to2d(f=36,c=32)\n",
       "      (2): Conv2d(32, 96, kernel_size=(3, 1), stride=(3, 1))\n",
       "      (3): ConvBlock2d(\n",
       "        (conv_block): ConvNeXtLikeBlock(\n",
       "          (dwconvs): ModuleList(\n",
       "            (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=same, groups=24)\n",
       "          )\n",
       "          (norm): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (pwconv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (4): ConvBlock2d(\n",
       "        (conv_block): ConvNeXtLikeBlock(\n",
       "          (dwconvs): ModuleList(\n",
       "            (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=same, groups=24)\n",
       "          )\n",
       "          (norm): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (pwconv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (5): ConvBlock2d(\n",
       "        (conv_block): ConvNeXtLikeBlock(\n",
       "          (dwconvs): ModuleList(\n",
       "            (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=same, groups=24)\n",
       "          )\n",
       "          (norm): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (pwconv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (6): ConvBlock2d(\n",
       "        (conv_block): ConvNeXtLikeBlock(\n",
       "          (dwconvs): ModuleList(\n",
       "            (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=same, groups=24)\n",
       "          )\n",
       "          (norm): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (pwconv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (7): to1d()\n",
       "      (8): TimeContextBlock1d(\n",
       "        (red_dim_conv): Sequential(\n",
       "          (0): Conv1d(1152, 144, kernel_size=(1,), stride=(1,))\n",
       "          (1): LayerNorm(C=(144,), data_format=channels_first, eps=1e-06)\n",
       "        )\n",
       "        (tcm): Sequential(\n",
       "          (0): ConvNeXtLikeBlock(\n",
       "            (dwconvs): ModuleList(\n",
       "              (0): Conv1dAs2d_split(\n",
       "                (conv): Conv2d(144, 144, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), groups=144)\n",
       "              )\n",
       "            )\n",
       "            (norm): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act): NewGELUActivation()\n",
       "            (pwconv1): Conv1dAs2d_split(\n",
       "              (conv): Conv2d(144, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "          )\n",
       "          (1): ConvNeXtLikeBlock(\n",
       "            (dwconvs): ModuleList(\n",
       "              (0): Conv1dAs2d_split(\n",
       "                (conv): Sequential(\n",
       "                  (0): Conv2d(144, 144, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), groups=144, bias=False)\n",
       "                  (1): Conv2d(144, 144, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), groups=144, bias=False)\n",
       "                  (2): Conv2d(144, 144, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), groups=144, bias=False)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (norm): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act): NewGELUActivation()\n",
       "            (pwconv1): Conv1dAs2d_split(\n",
       "              (conv): Conv2d(144, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "          )\n",
       "          (2): ConvNeXtLikeBlock(\n",
       "            (dwconvs): ModuleList(\n",
       "              (0): Conv1dAs2d_split(\n",
       "                (conv): Sequential(\n",
       "                  (0): Conv2d(144, 144, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), groups=144, bias=False)\n",
       "                  (1): Conv2d(144, 144, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), groups=144, bias=False)\n",
       "                  (2): Conv2d(144, 144, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), groups=144, bias=False)\n",
       "                  (3): Conv2d(144, 144, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), groups=144, bias=False)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (norm): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act): NewGELUActivation()\n",
       "            (pwconv1): Conv1dAs2d_split(\n",
       "              (conv): Conv2d(144, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "          )\n",
       "          (3): ConvNeXtLikeBlock(\n",
       "            (dwconvs): ModuleList(\n",
       "              (0): Conv1dAs2d_split(\n",
       "                (conv): Sequential(\n",
       "                  (0): Conv2d(144, 144, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), groups=144, bias=False)\n",
       "                  (1): Conv2d(144, 144, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), groups=144, bias=False)\n",
       "                  (2): Conv2d(144, 144, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), groups=144, bias=False)\n",
       "                  (3): Conv2d(144, 144, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), groups=144, bias=False)\n",
       "                  (4): Conv2d(144, 144, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), groups=144, bias=False)\n",
       "                  (5): Conv2d(144, 144, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), groups=144, bias=False)\n",
       "                  (6): Conv2d(144, 144, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), groups=144, bias=False)\n",
       "                  (7): Conv2d(144, 144, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), groups=144, bias=False)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (norm): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act): NewGELUActivation()\n",
       "            (pwconv1): Conv1dAs2d_split(\n",
       "              (conv): Conv2d(144, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "          )\n",
       "          (4): TransformerEncoderLayer(\n",
       "            (attention): MultiHeadAttention(\n",
       "              (k_proj): Linear(in_features=144, out_features=144, bias=True)\n",
       "              (v_proj): Linear(in_features=144, out_features=144, bias=True)\n",
       "              (q_proj): Linear(in_features=144, out_features=144, bias=True)\n",
       "              (out_proj): Linear(in_features=144, out_features=144, bias=True)\n",
       "            )\n",
       "            (layer_norm): LayerNorm((144,), eps=1e-06, elementwise_affine=True)\n",
       "            (feed_forward): FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.0, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=144, out_features=144, bias=True)\n",
       "              (intermediate_act_fn): NewGELUActivation()\n",
       "              (output_dense): Linear(in_features=144, out_features=144, bias=True)\n",
       "              (output_dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((144,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (exp_dim_conv): Conv1d(144, 1152, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "    )\n",
       "    (stage4): Sequential(\n",
       "      (0): weigth1d(w=(1, 5, 1152, 1),sequential=False)\n",
       "      (1): to2d(f=12,c=96)\n",
       "      (2): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (3): ConvBlock2d(\n",
       "        (conv_block): ConvNeXtLikeBlock(\n",
       "          (dwconvs): ModuleList(\n",
       "            (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=same, groups=24)\n",
       "          )\n",
       "          (norm): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (pwconv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (4): ConvBlock2d(\n",
       "        (conv_block): ConvNeXtLikeBlock(\n",
       "          (dwconvs): ModuleList(\n",
       "            (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=same, groups=24)\n",
       "          )\n",
       "          (norm): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (pwconv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (5): ConvBlock2d(\n",
       "        (conv_block): ConvNeXtLikeBlock(\n",
       "          (dwconvs): ModuleList(\n",
       "            (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=same, groups=24)\n",
       "          )\n",
       "          (norm): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (pwconv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (6): ConvBlock2d(\n",
       "        (conv_block): ConvNeXtLikeBlock(\n",
       "          (dwconvs): ModuleList(\n",
       "            (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=same, groups=24)\n",
       "          )\n",
       "          (norm): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (pwconv1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (7): to1d()\n",
       "      (8): TimeContextBlock1d(\n",
       "        (red_dim_conv): Sequential(\n",
       "          (0): Conv1d(1152, 144, kernel_size=(1,), stride=(1,))\n",
       "          (1): LayerNorm(C=(144,), data_format=channels_first, eps=1e-06)\n",
       "        )\n",
       "        (tcm): Sequential(\n",
       "          (0): ConvNeXtLikeBlock(\n",
       "            (dwconvs): ModuleList(\n",
       "              (0): Conv1dAs2d_split(\n",
       "                (conv): Conv2d(144, 144, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), groups=144)\n",
       "              )\n",
       "            )\n",
       "            (norm): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act): NewGELUActivation()\n",
       "            (pwconv1): Conv1dAs2d_split(\n",
       "              (conv): Conv2d(144, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "          )\n",
       "          (1): ConvNeXtLikeBlock(\n",
       "            (dwconvs): ModuleList(\n",
       "              (0): Conv1dAs2d_split(\n",
       "                (conv): Sequential(\n",
       "                  (0): Conv2d(144, 144, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), groups=144, bias=False)\n",
       "                  (1): Conv2d(144, 144, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), groups=144, bias=False)\n",
       "                  (2): Conv2d(144, 144, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), groups=144, bias=False)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (norm): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act): NewGELUActivation()\n",
       "            (pwconv1): Conv1dAs2d_split(\n",
       "              (conv): Conv2d(144, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "          )\n",
       "          (2): ConvNeXtLikeBlock(\n",
       "            (dwconvs): ModuleList(\n",
       "              (0): Conv1dAs2d_split(\n",
       "                (conv): Sequential(\n",
       "                  (0): Conv2d(144, 144, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), groups=144, bias=False)\n",
       "                  (1): Conv2d(144, 144, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), groups=144, bias=False)\n",
       "                  (2): Conv2d(144, 144, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), groups=144, bias=False)\n",
       "                  (3): Conv2d(144, 144, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), groups=144, bias=False)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (norm): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act): NewGELUActivation()\n",
       "            (pwconv1): Conv1dAs2d_split(\n",
       "              (conv): Conv2d(144, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "          )\n",
       "          (3): ConvNeXtLikeBlock(\n",
       "            (dwconvs): ModuleList(\n",
       "              (0): Conv1dAs2d_split(\n",
       "                (conv): Sequential(\n",
       "                  (0): Conv2d(144, 144, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), groups=144, bias=False)\n",
       "                  (1): Conv2d(144, 144, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), groups=144, bias=False)\n",
       "                  (2): Conv2d(144, 144, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), groups=144, bias=False)\n",
       "                  (3): Conv2d(144, 144, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), groups=144, bias=False)\n",
       "                  (4): Conv2d(144, 144, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), groups=144, bias=False)\n",
       "                  (5): Conv2d(144, 144, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), groups=144, bias=False)\n",
       "                  (6): Conv2d(144, 144, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), groups=144, bias=False)\n",
       "                  (7): Conv2d(144, 144, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), groups=144, bias=False)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (norm): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act): NewGELUActivation()\n",
       "            (pwconv1): Conv1dAs2d_split(\n",
       "              (conv): Conv2d(144, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "          )\n",
       "          (4): TransformerEncoderLayer(\n",
       "            (attention): MultiHeadAttention(\n",
       "              (k_proj): Linear(in_features=144, out_features=144, bias=True)\n",
       "              (v_proj): Linear(in_features=144, out_features=144, bias=True)\n",
       "              (q_proj): Linear(in_features=144, out_features=144, bias=True)\n",
       "              (out_proj): Linear(in_features=144, out_features=144, bias=True)\n",
       "            )\n",
       "            (layer_norm): LayerNorm((144,), eps=1e-06, elementwise_affine=True)\n",
       "            (feed_forward): FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.0, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=144, out_features=144, bias=True)\n",
       "              (intermediate_act_fn): NewGELUActivation()\n",
       "              (output_dense): Linear(in_features=144, out_features=144, bias=True)\n",
       "              (output_dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((144,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (exp_dim_conv): Conv1d(144, 1152, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "    )\n",
       "    (stage5): Sequential(\n",
       "      (0): weigth1d(w=(1, 6, 1152, 1),sequential=False)\n",
       "      (1): to2d(f=12,c=96)\n",
       "      (2): Conv2d(96, 192, kernel_size=(2, 1), stride=(2, 1))\n",
       "      (3): ConvBlock2d(\n",
       "        (conv_block): ConvNeXtLikeBlock(\n",
       "          (dwconvs): ModuleList(\n",
       "            (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=same, groups=48)\n",
       "          )\n",
       "          (norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (pwconv1): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (4): ConvBlock2d(\n",
       "        (conv_block): ConvNeXtLikeBlock(\n",
       "          (dwconvs): ModuleList(\n",
       "            (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=same, groups=48)\n",
       "          )\n",
       "          (norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (pwconv1): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (5): ConvBlock2d(\n",
       "        (conv_block): ConvNeXtLikeBlock(\n",
       "          (dwconvs): ModuleList(\n",
       "            (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=same, groups=48)\n",
       "          )\n",
       "          (norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (pwconv1): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (6): ConvBlock2d(\n",
       "        (conv_block): ConvNeXtLikeBlock(\n",
       "          (dwconvs): ModuleList(\n",
       "            (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=same, groups=48)\n",
       "          )\n",
       "          (norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (pwconv1): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (7): to1d()\n",
       "      (8): TimeContextBlock1d(\n",
       "        (red_dim_conv): Sequential(\n",
       "          (0): Conv1d(1152, 288, kernel_size=(1,), stride=(1,))\n",
       "          (1): LayerNorm(C=(288,), data_format=channels_first, eps=1e-06)\n",
       "        )\n",
       "        (tcm): Sequential(\n",
       "          (0): ConvNeXtLikeBlock(\n",
       "            (dwconvs): ModuleList(\n",
       "              (0): Conv1dAs2d_split(\n",
       "                (conv): Conv2d(288, 288, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), groups=288)\n",
       "              )\n",
       "            )\n",
       "            (norm): BatchNorm1d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act): NewGELUActivation()\n",
       "            (pwconv1): Conv1dAs2d_split(\n",
       "              (conv): Conv2d(288, 288, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "          )\n",
       "          (1): ConvNeXtLikeBlock(\n",
       "            (dwconvs): ModuleList(\n",
       "              (0): Conv1dAs2d_split(\n",
       "                (conv): Sequential(\n",
       "                  (0): Conv2d(288, 288, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), groups=288, bias=False)\n",
       "                  (1): Conv2d(288, 288, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), groups=288, bias=False)\n",
       "                  (2): Conv2d(288, 288, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), groups=288, bias=False)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (norm): BatchNorm1d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act): NewGELUActivation()\n",
       "            (pwconv1): Conv1dAs2d_split(\n",
       "              (conv): Conv2d(288, 288, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "          )\n",
       "          (2): ConvNeXtLikeBlock(\n",
       "            (dwconvs): ModuleList(\n",
       "              (0): Conv1dAs2d_split(\n",
       "                (conv): Sequential(\n",
       "                  (0): Conv2d(288, 288, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), groups=288, bias=False)\n",
       "                  (1): Conv2d(288, 288, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), groups=288, bias=False)\n",
       "                  (2): Conv2d(288, 288, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), groups=288, bias=False)\n",
       "                  (3): Conv2d(288, 288, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), groups=288, bias=False)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (norm): BatchNorm1d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act): NewGELUActivation()\n",
       "            (pwconv1): Conv1dAs2d_split(\n",
       "              (conv): Conv2d(288, 288, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "          )\n",
       "          (3): ConvNeXtLikeBlock(\n",
       "            (dwconvs): ModuleList(\n",
       "              (0): Conv1dAs2d_split(\n",
       "                (conv): Sequential(\n",
       "                  (0): Conv2d(288, 288, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), groups=288, bias=False)\n",
       "                  (1): Conv2d(288, 288, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), groups=288, bias=False)\n",
       "                  (2): Conv2d(288, 288, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), groups=288, bias=False)\n",
       "                  (3): Conv2d(288, 288, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), groups=288, bias=False)\n",
       "                  (4): Conv2d(288, 288, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), groups=288, bias=False)\n",
       "                  (5): Conv2d(288, 288, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), groups=288, bias=False)\n",
       "                  (6): Conv2d(288, 288, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), groups=288, bias=False)\n",
       "                  (7): Conv2d(288, 288, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), groups=288, bias=False)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (norm): BatchNorm1d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act): NewGELUActivation()\n",
       "            (pwconv1): Conv1dAs2d_split(\n",
       "              (conv): Conv2d(288, 288, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "          )\n",
       "          (4): TransformerEncoderLayer(\n",
       "            (attention): MultiHeadAttention(\n",
       "              (k_proj): Linear(in_features=288, out_features=288, bias=True)\n",
       "              (v_proj): Linear(in_features=288, out_features=288, bias=True)\n",
       "              (q_proj): Linear(in_features=288, out_features=288, bias=True)\n",
       "              (out_proj): Linear(in_features=288, out_features=288, bias=True)\n",
       "            )\n",
       "            (layer_norm): LayerNorm((288,), eps=1e-06, elementwise_affine=True)\n",
       "            (feed_forward): FeedForward(\n",
       "              (intermediate_dropout): Dropout(p=0.0, inplace=False)\n",
       "              (intermediate_dense): Linear(in_features=288, out_features=288, bias=True)\n",
       "              (intermediate_act_fn): NewGELUActivation()\n",
       "              (output_dense): Linear(in_features=288, out_features=288, bias=True)\n",
       "              (output_dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (final_layer_norm): LayerNorm((288,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (exp_dim_conv): Conv1d(288, 1152, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "    )\n",
       "    (fin_wght1d): weigth1d(w=(1, 7, 1152, 1),sequential=False)\n",
       "    (mfa): Identity()\n",
       "    (fin_to2d): Identity()\n",
       "  )\n",
       "  (pool): ASTP_RKNNSafe_cpu(\n",
       "    (linear1): Conv1d(1152, 128, kernel_size=(1,), stride=(1,))\n",
       "    (linear2): Conv1d(128, 1152, kernel_size=(1,), stride=(1,))\n",
       "  )\n",
       "  (bn): BatchNorm1d(2304, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (linear): Linear(in_features=2304, out_features=192, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_no_mel.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==============================================================================================================\n",
       "Layer (type:depth-idx)                                       Output Shape              Param #\n",
       "==============================================================================================================\n",
       "ReDimNetNoMel                                                [1, 192]                  --\n",
       "├─ReDimNet: 1-1                                              [1, 1152, 134]            --\n",
       "│    └─Sequential: 2-1                                       [1, 1152, 134]            --\n",
       "│    │    └─Conv2d: 3-1                                      [1, 16, 72, 134]          160\n",
       "│    │    └─LayerNorm: 3-2                                   [1, 16, 72, 134]          32\n",
       "│    │    └─to1d: 3-3                                        [1, 1152, 134]            --\n",
       "│    └─Sequential: 2-2                                       [1, 1152, 134]            --\n",
       "│    │    └─weigth1d: 3-4                                    [1, 1152, 134]            (1)\n",
       "│    │    └─to2d: 3-5                                        [1, 16, 72, 134]          --\n",
       "│    │    └─Conv2d: 3-6                                      [1, 16, 72, 134]          272\n",
       "│    │    └─ConvBlock2d: 3-7                                 [1, 16, 72, 134]          896\n",
       "│    │    └─ConvBlock2d: 3-8                                 [1, 16, 72, 134]          896\n",
       "│    │    └─to1d: 3-9                                        [1, 1152, 134]            --\n",
       "│    │    └─TimeContextBlock1d: 3-10                         [1, 1152, 134]            329,280\n",
       "│    └─Sequential: 2-3                                       [1, 1152, 134]            --\n",
       "│    │    └─weigth1d: 3-11                                   [1, 1152, 134]            2,304\n",
       "│    │    └─to2d: 3-12                                       [1, 16, 72, 134]          --\n",
       "│    │    └─Conv2d: 3-13                                     [1, 32, 36, 134]          1,056\n",
       "│    │    └─ConvBlock2d: 3-14                                [1, 32, 36, 134]          2,304\n",
       "│    │    └─ConvBlock2d: 3-15                                [1, 32, 36, 134]          2,304\n",
       "│    │    └─to1d: 3-16                                       [1, 1152, 134]            --\n",
       "│    │    └─TimeContextBlock1d: 3-17                         [1, 1152, 134]            329,280\n",
       "│    └─Sequential: 2-4                                       [1, 1152, 134]            --\n",
       "│    │    └─weigth1d: 3-18                                   [1, 1152, 134]            3,456\n",
       "│    │    └─to2d: 3-19                                       [1, 32, 36, 134]          --\n",
       "│    │    └─Conv2d: 3-20                                     [1, 32, 36, 134]          1,056\n",
       "│    │    └─ConvBlock2d: 3-21                                [1, 32, 36, 134]          2,304\n",
       "│    │    └─ConvBlock2d: 3-22                                [1, 32, 36, 134]          2,304\n",
       "│    │    └─ConvBlock2d: 3-23                                [1, 32, 36, 134]          2,304\n",
       "│    │    └─to1d: 3-24                                       [1, 1152, 134]            --\n",
       "│    │    └─TimeContextBlock1d: 3-25                         [1, 1152, 134]            329,280\n",
       "│    └─Sequential: 2-5                                       [1, 1152, 134]            --\n",
       "│    │    └─weigth1d: 3-26                                   [1, 1152, 134]            4,608\n",
       "│    │    └─to2d: 3-27                                       [1, 32, 36, 134]          --\n",
       "│    │    └─Conv2d: 3-28                                     [1, 96, 12, 134]          9,312\n",
       "│    │    └─ConvBlock2d: 3-29                                [1, 96, 12, 134]          13,056\n",
       "│    │    └─ConvBlock2d: 3-30                                [1, 96, 12, 134]          13,056\n",
       "│    │    └─ConvBlock2d: 3-31                                [1, 96, 12, 134]          13,056\n",
       "│    │    └─ConvBlock2d: 3-32                                [1, 96, 12, 134]          13,056\n",
       "│    │    └─to1d: 3-33                                       [1, 1152, 134]            --\n",
       "│    │    └─TimeContextBlock1d: 3-34                         [1, 1152, 134]            562,464\n",
       "│    └─Sequential: 2-6                                       [1, 1152, 134]            --\n",
       "│    │    └─weigth1d: 3-35                                   [1, 1152, 134]            5,760\n",
       "│    │    └─to2d: 3-36                                       [1, 96, 12, 134]          --\n",
       "│    │    └─Conv2d: 3-37                                     [1, 96, 12, 134]          9,312\n",
       "│    │    └─ConvBlock2d: 3-38                                [1, 96, 12, 134]          13,056\n",
       "│    │    └─ConvBlock2d: 3-39                                [1, 96, 12, 134]          13,056\n",
       "│    │    └─ConvBlock2d: 3-40                                [1, 96, 12, 134]          13,056\n",
       "│    │    └─ConvBlock2d: 3-41                                [1, 96, 12, 134]          13,056\n",
       "│    │    └─to1d: 3-42                                       [1, 1152, 134]            --\n",
       "│    │    └─TimeContextBlock1d: 3-43                         [1, 1152, 134]            562,464\n",
       "│    └─Sequential: 2-7                                       [1, 1152, 134]            --\n",
       "│    │    └─weigth1d: 3-44                                   [1, 1152, 134]            6,912\n",
       "│    │    └─to2d: 3-45                                       [1, 96, 12, 134]          --\n",
       "│    │    └─Conv2d: 3-46                                     [1, 192, 6, 134]          37,056\n",
       "│    │    └─ConvBlock2d: 3-47                                [1, 192, 6, 134]          44,544\n",
       "│    │    └─ConvBlock2d: 3-48                                [1, 192, 6, 134]          44,544\n",
       "│    │    └─ConvBlock2d: 3-49                                [1, 192, 6, 134]          44,544\n",
       "│    │    └─ConvBlock2d: 3-50                                [1, 192, 6, 134]          44,544\n",
       "│    │    └─to1d: 3-51                                       [1, 1152, 134]            --\n",
       "│    │    └─TimeContextBlock1d: 3-52                         [1, 1152, 134]            1,538,496\n",
       "│    └─weigth1d: 2-8                                         [1, 1152, 134]            8,064\n",
       "│    └─Identity: 2-9                                         [1, 1152, 134]            --\n",
       "│    └─Identity: 2-10                                        [1, 1152, 134]            --\n",
       "├─ASTP_RKNNSafe_cpu: 1-2                                     [1, 2304]                 --\n",
       "│    └─Conv1d: 2-11                                          [1, 128, 134]             147,584\n",
       "│    └─Conv1d: 2-12                                          [1, 1152, 134]            148,608\n",
       "├─BatchNorm1d: 1-3                                           [1, 2304]                 4,608\n",
       "├─Linear: 1-4                                                [1, 192]                  442,560\n",
       "==============================================================================================================\n",
       "Total params: 4,779,921\n",
       "Trainable params: 4,779,920\n",
       "Non-trainable params: 1\n",
       "Total mult-adds (M): 858.06\n",
       "==============================================================================================================\n",
       "Input size (MB): 0.04\n",
       "Forward/backward pass size (MB): 129.21\n",
       "Params size (MB): 19.12\n",
       "Estimated Total Size (MB): 148.37\n",
       "=============================================================================================================="
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model_no_mel, (1, 1, 72, 134))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TORCH SIDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_inference(wav_path: str):\n",
    "    # (a) Load audio\n",
    "    waveform, sample_rate = torchaudio.load(wav_path)  # shape: [channels, time]\n",
    "    # If stereo, select one channel, or average:\n",
    "    if waveform.shape[0] > 1:\n",
    "        waveform = waveform.mean(dim=0, keepdim=True)\n",
    "\n",
    "    # Resample if needed\n",
    "    target_sample_rate=16000\n",
    "    if sample_rate != target_sample_rate:\n",
    "        resampler = T.Resample(orig_freq=sample_rate, new_freq=target_sample_rate)\n",
    "        waveform = resampler(waveform)\n",
    "\n",
    "    # (b) Convert to log-mel\n",
    "    log_mel = myUtils.waveform_to_logmel(waveform)\n",
    "    print('feeding logmel shape:', log_mel.shape)\n",
    "    \n",
    "    # (c) Forward pass\n",
    "    with torch.no_grad():\n",
    "        embedding = model_no_mel(log_mel)  # shape typically [1, 192] or so\n",
    "\n",
    "    print(\"Embedding shape:\", embedding.shape)\n",
    "    #print(\"Embedding:\", embedding)\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* run test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input waveform shape: torch.Size([1, 32000])\n",
      "feeding logmel shape: torch.Size([1, 1, 72, 134])\n",
      "Embedding shape: torch.Size([1, 192])\n",
      "Input waveform shape: torch.Size([1, 25776])\n",
      "Padding log_mel from 108 to 134 frames\n",
      "feeding logmel shape: torch.Size([1, 1, 72, 134])\n",
      "Embedding shape: torch.Size([1, 192])\n",
      "Input waveform shape: torch.Size([1, 23570])\n",
      "Padding log_mel from 99 to 134 frames\n",
      "feeding logmel shape: torch.Size([1, 1, 72, 134])\n",
      "Embedding shape: torch.Size([1, 192])\n",
      "Input waveform shape: torch.Size([1, 32000])\n",
      "feeding logmel shape: torch.Size([1, 1, 72, 134])\n",
      "Embedding shape: torch.Size([1, 192])\n",
      "Input waveform shape: torch.Size([1, 32000])\n",
      "feeding logmel shape: torch.Size([1, 1, 72, 134])\n",
      "Embedding shape: torch.Size([1, 192])\n",
      "Input waveform shape: torch.Size([1, 28126])\n",
      "Padding log_mel from 118 to 134 frames\n",
      "feeding logmel shape: torch.Size([1, 1, 72, 134])\n",
      "Embedding shape: torch.Size([1, 192])\n",
      "Input waveform shape: torch.Size([1, 32000])\n",
      "feeding logmel shape: torch.Size([1, 1, 72, 134])\n",
      "Embedding shape: torch.Size([1, 192])\n",
      "**************************************************************************\n",
      "*************************   compare summary ******************************\n",
      "**************************************************************************\n",
      "====>>>> should be similar:\n",
      "Similarity (robot1 to robot2 ): 0.9347355961799622\n",
      "Similarity (human1 to human1 ): 0.8388652801513672\n",
      "Similarity (human2 to human2 ): 0.744097888469696\n",
      "====>>>> should be differnet:\n",
      "Similarity (robot to human1  ): 0.4990474581718445\n",
      "Similarity (robot to human2  ): 0.4832248091697693\n",
      "Similarity (human1 to human2 ): 0.6287989616394043\n"
     ]
    }
   ],
   "source": [
    "torch_embedding = test_all_voices(\n",
    "    extract_speaker_embedding_function = torch_inference,\n",
    "    cosine_similarity_function = myUtils.cosine_similarity\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## compare to baseline\n",
    "\n",
    "* test embedding compare of voice in the currnet model with baseline model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity embde0: 0.407304972410202\n",
      "Similarity embde1: 0.5619342923164368\n",
      "Similarity embde2: 0.6011209487915039\n",
      "Similarity embde3: 0.48210597038269043\n",
      "Similarity embde4: 0.46966877579689026\n",
      "Similarity embde5: 0.2696359157562256\n",
      "Similarity embde6: 0.35251766443252563\n"
     ]
    }
   ],
   "source": [
    "print(f\"Similarity embde0: {myUtils.cosine_similarity(base_line_embedding['embed0'], torch_embedding['embed0'])}\")\n",
    "print(f\"Similarity embde1: {myUtils.cosine_similarity(base_line_embedding['embed1'], torch_embedding['embed1'])}\")\n",
    "print(f\"Similarity embde2: {myUtils.cosine_similarity(base_line_embedding['embed2'], torch_embedding['embed2'])}\")\n",
    "print(f\"Similarity embde3: {myUtils.cosine_similarity(base_line_embedding['embed3'], torch_embedding['embed3'])}\")\n",
    "print(f\"Similarity embde4: {myUtils.cosine_similarity(base_line_embedding['embed4'], torch_embedding['embed4'])}\")\n",
    "print(f\"Similarity embde5: {myUtils.cosine_similarity(base_line_embedding['embed5'], torch_embedding['embed5'])}\")\n",
    "print(f\"Similarity embde6: {myUtils.cosine_similarity(base_line_embedding['embed6'], torch_embedding['embed6'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ONNX SIDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1176289/991109579.py:8: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  (torch.sqrt(torch.tensor(2.0 / torch.pi, device=x.device)) *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported NHWC model to ReDimNet_no_mel_nhwc.onnx\n",
      "Exported to ReDimNet_no_mel.onnx\n",
      "-rw-rw-r-- 1 vlad vlad 19M Jul  6 07:01 ReDimNet_no_mel.onnx\n"
     ]
    }
   ],
   "source": [
    "myUtils.export_to_onnx(model_no_mel,onnx_path = \"ReDimNet_no_mel.onnx\")\n",
    "!ls -lah ReDimNet_no_mel.onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### store half"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/proj/voice/pyvoice_venv/lib/python3.10/site-packages/onnxconverter_common/float16.py:53: UserWarning: the float32 number -9.017003321787342e-08 will be truncated to -1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(neg_max, -min_positive_val))\n",
      "/data/proj/voice/pyvoice_venv/lib/python3.10/site-packages/onnxconverter_common/float16.py:43: UserWarning: the float32 number 7.632494458675865e-08 will be truncated to 1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(pos_min, min_positive_val))\n",
      "/data/proj/voice/pyvoice_venv/lib/python3.10/site-packages/onnxconverter_common/float16.py:53: UserWarning: the float32 number -6.545661079826459e-08 will be truncated to -1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(neg_max, -min_positive_val))\n",
      "/data/proj/voice/pyvoice_venv/lib/python3.10/site-packages/onnxconverter_common/float16.py:53: UserWarning: the float32 number -9.70192957083782e-08 will be truncated to -1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(neg_max, -min_positive_val))\n",
      "/data/proj/voice/pyvoice_venv/lib/python3.10/site-packages/onnxconverter_common/float16.py:53: UserWarning: the float32 number -7.75507515982099e-08 will be truncated to -1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(neg_max, -min_positive_val))\n",
      "/data/proj/voice/pyvoice_venv/lib/python3.10/site-packages/onnxconverter_common/float16.py:53: UserWarning: the float32 number -1.3312886792959944e-08 will be truncated to -1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(neg_max, -min_positive_val))\n",
      "/data/proj/voice/pyvoice_venv/lib/python3.10/site-packages/onnxconverter_common/float16.py:53: UserWarning: the float32 number -1.1724716308947336e-08 will be truncated to -1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(neg_max, -min_positive_val))\n",
      "/data/proj/voice/pyvoice_venv/lib/python3.10/site-packages/onnxconverter_common/float16.py:43: UserWarning: the float32 number 1.3119730302157961e-10 will be truncated to 1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(pos_min, min_positive_val))\n",
      "/data/proj/voice/pyvoice_venv/lib/python3.10/site-packages/onnxconverter_common/float16.py:43: UserWarning: the float32 number 9.999999960041972e-13 will be truncated to 1e-07\n",
      "  warnings.warn(\"the float32 number {} will be truncated to {}\".format(pos_min, min_positive_val))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted ReDimNet_no_mel.onnx to half precision and saved as ReDimNet_no_mel_fp16.onnx\n",
      "Converted ReDimNet_no_mel_nhwc.onnx to half precision and saved as ReDimNet_no_mel_nhwc_fp16.onnx\n"
     ]
    }
   ],
   "source": [
    "myUtils.restore_in_half_precision('ReDimNet_no_mel.onnx','ReDimNet_no_mel_fp16.onnx')\n",
    "myUtils.restore_in_half_precision('ReDimNet_no_mel_nhwc.onnx','ReDimNet_no_mel_nhwc_fp16.onnx')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## verify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# onnx_path = \"ReDimNet_no_mel.onnx\"\n",
    "onnx_path = \"ReDimNet_no_mel_fp16.onnx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX model is valid!\n"
     ]
    }
   ],
   "source": [
    "onnx_model = onnx.load(onnx_path)\n",
    "onnx.checker.check_model(onnx_model)\n",
    "print(\"ONNX model is valid!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Conv nodes with auto_pad = 'SAME_UPPER' or 'SAME_LOWER' ===\n",
      "── /backbone/stem/stem.0/Conv\n",
      "   auto_pad : SAME_UPPER\n",
      "   pads     : auto\n",
      "   kernel   : k=3  (C_in=1, C_out=16)\n",
      "   input    : graph_input_cast_0\n",
      "   output   : /backbone/stem/stem.0/Conv_output_0\n",
      "── /backbone/stage0/stage0.3/conv_block/dwconvs.0/Conv\n",
      "   auto_pad : SAME_UPPER\n",
      "   pads     : auto\n",
      "   kernel   : k=3  (C_in=4, C_out=16)\n",
      "   input    : /backbone/stage0/stage0.2/Conv_output_0\n",
      "   output   : /backbone/stage0/stage0.3/conv_block/dwconvs.0/Conv_output_0\n",
      "── /backbone/stage0/stage0.4/conv_block/dwconvs.0/Conv\n",
      "   auto_pad : SAME_UPPER\n",
      "   pads     : auto\n",
      "   kernel   : k=3  (C_in=4, C_out=16)\n",
      "   input    : /backbone/stage0/stage0.3/conv_block/Add_output_0\n",
      "   output   : /backbone/stage0/stage0.4/conv_block/dwconvs.0/Conv_output_0\n",
      "── /backbone/stage1/stage1.3/conv_block/dwconvs.0/Conv\n",
      "   auto_pad : SAME_UPPER\n",
      "   pads     : auto\n",
      "   kernel   : k=3  (C_in=4, C_out=32)\n",
      "   input    : /backbone/stage1/stage1.2/Conv_output_0\n",
      "   output   : /backbone/stage1/stage1.3/conv_block/dwconvs.0/Conv_output_0\n",
      "── /backbone/stage1/stage1.4/conv_block/dwconvs.0/Conv\n",
      "   auto_pad : SAME_UPPER\n",
      "   pads     : auto\n",
      "   kernel   : k=3  (C_in=4, C_out=32)\n",
      "   input    : /backbone/stage1/stage1.3/conv_block/Add_output_0\n",
      "   output   : /backbone/stage1/stage1.4/conv_block/dwconvs.0/Conv_output_0\n",
      "── /backbone/stage2/stage2.3/conv_block/dwconvs.0/Conv\n",
      "   auto_pad : SAME_UPPER\n",
      "   pads     : auto\n",
      "   kernel   : k=3  (C_in=4, C_out=32)\n",
      "   input    : /backbone/stage2/stage2.2/Conv_output_0\n",
      "   output   : /backbone/stage2/stage2.3/conv_block/dwconvs.0/Conv_output_0\n",
      "── /backbone/stage2/stage2.4/conv_block/dwconvs.0/Conv\n",
      "   auto_pad : SAME_UPPER\n",
      "   pads     : auto\n",
      "   kernel   : k=3  (C_in=4, C_out=32)\n",
      "   input    : /backbone/stage2/stage2.3/conv_block/Add_output_0\n",
      "   output   : /backbone/stage2/stage2.4/conv_block/dwconvs.0/Conv_output_0\n",
      "── /backbone/stage2/stage2.5/conv_block/dwconvs.0/Conv\n",
      "   auto_pad : SAME_UPPER\n",
      "   pads     : auto\n",
      "   kernel   : k=3  (C_in=4, C_out=32)\n",
      "   input    : /backbone/stage2/stage2.4/conv_block/Add_output_0\n",
      "   output   : /backbone/stage2/stage2.5/conv_block/dwconvs.0/Conv_output_0\n",
      "── /backbone/stage3/stage3.3/conv_block/dwconvs.0/Conv\n",
      "   auto_pad : SAME_UPPER\n",
      "   pads     : auto\n",
      "   kernel   : k=3  (C_in=4, C_out=96)\n",
      "   input    : /backbone/stage3/stage3.2/Conv_output_0\n",
      "   output   : /backbone/stage3/stage3.3/conv_block/dwconvs.0/Conv_output_0\n",
      "── /backbone/stage3/stage3.4/conv_block/dwconvs.0/Conv\n",
      "   auto_pad : SAME_UPPER\n",
      "   pads     : auto\n",
      "   kernel   : k=3  (C_in=4, C_out=96)\n",
      "   input    : /backbone/stage3/stage3.3/conv_block/Add_output_0\n",
      "   output   : /backbone/stage3/stage3.4/conv_block/dwconvs.0/Conv_output_0\n",
      "── /backbone/stage3/stage3.5/conv_block/dwconvs.0/Conv\n",
      "   auto_pad : SAME_UPPER\n",
      "   pads     : auto\n",
      "   kernel   : k=3  (C_in=4, C_out=96)\n",
      "   input    : /backbone/stage3/stage3.4/conv_block/Add_output_0\n",
      "   output   : /backbone/stage3/stage3.5/conv_block/dwconvs.0/Conv_output_0\n",
      "── /backbone/stage3/stage3.6/conv_block/dwconvs.0/Conv\n",
      "   auto_pad : SAME_UPPER\n",
      "   pads     : auto\n",
      "   kernel   : k=3  (C_in=4, C_out=96)\n",
      "   input    : /backbone/stage3/stage3.5/conv_block/Add_output_0\n",
      "   output   : /backbone/stage3/stage3.6/conv_block/dwconvs.0/Conv_output_0\n",
      "── /backbone/stage4/stage4.3/conv_block/dwconvs.0/Conv\n",
      "   auto_pad : SAME_UPPER\n",
      "   pads     : auto\n",
      "   kernel   : k=3  (C_in=4, C_out=96)\n",
      "   input    : /backbone/stage4/stage4.2/Conv_output_0\n",
      "   output   : /backbone/stage4/stage4.3/conv_block/dwconvs.0/Conv_output_0\n",
      "── /backbone/stage4/stage4.4/conv_block/dwconvs.0/Conv\n",
      "   auto_pad : SAME_UPPER\n",
      "   pads     : auto\n",
      "   kernel   : k=3  (C_in=4, C_out=96)\n",
      "   input    : /backbone/stage4/stage4.3/conv_block/Add_output_0\n",
      "   output   : /backbone/stage4/stage4.4/conv_block/dwconvs.0/Conv_output_0\n",
      "── /backbone/stage4/stage4.5/conv_block/dwconvs.0/Conv\n",
      "   auto_pad : SAME_UPPER\n",
      "   pads     : auto\n",
      "   kernel   : k=3  (C_in=4, C_out=96)\n",
      "   input    : /backbone/stage4/stage4.4/conv_block/Add_output_0\n",
      "   output   : /backbone/stage4/stage4.5/conv_block/dwconvs.0/Conv_output_0\n",
      "── /backbone/stage4/stage4.6/conv_block/dwconvs.0/Conv\n",
      "   auto_pad : SAME_UPPER\n",
      "   pads     : auto\n",
      "   kernel   : k=3  (C_in=4, C_out=96)\n",
      "   input    : /backbone/stage4/stage4.5/conv_block/Add_output_0\n",
      "   output   : /backbone/stage4/stage4.6/conv_block/dwconvs.0/Conv_output_0\n",
      "── /backbone/stage5/stage5.3/conv_block/dwconvs.0/Conv\n",
      "   auto_pad : SAME_UPPER\n",
      "   pads     : auto\n",
      "   kernel   : k=3  (C_in=4, C_out=192)\n",
      "   input    : /backbone/stage5/stage5.2/Conv_output_0\n",
      "   output   : /backbone/stage5/stage5.3/conv_block/dwconvs.0/Conv_output_0\n",
      "── /backbone/stage5/stage5.4/conv_block/dwconvs.0/Conv\n",
      "   auto_pad : SAME_UPPER\n",
      "   pads     : auto\n",
      "   kernel   : k=3  (C_in=4, C_out=192)\n",
      "   input    : /backbone/stage5/stage5.3/conv_block/Add_output_0\n",
      "   output   : /backbone/stage5/stage5.4/conv_block/dwconvs.0/Conv_output_0\n",
      "── /backbone/stage5/stage5.5/conv_block/dwconvs.0/Conv\n",
      "   auto_pad : SAME_UPPER\n",
      "   pads     : auto\n",
      "   kernel   : k=3  (C_in=4, C_out=192)\n",
      "   input    : /backbone/stage5/stage5.4/conv_block/Add_output_0\n",
      "   output   : /backbone/stage5/stage5.5/conv_block/dwconvs.0/Conv_output_0\n",
      "── /backbone/stage5/stage5.6/conv_block/dwconvs.0/Conv\n",
      "   auto_pad : SAME_UPPER\n",
      "   pads     : auto\n",
      "   kernel   : k=3  (C_in=4, C_out=192)\n",
      "   input    : /backbone/stage5/stage5.5/conv_block/Add_output_0\n",
      "   output   : /backbone/stage5/stage5.6/conv_block/dwconvs.0/Conv_output_0\n"
     ]
    }
   ],
   "source": [
    "import torch, onnx, textwrap, json\n",
    "from pathlib import Path\n",
    "\n",
    "m= onnx_model\n",
    "\n",
    "print(\"\\n=== Conv nodes with auto_pad = 'SAME_UPPER' or 'SAME_LOWER' ===\")\n",
    "for n in m.graph.node:\n",
    "    if n.op_type != 'Conv':\n",
    "        continue\n",
    "\n",
    "    auto_attr = next((a for a in n.attribute if a.name == 'auto_pad'), None)\n",
    "    if auto_attr and auto_attr.s.decode() != 'NOTSET':\n",
    "        # gather some extra context -------------------------------------------\n",
    "        # kernel size & in/out channels\n",
    "        w_init = next(i for i in m.graph.initializer if i.name == n.input[1])\n",
    "        C_out, C_in_div_g, kH, kW = w_init.dims\n",
    "        pads_attr = next((a for a in n.attribute if a.name == 'pads'), None)\n",
    "        pads = json.loads(str(list(pads_attr.ints))) if pads_attr else 'auto'\n",
    "\n",
    "        print(textwrap.dedent(f\"\"\"\\\n",
    "            ── {n.name}\n",
    "               auto_pad : {auto_attr.s.decode()}\n",
    "               pads     : {pads}\n",
    "               kernel   : k={kH}  (C_in={C_in_div_g}, C_out={C_out})\n",
    "               input    : {n.input[0]}\n",
    "               output   : {n.output[0]}\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "───────── Gather nodes in model_cf_false_op11.onnx ─────────\n",
      "\n",
      "───────── Gather nodes in model_cf_false_op11.onnx ─────────\n"
     ]
    }
   ],
   "source": [
    "from onnx import AttributeProto, numpy_helper\n",
    "\n",
    "def walk_graph(g, scope=\"\"):\n",
    "    for n in g.node:\n",
    "        if n.op_type == \"Gather\":\n",
    "            axis = next((a.i for a in n.attribute if a.name==\"axis\"), \"?\")\n",
    "            print(f\"\\n🔹 {scope}{n.name or '(unnamed)'}   axis={axis}\")\n",
    "            print(f\"   inputs : {n.input}\")\n",
    "            print(f\"   outputs: {n.output}\")\n",
    "        # dive into sub-graphs (Loop/If/etc.)\n",
    "        for a in n.attribute:\n",
    "            if a.type == AttributeProto.GRAPH:\n",
    "                walk_graph(a.g, scope + n.name + \"/\")\n",
    "\n",
    "\n",
    "m= onnx_model\n",
    "\n",
    "print(\"\\n───────── Gather nodes in model_cf_false_op11.onnx ─────────\")\n",
    "walk_graph(m.graph)\n",
    "print(\"\\n───────── Gather nodes in model_cf_false_op11.onnx ─────────\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pow /backbone/stem/stem.1/Pow\n",
      "Sqrt /backbone/stem/stem.1/Sqrt\n",
      "Transpose /backbone/stem/stem.2/Transpose\n",
      "Transpose /backbone/stage0/stage0.1/Transpose\n",
      "Pow /backbone/stage0/stage0.3/conv_block/act/Pow\n",
      "Pow /backbone/stage0/stage0.4/conv_block/act/Pow\n",
      "Transpose /backbone/stage0/stage0.5/Transpose\n",
      "Pow /backbone/stage0/stage0.6/red_dim_conv/red_dim_conv.1/Pow\n",
      "Sqrt /backbone/stage0/stage0.6/red_dim_conv/red_dim_conv.1/Sqrt\n",
      "Pow /backbone/stage0/stage0.6/tcm/tcm.0/act/Pow\n",
      "Pow /backbone/stage0/stage0.6/tcm/tcm.1/act/Pow\n",
      "Pow /backbone/stage0/stage0.6/tcm/tcm.2/act/Pow\n",
      "Pow /backbone/stage0/stage0.6/tcm/tcm.3/act/Pow\n",
      "Transpose /backbone/stage0/stage0.6/tcm/tcm.4/Transpose\n",
      "Transpose /backbone/stage0/stage0.6/tcm/tcm.4/attention/Transpose\n",
      "Transpose /backbone/stage0/stage0.6/tcm/tcm.4/attention/Transpose_1\n",
      "Transpose /backbone/stage0/stage0.6/tcm/tcm.4/attention/Transpose_2\n",
      "Transpose /backbone/stage0/stage0.6/tcm/tcm.4/attention/Transpose_3\n",
      "Transpose /backbone/stage0/stage0.6/tcm/tcm.4/attention/Transpose_4\n",
      "Pow /backbone/stage0/stage0.6/tcm/tcm.4/feed_forward/intermediate_act_fn/Pow\n",
      "Transpose /backbone/stage0/stage0.6/tcm/tcm.4/Transpose_1\n",
      "Transpose /backbone/stage1/stage1.1/Transpose\n",
      "Pow /backbone/stage1/stage1.3/conv_block/act/Pow\n",
      "Pow /backbone/stage1/stage1.4/conv_block/act/Pow\n",
      "Transpose /backbone/stage1/stage1.5/Transpose\n",
      "Pow /backbone/stage1/stage1.6/red_dim_conv/red_dim_conv.1/Pow\n",
      "Sqrt /backbone/stage1/stage1.6/red_dim_conv/red_dim_conv.1/Sqrt\n",
      "Pow /backbone/stage1/stage1.6/tcm/tcm.0/act/Pow\n",
      "Pow /backbone/stage1/stage1.6/tcm/tcm.1/act/Pow\n",
      "Pow /backbone/stage1/stage1.6/tcm/tcm.2/act/Pow\n",
      "Pow /backbone/stage1/stage1.6/tcm/tcm.3/act/Pow\n",
      "Transpose /backbone/stage1/stage1.6/tcm/tcm.4/Transpose\n",
      "Transpose /backbone/stage1/stage1.6/tcm/tcm.4/attention/Transpose\n",
      "Transpose /backbone/stage1/stage1.6/tcm/tcm.4/attention/Transpose_1\n",
      "Transpose /backbone/stage1/stage1.6/tcm/tcm.4/attention/Transpose_2\n",
      "Transpose /backbone/stage1/stage1.6/tcm/tcm.4/attention/Transpose_3\n",
      "Transpose /backbone/stage1/stage1.6/tcm/tcm.4/attention/Transpose_4\n",
      "Pow /backbone/stage1/stage1.6/tcm/tcm.4/feed_forward/intermediate_act_fn/Pow\n",
      "Transpose /backbone/stage1/stage1.6/tcm/tcm.4/Transpose_1\n",
      "Transpose /backbone/stage2/stage2.1/Transpose\n",
      "Pow /backbone/stage2/stage2.3/conv_block/act/Pow\n",
      "Pow /backbone/stage2/stage2.4/conv_block/act/Pow\n",
      "Pow /backbone/stage2/stage2.5/conv_block/act/Pow\n",
      "Transpose /backbone/stage2/stage2.6/Transpose\n",
      "Pow /backbone/stage2/stage2.7/red_dim_conv/red_dim_conv.1/Pow\n",
      "Sqrt /backbone/stage2/stage2.7/red_dim_conv/red_dim_conv.1/Sqrt\n",
      "Pow /backbone/stage2/stage2.7/tcm/tcm.0/act/Pow\n",
      "Pow /backbone/stage2/stage2.7/tcm/tcm.1/act/Pow\n",
      "Pow /backbone/stage2/stage2.7/tcm/tcm.2/act/Pow\n",
      "Pow /backbone/stage2/stage2.7/tcm/tcm.3/act/Pow\n",
      "Transpose /backbone/stage2/stage2.7/tcm/tcm.4/Transpose\n",
      "Transpose /backbone/stage2/stage2.7/tcm/tcm.4/attention/Transpose\n",
      "Transpose /backbone/stage2/stage2.7/tcm/tcm.4/attention/Transpose_1\n",
      "Transpose /backbone/stage2/stage2.7/tcm/tcm.4/attention/Transpose_2\n",
      "Transpose /backbone/stage2/stage2.7/tcm/tcm.4/attention/Transpose_3\n",
      "Transpose /backbone/stage2/stage2.7/tcm/tcm.4/attention/Transpose_4\n",
      "Pow /backbone/stage2/stage2.7/tcm/tcm.4/feed_forward/intermediate_act_fn/Pow\n",
      "Transpose /backbone/stage2/stage2.7/tcm/tcm.4/Transpose_1\n",
      "Transpose /backbone/stage3/stage3.1/Transpose\n",
      "Pow /backbone/stage3/stage3.3/conv_block/act/Pow\n",
      "Pow /backbone/stage3/stage3.4/conv_block/act/Pow\n",
      "Pow /backbone/stage3/stage3.5/conv_block/act/Pow\n",
      "Pow /backbone/stage3/stage3.6/conv_block/act/Pow\n",
      "Transpose /backbone/stage3/stage3.7/Transpose\n",
      "Pow /backbone/stage3/stage3.8/red_dim_conv/red_dim_conv.1/Pow\n",
      "Sqrt /backbone/stage3/stage3.8/red_dim_conv/red_dim_conv.1/Sqrt\n",
      "Pow /backbone/stage3/stage3.8/tcm/tcm.0/act/Pow\n",
      "Pow /backbone/stage3/stage3.8/tcm/tcm.1/act/Pow\n",
      "Pow /backbone/stage3/stage3.8/tcm/tcm.2/act/Pow\n",
      "Pow /backbone/stage3/stage3.8/tcm/tcm.3/act/Pow\n",
      "Transpose /backbone/stage3/stage3.8/tcm/tcm.4/Transpose\n",
      "Transpose /backbone/stage3/stage3.8/tcm/tcm.4/attention/Transpose\n",
      "Transpose /backbone/stage3/stage3.8/tcm/tcm.4/attention/Transpose_1\n",
      "Transpose /backbone/stage3/stage3.8/tcm/tcm.4/attention/Transpose_2\n",
      "Transpose /backbone/stage3/stage3.8/tcm/tcm.4/attention/Transpose_3\n",
      "Transpose /backbone/stage3/stage3.8/tcm/tcm.4/attention/Transpose_4\n",
      "Pow /backbone/stage3/stage3.8/tcm/tcm.4/feed_forward/intermediate_act_fn/Pow\n",
      "Transpose /backbone/stage3/stage3.8/tcm/tcm.4/Transpose_1\n",
      "Transpose /backbone/stage4/stage4.1/Transpose\n",
      "Pow /backbone/stage4/stage4.3/conv_block/act/Pow\n",
      "Pow /backbone/stage4/stage4.4/conv_block/act/Pow\n",
      "Pow /backbone/stage4/stage4.5/conv_block/act/Pow\n",
      "Pow /backbone/stage4/stage4.6/conv_block/act/Pow\n",
      "Transpose /backbone/stage4/stage4.7/Transpose\n",
      "Pow /backbone/stage4/stage4.8/red_dim_conv/red_dim_conv.1/Pow\n",
      "Sqrt /backbone/stage4/stage4.8/red_dim_conv/red_dim_conv.1/Sqrt\n",
      "Pow /backbone/stage4/stage4.8/tcm/tcm.0/act/Pow\n",
      "Pow /backbone/stage4/stage4.8/tcm/tcm.1/act/Pow\n",
      "Pow /backbone/stage4/stage4.8/tcm/tcm.2/act/Pow\n",
      "Pow /backbone/stage4/stage4.8/tcm/tcm.3/act/Pow\n",
      "Transpose /backbone/stage4/stage4.8/tcm/tcm.4/Transpose\n",
      "Transpose /backbone/stage4/stage4.8/tcm/tcm.4/attention/Transpose\n",
      "Transpose /backbone/stage4/stage4.8/tcm/tcm.4/attention/Transpose_1\n",
      "Transpose /backbone/stage4/stage4.8/tcm/tcm.4/attention/Transpose_2\n",
      "Transpose /backbone/stage4/stage4.8/tcm/tcm.4/attention/Transpose_3\n",
      "Transpose /backbone/stage4/stage4.8/tcm/tcm.4/attention/Transpose_4\n",
      "Pow /backbone/stage4/stage4.8/tcm/tcm.4/feed_forward/intermediate_act_fn/Pow\n",
      "Transpose /backbone/stage4/stage4.8/tcm/tcm.4/Transpose_1\n",
      "Transpose /backbone/stage5/stage5.1/Transpose\n",
      "Pow /backbone/stage5/stage5.3/conv_block/act/Pow\n",
      "Pow /backbone/stage5/stage5.4/conv_block/act/Pow\n",
      "Pow /backbone/stage5/stage5.5/conv_block/act/Pow\n",
      "Pow /backbone/stage5/stage5.6/conv_block/act/Pow\n",
      "Transpose /backbone/stage5/stage5.7/Transpose\n",
      "Pow /backbone/stage5/stage5.8/red_dim_conv/red_dim_conv.1/Pow\n",
      "Sqrt /backbone/stage5/stage5.8/red_dim_conv/red_dim_conv.1/Sqrt\n",
      "Pow /backbone/stage5/stage5.8/tcm/tcm.0/act/Pow\n",
      "Pow /backbone/stage5/stage5.8/tcm/tcm.1/act/Pow\n",
      "Pow /backbone/stage5/stage5.8/tcm/tcm.2/act/Pow\n",
      "Pow /backbone/stage5/stage5.8/tcm/tcm.3/act/Pow\n",
      "Transpose /backbone/stage5/stage5.8/tcm/tcm.4/Transpose\n",
      "Transpose /backbone/stage5/stage5.8/tcm/tcm.4/attention/Transpose\n",
      "Transpose /backbone/stage5/stage5.8/tcm/tcm.4/attention/Transpose_1\n",
      "Transpose /backbone/stage5/stage5.8/tcm/tcm.4/attention/Transpose_2\n",
      "Transpose /backbone/stage5/stage5.8/tcm/tcm.4/attention/Transpose_3\n",
      "Transpose /backbone/stage5/stage5.8/tcm/tcm.4/attention/Transpose_4\n",
      "Pow /backbone/stage5/stage5.8/tcm/tcm.4/feed_forward/intermediate_act_fn/Pow\n",
      "Transpose /backbone/stage5/stage5.8/tcm/tcm.4/Transpose_1\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "m= onnx_model\n",
    "bad_ops = {\"Gather\", \"Pow\", \"Sqrt\", \"Log\", \"Exp\", \"Transpose\"}  # add more if needed\n",
    "for n in m.graph.node:\n",
    "    if n.op_type in bad_ops:\n",
    "        print(n.op_type, n.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_onnx(wav_path):\n",
    "    \"\"\"\n",
    "    Loads an audio file, converts to log-mel, and runs inference\n",
    "    in an ONNX session. Returns the embedding as a NumPy array.\n",
    "    \"\"\"\n",
    "    print(\"===================================================\")\n",
    "    print(\"===========   run_inference_onnx   ================\")\n",
    "    print(\"===================================================\")\n",
    "    #######################################\n",
    "    # 1) Load your ONNX model\n",
    "    #######################################\n",
    "    # (Optional) onnx.checker to confirm it’s valid\n",
    "    onnx_model = onnx.load(onnx_path)\n",
    "    onnx.checker.check_model(onnx_model)\n",
    "    print(f\"Loaded and checked ONNX model from: {onnx_path}\")\n",
    "\n",
    "    # Create an inference session\n",
    "    session = ort.InferenceSession(onnx_path)\n",
    "\n",
    "    # Usually we retrieve the first input & output name\n",
    "    input_name = session.get_inputs()[0].name\n",
    "    output_name = session.get_outputs()[0].name\n",
    "\n",
    "    #######################################\n",
    "    # 2) Load audio, get log-mel\n",
    "    #######################################\n",
    "    print(\"loading audio from:\", wav_path)\n",
    "    waveform, sample_rate = torchaudio.load(wav_path)\n",
    "    print(f\"...Waveform rate {sample_rate}  ; shape : {waveform.shape}\")\n",
    "\n",
    "    \n",
    "    # If multi-channel, downmix:\n",
    "    if waveform.shape[0] > 1:\n",
    "        waveform = waveform.mean(dim=0, keepdim=True)\n",
    "        \n",
    "    # Resample if needed\n",
    "    target_sample_rate=16000\n",
    "    if sample_rate != target_sample_rate:\n",
    "        resampler = T.Resample(orig_freq=sample_rate, new_freq=target_sample_rate)\n",
    "        waveform = resampler(waveform)\n",
    "        # save resampled waveform to files with suffix \"_resampled_16.wav\"\n",
    "        # torchaudio.save(wav_path.replace(\".wav\", \"_resampled_16.wav\"), waveform, target_sample_rate)\n",
    "\n",
    "    log_mel =  myUtils.waveform_to_logmel(waveform)\n",
    "    \n",
    "    #######################################\n",
    "    # 3) ONNX Inference\n",
    "    #######################################\n",
    "    # Convert to NumPy for ONNX runtime\n",
    "    log_mel_np = log_mel.cpu().numpy()\n",
    "    \n",
    "    ## save log_mel_np to file with suffix \"_logmel.npy\" to check later\n",
    "    print(\"logmelshape : \", log_mel_np.shape)\n",
    "    log_mel_fp16 = log_mel_np.astype(np.float16)  # → half precision\n",
    "    orig_name = os.path.splitext(os.path.basename(wav_path))[0]\n",
    "    folder = os.path.dirname(wav_path)\n",
    "    out_path = os.path.join(folder, f\"logmel_{orig_name}.npy\")\n",
    "    np.save(out_path, log_mel_fp16)\n",
    "    \n",
    "    # Run inference\n",
    "    outputs = session.run([output_name], {input_name: log_mel_np})\n",
    "    # outputs is a list; typically we want the first item\n",
    "    embedding = outputs[0]  # shape is [1, embedding_dim]\n",
    "\n",
    "    # print(\"Embedding[10]: \", embedding[0:10])  # Print the 10th element of the embedding\n",
    "    print(\"Embedding shape:\", embedding.shape)\n",
    "    # print(\"Embedding data:\\n\", embedding)\n",
    "    return embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================================\n",
      "===========   run_inference_onnx   ================\n",
      "===================================================\n",
      "Loaded and checked ONNX model from: ReDimNet_no_mel_fp16.onnx\n",
      "loading audio from: /data/proj/voice/redimnet/wrkB2/utils/../audio/test000.wav\n",
      "...Waveform rate 16000  ; shape : torch.Size([1, 293699])\n",
      "Input waveform shape: torch.Size([1, 32000])\n",
      "logmelshape :  (1, 1, 72, 134)\n",
      "Embedding shape: (1, 192)\n",
      "===================================================\n",
      "===========   run_inference_onnx   ================\n",
      "===================================================\n",
      "Loaded and checked ONNX model from: ReDimNet_no_mel_fp16.onnx\n",
      "loading audio from: /data/proj/voice/redimnet/wrkB2/utils/../audio/testRob1.wav\n",
      "...Waveform rate 22050  ; shape : torch.Size([1, 35522])\n",
      "Input waveform shape: torch.Size([1, 25776])\n",
      "Padding log_mel from 108 to 134 frames\n",
      "logmelshape :  (1, 1, 72, 134)\n",
      "Embedding shape: (1, 192)\n",
      "===================================================\n",
      "===========   run_inference_onnx   ================\n",
      "===================================================\n",
      "Loaded and checked ONNX model from: ReDimNet_no_mel_fp16.onnx\n",
      "loading audio from: /data/proj/voice/redimnet/wrkB2/utils/../audio/testRob2.wav\n",
      "...Waveform rate 22050  ; shape : torch.Size([1, 32482])\n",
      "Input waveform shape: torch.Size([1, 23570])\n",
      "Padding log_mel from 99 to 134 frames\n",
      "logmelshape :  (1, 1, 72, 134)\n",
      "Embedding shape: (1, 192)\n",
      "===================================================\n",
      "===========   run_inference_onnx   ================\n",
      "===================================================\n",
      "Loaded and checked ONNX model from: ReDimNet_no_mel_fp16.onnx\n",
      "loading audio from: /data/proj/voice/redimnet/wrkB2/utils/../audio/test_human1_1.wav\n",
      "...Waveform rate 16000  ; shape : torch.Size([1, 65867])\n",
      "Input waveform shape: torch.Size([1, 32000])\n",
      "logmelshape :  (1, 1, 72, 134)\n",
      "Embedding shape: (1, 192)\n",
      "===================================================\n",
      "===========   run_inference_onnx   ================\n",
      "===================================================\n",
      "Loaded and checked ONNX model from: ReDimNet_no_mel_fp16.onnx\n",
      "loading audio from: /data/proj/voice/redimnet/wrkB2/utils/../audio/test_human1_2.wav\n",
      "...Waveform rate 16000  ; shape : torch.Size([1, 101189])\n",
      "Input waveform shape: torch.Size([1, 32000])\n",
      "logmelshape :  (1, 1, 72, 134)\n",
      "Embedding shape: (1, 192)\n",
      "===================================================\n",
      "===========   run_inference_onnx   ================\n",
      "===================================================\n",
      "Loaded and checked ONNX model from: ReDimNet_no_mel_fp16.onnx\n",
      "loading audio from: /data/proj/voice/redimnet/wrkB2/utils/../audio/test_human2_1.wav\n",
      "...Waveform rate 48000  ; shape : torch.Size([1, 84376])\n",
      "Input waveform shape: torch.Size([1, 28126])\n",
      "Padding log_mel from 118 to 134 frames\n",
      "logmelshape :  (1, 1, 72, 134)\n",
      "Embedding shape: (1, 192)\n",
      "===================================================\n",
      "===========   run_inference_onnx   ================\n",
      "===================================================\n",
      "Loaded and checked ONNX model from: ReDimNet_no_mel_fp16.onnx\n",
      "loading audio from: /data/proj/voice/redimnet/wrkB2/utils/../audio/test_human2_2.wav\n",
      "...Waveform rate 48000  ; shape : torch.Size([1, 159256])\n",
      "Input waveform shape: torch.Size([1, 32000])\n",
      "logmelshape :  (1, 1, 72, 134)\n",
      "Embedding shape: (1, 192)\n",
      "**************************************************************************\n",
      "*************************   compare summary ******************************\n",
      "**************************************************************************\n",
      "====>>>> should be similar:\n",
      "Similarity (robot1 to robot2 ): 0.9347119927406311\n",
      "Similarity (human1 to human1 ): 0.8388952612876892\n",
      "Similarity (human2 to human2 ): 0.7440540194511414\n",
      "====>>>> should be differnet:\n",
      "Similarity (robot to human1  ): 0.4990156888961792\n",
      "Similarity (robot to human2  ): 0.4831867218017578\n",
      "Similarity (human1 to human2 ): 0.6287863254547119\n"
     ]
    }
   ],
   "source": [
    "onnx_embedding = test_all_voices(\n",
    "    extract_speaker_embedding_function = inference_onnx,\n",
    "    cosine_similarity_function = myUtils.cosine_similarity_numpys,\n",
    "    save_embeddings=True,  # Save embeddings to files\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### compare onnx with torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity embde0: 0.9999987483024597\n",
      "Similarity embde1: 0.9999989867210388\n",
      "Similarity embde2: 0.9999990463256836\n",
      "Similarity embde3: 0.9999991655349731\n",
      "Similarity embde4: 0.9999986886978149\n",
      "Similarity embde5: 0.9999983906745911\n",
      "Similarity embde6: 0.999998927116394\n"
     ]
    }
   ],
   "source": [
    "print(f\"Similarity embde0: {myUtils.cosine_similarity_numpys(torch_embedding['embed0'], onnx_embedding['embed0'])}\")\n",
    "print(f\"Similarity embde1: {myUtils.cosine_similarity_numpys(torch_embedding['embed1'], onnx_embedding['embed1'])}\")\n",
    "print(f\"Similarity embde2: {myUtils.cosine_similarity_numpys(torch_embedding['embed2'], onnx_embedding['embed2'])}\")\n",
    "print(f\"Similarity embde3: {myUtils.cosine_similarity_numpys(torch_embedding['embed3'], onnx_embedding['embed3'])}\")\n",
    "print(f\"Similarity embde4: {myUtils.cosine_similarity_numpys(torch_embedding['embed4'], onnx_embedding['embed4'])}\")\n",
    "print(f\"Similarity embde5: {myUtils.cosine_similarity_numpys(torch_embedding['embed5'], onnx_embedding['embed5'])}\")\n",
    "print(f\"Similarity embde6: {myUtils.cosine_similarity_numpys(torch_embedding['embed6'], onnx_embedding['embed6'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### compare onnx with base line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity embde0: 0.40715181827545166\n",
      "Similarity embde1: 0.5618564486503601\n",
      "Similarity embde2: 0.6011956930160522\n",
      "Similarity embde3: 0.4821603298187256\n",
      "Similarity embde4: 0.46946969628334045\n",
      "Similarity embde5: 0.26959607005119324\n",
      "Similarity embde6: 0.3525412976741791\n"
     ]
    }
   ],
   "source": [
    "print(f\"Similarity embde0: {myUtils.cosine_similarity_numpys(base_line_embedding['embed0'], onnx_embedding['embed0'])}\")\n",
    "print(f\"Similarity embde1: {myUtils.cosine_similarity_numpys(base_line_embedding['embed1'], onnx_embedding['embed1'])}\")\n",
    "print(f\"Similarity embde2: {myUtils.cosine_similarity_numpys(base_line_embedding['embed2'], onnx_embedding['embed2'])}\")\n",
    "print(f\"Similarity embde3: {myUtils.cosine_similarity_numpys(base_line_embedding['embed3'], onnx_embedding['embed3'])}\")\n",
    "print(f\"Similarity embde4: {myUtils.cosine_similarity_numpys(base_line_embedding['embed4'], onnx_embedding['embed4'])}\")\n",
    "print(f\"Similarity embde5: {myUtils.cosine_similarity_numpys(base_line_embedding['embed5'], onnx_embedding['embed5'])}\")\n",
    "print(f\"Similarity embde6: {myUtils.cosine_similarity_numpys(base_line_embedding['embed6'], onnx_embedding['embed6'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cal fake data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* dummy for nchw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import numpy as np\n",
    "# import torch\n",
    "\n",
    "# # Directory for calibration inputs\n",
    "# os.makedirs(\"calib_npy\", exist_ok=True)\n",
    "\n",
    "# # Create 100 dummy log-mel tensors\n",
    "# for i in range(10):\n",
    "#     log_mel = torch.randn(1, 1, 60, 134).numpy().astype(np.float16)\n",
    "#     np.save(f\"calib_npy/sample_{i}.npy\", log_mel)\n",
    "\n",
    "# # Write dataset.txt listing all paths\n",
    "# with open(\"dataset.txt\", \"w\") as f:\n",
    "#     for i in range(10):\n",
    "#         f.write(f\"calib_npy/sample_{i}.npy\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* dummy for nchw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import numpy as np\n",
    "# import torch\n",
    "\n",
    "# # Directory for calibration inputs\n",
    "# os.makedirs(\"calib_npy\", exist_ok=True)\n",
    "\n",
    "# # Create 100 dummy log-mel tensors\n",
    "# for i in range(2):\n",
    "#     log_mel = torch.randn(1, 72, 134,1).numpy().astype(np.float16)\n",
    "#     np.save(f\"calib_npy/sample_{i}.npy\", log_mel)\n",
    "\n",
    "# # Write dataset.txt listing all paths\n",
    "# with open(\"dataset.txt\", \"w\") as f:\n",
    "#     for i in range(10):\n",
    "#         f.write(f\"calib_npy/sample_{i}.npy\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# converts\n",
    "\n",
    "```\n",
    "python convert.py \\\n",
    "       ../wrkB0/ReDimNet_no_mel_fp16.onnx rk3588 fp ReDimNet_no_mel.rknn \\\n",
    "       ../wrkB0/audio/logmel_testRob1.npy  ../wrkB0/audio/embedding_testRob1.torch\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyvoice_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
